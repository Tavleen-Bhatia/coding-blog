<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-06-06T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">432240</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2506.05349v1</id>
    <updated>2025-06-05T17:59:58Z</updated>
    <published>2025-06-05T17:59:58Z</published>
    <title>VideoMathQA: Benchmarking Mathematical Reasoning via Multimodal
  Understanding in Videos</title>
    <summary>  Mathematical reasoning in real-world video settings presents a fundamentally
different challenge than in static images or text. It requires interpreting
fine-grained visual information, accurately reading handwritten or digital
text, and integrating spoken cues, often dispersed non-linearly over time. In
such multimodal contexts, success hinges not just on perception, but on
selectively identifying and integrating the right contextual details from a
rich and noisy stream of content. To this end, we introduce VideoMathQA, a
benchmark designed to evaluate whether models can perform such temporally
extended cross-modal reasoning on videos. The benchmark spans 10 diverse
mathematical domains, covering videos ranging from 10 seconds to over 1 hour.
It requires models to interpret structured visual content, understand
instructional narratives, and jointly ground concepts across visual, audio, and
textual modalities. We employ graduate-level experts to ensure high quality,
totaling over $920$ man-hours of annotation. To reflect real-world scenarios,
questions are designed around three core reasoning challenges: direct problem
solving, where answers are grounded in the presented question; conceptual
transfer, which requires applying learned methods to new problems; and deep
instructional comprehension, involving multi-step reasoning over extended
explanations and partially worked-out solutions. Each question includes
multi-step reasoning annotations, enabling fine-grained diagnosis of model
capabilities. Through this benchmark, we highlight the limitations of existing
approaches and establish a systematic evaluation framework for models that must
reason, rather than merely perceive, across temporally extended and
modality-rich mathematical problem settings. Our benchmark and evaluation code
are available at: https://mbzuai-oryx.github.io/VideoMathQA
</summary>
    <author>
      <name>Hanoona Rasheed</name>
    </author>
    <author>
      <name>Abdelrahman Shaker</name>
    </author>
    <author>
      <name>Anqi Tang</name>
    </author>
    <author>
      <name>Muhammad Maaz</name>
    </author>
    <author>
      <name>Ming-Hsuan Yang</name>
    </author>
    <author>
      <name>Salman Khan</name>
    </author>
    <author>
      <name>Fahad Khan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VideoMathQA Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.05349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.05349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.05345v1</id>
    <updated>2025-06-05T17:59:55Z</updated>
    <published>2025-06-05T17:59:55Z</published>
    <title>Inference-Time Hyper-Scaling with KV Cache Compression</title>
    <summary>  Inference-time scaling trades efficiency for increased reasoning accuracy by
generating longer or more parallel sequences. However, in Transformer LLMs,
generation cost is bottlenecked by the size of the key-value (KV) cache, rather
than the number of generated tokens. Hence, we explore inference-time
hyper-scaling: by compressing the KV cache, we can generate more tokens within
the same compute budget and further improve the accuracy of scaled inference.
The success of this approach, however, hinges on the ability of compression
methods to preserve accuracy even at high compression ratios. To make
hyper-scaling practical, we introduce Dynamic Memory Sparsification (DMS), a
novel method for sparsifying KV caches that only requires 1K training steps to
achieve 8$\times$ compression, while maintaining better accuracy than
training-free sparse attention. Instead of prematurely discarding cached
tokens, DMS delays token eviction, implicitly merging representations and
preserving critical information. We demonstrate the effectiveness of
inference-time hyper-scaling with DMS on multiple families of LLMs, showing
that it boosts accuracy for comparable inference runtime and memory load. For
instance, we enhance Qwen-R1 32B by an average of 9.1 points on AIME 24, 7.6 on
GPQA, and 9.6 on LiveCodeBench across compute budgets.
</summary>
    <author>
      <name>Adrian Łańcucki</name>
    </author>
    <author>
      <name>Konrad Staniszewski</name>
    </author>
    <author>
      <name>Piotr Nawrot</name>
    </author>
    <author>
      <name>Edoardo M. Ponti</name>
    </author>
    <link href="http://arxiv.org/abs/2506.05345v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.05345v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.05346v1</id>
    <updated>2025-06-05T17:59:55Z</updated>
    <published>2025-06-05T17:59:55Z</published>
    <title>Why LLM Safety Guardrails Collapse After Fine-tuning: A Similarity
  Analysis Between Alignment and Fine-tuning Datasets</title>
    <summary>  Recent advancements in large language models (LLMs) have underscored their
vulnerability to safety alignment jailbreaks, particularly when subjected to
downstream fine-tuning. However, existing mitigation strategies primarily focus
on reactively addressing jailbreak incidents after safety guardrails have been
compromised, removing harmful gradients during fine-tuning, or continuously
reinforcing safety alignment throughout fine-tuning. As such, they tend to
overlook a critical upstream factor: the role of the original safety-alignment
data. This paper therefore investigates the degradation of safety guardrails
through the lens of representation similarity between upstream alignment
datasets and downstream fine-tuning tasks. Our experiments demonstrate that
high similarity between these datasets significantly weakens safety guardrails,
making models more susceptible to jailbreaks. Conversely, low similarity
between these two types of datasets yields substantially more robust models and
thus reduces harmfulness score by up to 10.33%. By highlighting the importance
of upstream dataset design in the building of durable safety guardrails and
reducing real-world vulnerability to jailbreak attacks, these findings offer
actionable insights for fine-tuning service providers.
</summary>
    <author>
      <name>Lei Hsiung</name>
    </author>
    <author>
      <name>Tianyu Pang</name>
    </author>
    <author>
      <name>Yung-Chen Tang</name>
    </author>
    <author>
      <name>Linyue Song</name>
    </author>
    <author>
      <name>Tsung-Yi Ho</name>
    </author>
    <author>
      <name>Pin-Yu Chen</name>
    </author>
    <author>
      <name>Yaoqing Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://hsiung.cc/llm-similarity-risk/</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.05346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.05346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.05343v1</id>
    <updated>2025-06-05T17:59:54Z</updated>
    <published>2025-06-05T17:59:54Z</published>
    <title>ContentV: Efficient Training of Video Generation Models with Limited
  Compute</title>
    <summary>  Recent advances in video generation demand increasingly efficient training
recipes to mitigate escalating computational costs. In this report, we present
ContentV, an 8B-parameter text-to-video model that achieves state-of-the-art
performance (85.14 on VBench) after training on 256 x 64GB Neural Processing
Units (NPUs) for merely four weeks. ContentV generates diverse, high-quality
videos across multiple resolutions and durations from text prompts, enabled by
three key innovations: (1) A minimalist architecture that maximizes reuse of
pre-trained image generation models for video generation; (2) A systematic
multi-stage training strategy leveraging flow matching for enhanced efficiency;
and (3) A cost-effective reinforcement learning with human feedback framework
that improves generation quality without requiring additional human
annotations. All the code and models are available at:
https://contentv.github.io.
</summary>
    <author>
      <name>Wenfeng Lin</name>
    </author>
    <author>
      <name>Renjie Chen</name>
    </author>
    <author>
      <name>Boyuan Liu</name>
    </author>
    <author>
      <name>Shiyue Yan</name>
    </author>
    <author>
      <name>Ruoyu Feng</name>
    </author>
    <author>
      <name>Jiangchuan Wei</name>
    </author>
    <author>
      <name>Yichen Zhang</name>
    </author>
    <author>
      <name>Yimeng Zhou</name>
    </author>
    <author>
      <name>Chao Feng</name>
    </author>
    <author>
      <name>Jiao Ran</name>
    </author>
    <author>
      <name>Qi Wu</name>
    </author>
    <author>
      <name>Zuotao Liu</name>
    </author>
    <author>
      <name>Mingyu Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://contentv.github.io</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.05343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.05343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.05341v1</id>
    <updated>2025-06-05T17:59:42Z</updated>
    <published>2025-06-05T17:59:42Z</published>
    <title>Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via
  Spatial Reasoning</title>
    <summary>  Realistic 3D indoor scene synthesis is vital for embodied AI and digital
content creation. It can be naturally divided into two subtasks: object
generation and layout generation. While recent generative models have
significantly advanced object-level quality and controllability, layout
generation remains challenging due to limited datasets. Existing methods either
overfit to these datasets or rely on predefined constraints to optimize
numerical layout that sacrifice flexibility. As a result, they fail to generate
scenes that are both open-vocabulary and aligned with fine-grained user
instructions. We introduce DirectLayout, a framework that directly generates
numerical 3D layouts from text descriptions using generalizable spatial
reasoning of large language models (LLMs). DirectLayout decomposes the
generation into three stages: producing a Bird's-Eye View (BEV) layout, lifting
it into 3D space, and refining object placements. To enable explicit spatial
reasoning and help the model grasp basic principles of object placement, we
employ Chain-of-Thought (CoT) Activation based on the 3D-Front dataset.
Additionally, we design CoT-Grounded Generative Layout Reward to enhance
generalization and spatial planning. During inference, DirectLayout addresses
asset-layout mismatches via Iterative Asset-Layout Alignment through in-context
learning. Extensive experiments demonstrate that DirectLayout achieves
impressive semantic consistency, generalization and physical plausibility.
</summary>
    <author>
      <name>Xingjian Ran</name>
    </author>
    <author>
      <name>Yixuan Li</name>
    </author>
    <author>
      <name>Linning Xu</name>
    </author>
    <author>
      <name>Mulin Yu</name>
    </author>
    <author>
      <name>Bo Dai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://directlayout.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.05341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.05341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
