<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-02-17T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">404994</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2502.10392v1</id>
    <updated>2025-02-14T18:59:59Z</updated>
    <published>2025-02-14T18:59:59Z</published>
    <title>Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding</title>
    <summary>  In this paper, we propose an efficient multi-level convolution architecture
for 3D visual grounding. Conventional methods are difficult to meet the
requirements of real-time inference due to the two-stage or point-based
architecture. Inspired by the success of multi-level fully sparse convolutional
architecture in 3D object detection, we aim to build a new 3D visual grounding
framework following this technical route. However, as in 3D visual grounding
task the 3D scene representation should be deeply interacted with text
features, sparse convolution-based architecture is inefficient for this
interaction due to the large amount of voxel features. To this end, we propose
text-guided pruning (TGP) and completion-based addition (CBA) to deeply fuse 3D
scene representation and text features in an efficient way by gradual region
pruning and target completion. Specifically, TGP iteratively sparsifies the 3D
scene representation and thus efficiently interacts the voxel features with
text features by cross-attention. To mitigate the affect of pruning on delicate
geometric information, CBA adaptively fixes the over-pruned region by voxel
completion with negligible computational overhead. Compared with previous
single-stage methods, our method achieves top inference speed and surpasses
previous fastest method by 100\% FPS. Our method also achieves state-of-the-art
accuracy even compared with two-stage methods, with $+1.13$ lead of Acc@0.5 on
ScanRefer, and $+2.6$ and $+3.2$ leads on NR3D and SR3D respectively. The code
is available at
\href{https://github.com/GWxuan/TSP3D}{https://github.com/GWxuan/TSP3D}.
</summary>
    <author>
      <name>Wenxuan Guo</name>
    </author>
    <author>
      <name>Xiuwei Xu</name>
    </author>
    <author>
      <name>Ziwei Wang</name>
    </author>
    <author>
      <name>Jianjiang Feng</name>
    </author>
    <author>
      <name>Jie Zhou</name>
    </author>
    <author>
      <name>Jiwen Lu</name>
    </author>
    <link href="http://arxiv.org/abs/2502.10392v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.10392v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.10390v1</id>
    <updated>2025-02-14T18:59:40Z</updated>
    <published>2025-02-14T18:59:40Z</published>
    <title>(How) Can Transformers Predict Pseudo-Random Numbers?</title>
    <summary>  Transformers excel at discovering patterns in sequential data, yet their
fundamental limitations and learning mechanisms remain crucial topics of
investigation. In this paper, we study the ability of Transformers to learn
pseudo-random number sequences from linear congruential generators (LCGs),
defined by the recurrence relation $x_{t+1} = a x_t + c \;\mathrm{mod}\; m$.
Our analysis reveals that with sufficient architectural capacity and training
data variety, Transformers can perform in-context prediction of LCG sequences
with unseen moduli ($m$) and parameters ($a,c$). Through analysis of embedding
layers and attention patterns, we uncover how Transformers develop algorithmic
structures to learn these sequences in two scenarios of increasing complexity.
First, we analyze how Transformers learn LCG sequences with unseen ($a, c$) but
fixed modulus, and we demonstrate successful learning up to $m = 2^{32}$. Our
analysis reveals that models learn to factorize the modulus and utilize
digit-wise number representations to make sequential predictions. In the
second, more challenging scenario of unseen moduli, we show that Transformers
can generalize to unseen moduli up to $m_{\text{test}} = 2^{16}$. In this case,
the model employs a two-step strategy: first estimating the unknown modulus
from the context, then utilizing prime factorizations to generate predictions.
For this task, we observe a sharp transition in the accuracy at a critical
depth $=3$. We also find that the number of in-context sequence elements needed
to reach high accuracy scales sublinearly with the modulus.
</summary>
    <author>
      <name>Tao Tao</name>
    </author>
    <author>
      <name>Darshil Doshi</name>
    </author>
    <author>
      <name>Dayal Singh Kalra</name>
    </author>
    <author>
      <name>Tianyu He</name>
    </author>
    <author>
      <name>Maissam Barkeshli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10+16 pages, 12+20 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.10390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.10390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.10385v1</id>
    <updated>2025-02-14T18:58:04Z</updated>
    <published>2025-02-14T18:58:04Z</published>
    <title>Simplifying DINO via Coding Rate Regularization</title>
    <summary>  DINO and DINOv2 are two model families being widely used to learn
representations from unlabeled imagery data at large scales. Their learned
representations often enable state-of-the-art performance for downstream tasks,
such as image classification and segmentation. However, they employ many
empirically motivated design choices and their training pipelines are highly
complex and unstable -- many hyperparameters need to be carefully tuned to
ensure that the representations do not collapse -- which poses considerable
difficulty to improving them or adapting them to new domains. In this work, we
posit that we can remove most such-motivated idiosyncrasies in the pre-training
pipelines, and only need to add an explicit coding rate term in the loss
function to avoid collapse of the representations. As a result, we obtain
highly simplified variants of the DINO and DINOv2 which we call SimDINO and
SimDINOv2, respectively. Remarkably, these simplified models are more robust to
different design choices, such as network architecture and hyperparameters, and
they learn even higher-quality representations, measured by performance on
downstream tasks, offering a Pareto improvement over the corresponding DINO and
DINOv2 models. This work highlights the potential of using simplifying design
principles to improve the empirical practice of deep learning.
</summary>
    <author>
      <name>Ziyang Wu</name>
    </author>
    <author>
      <name>Jingyuan Zhang</name>
    </author>
    <author>
      <name>Druv Pai</name>
    </author>
    <author>
      <name>XuDong Wang</name>
    </author>
    <author>
      <name>Chandan Singh</name>
    </author>
    <author>
      <name>Jianwei Yang</name>
    </author>
    <author>
      <name>Jianfeng Gao</name>
    </author>
    <author>
      <name>Yi Ma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.10385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.10385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.10380v1</id>
    <updated>2025-02-14T18:57:16Z</updated>
    <published>2025-02-14T18:57:16Z</published>
    <title>A new and flexible class of sharp asymptotic time-uniform confidence
  sequences</title>
    <summary>  Confidence sequences are anytime-valid analogues of classical confidence
intervals that do not suffer from multiplicity issues under optional
continuation of the data collection. As in classical statistics, asymptotic
confidence sequences are a nonparametric tool showing under which high-level
assumptions asymptotic coverage is achieved so that they also give a certain
robustness guarantee against distributional deviations. In this paper, we
propose a new flexible class of confidence sequences yielding sharp asymptotic
time-uniform confidence sequences under mild assumptions. Furthermore, we
highlight the connection to corresponding sequential testing problems and
detail the underlying limit theorem.
</summary>
    <author>
      <name>Felix Gnettner</name>
    </author>
    <author>
      <name>Claudia Kirch</name>
    </author>
    <link href="http://arxiv.org/abs/2502.10380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.10380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62G15 (Primary) 62G10, 62G20, 62L10 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.10381v1</id>
    <updated>2025-02-14T18:57:16Z</updated>
    <published>2025-02-14T18:57:16Z</published>
    <title>Balancing the Scales: A Theoretical and Algorithmic Framework for
  Learning from Imbalanced Data</title>
    <summary>  Class imbalance remains a major challenge in machine learning, especially in
multi-class problems with long-tailed distributions. Existing methods, such as
data resampling, cost-sensitive techniques, and logistic loss modifications,
though popular and often effective, lack solid theoretical foundations. As an
example, we demonstrate that cost-sensitive methods are not Bayes consistent.
This paper introduces a novel theoretical framework for analyzing
generalization in imbalanced classification. We propose a new class-imbalanced
margin loss function for both binary and multi-class settings, prove its strong
$H$-consistency, and derive corresponding learning guarantees based on
empirical loss and a new notion of class-sensitive Rademacher complexity.
Leveraging these theoretical results, we devise novel and general learning
algorithms, IMMAX (Imbalanced Margin Maximization), which incorporate
confidence margins and are applicable to various hypothesis sets. While our
focus is theoretical, we also present extensive empirical results demonstrating
the effectiveness of our algorithms compared to existing baselines.
</summary>
    <author>
      <name>Corinna Cortes</name>
    </author>
    <author>
      <name>Anqi Mao</name>
    </author>
    <author>
      <name>Mehryar Mohri</name>
    </author>
    <author>
      <name>Yutao Zhong</name>
    </author>
    <link href="http://arxiv.org/abs/2502.10381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.10381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
