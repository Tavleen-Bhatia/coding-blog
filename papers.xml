<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-06-05T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">431891</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2506.04227v1</id>
    <updated>2025-06-04T17:59:06Z</updated>
    <published>2025-06-04T17:59:06Z</published>
    <title>Object-centric 3D Motion Field for Robot Learning from Human Videos</title>
    <summary>  Learning robot control policies from human videos is a promising direction
for scaling up robot learning. However, how to extract action knowledge (or
action representations) from videos for policy learning remains a key
challenge. Existing action representations such as video frames, pixelflow, and
pointcloud flow have inherent limitations such as modeling complexity or loss
of information. In this paper, we propose to use object-centric 3D motion field
to represent actions for robot learning from human videos, and present a novel
framework for extracting this representation from videos for zero-shot control.
We introduce two novel components in its implementation. First, a novel
training pipeline for training a ''denoising'' 3D motion field estimator to
extract fine object 3D motions from human videos with noisy depth robustly.
Second, a dense object-centric 3D motion field prediction architecture that
favors both cross-embodiment transfer and policy generalization to background.
We evaluate the system in real world setups. Experiments show that our method
reduces 3D motion estimation error by over 50% compared to the latest method,
achieve 55% average success rate in diverse tasks where prior approaches
fail~($\lesssim 10$\%), and can even acquire fine-grained manipulation skills
like insertion.
</summary>
    <author>
      <name>Zhao-Heng Yin</name>
    </author>
    <author>
      <name>Sherry Yang</name>
    </author>
    <author>
      <name>Pieter Abbeel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project: https://zhaohengyin.github.io/3DMF</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.04227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.04227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.04218v1</id>
    <updated>2025-06-04T17:57:53Z</updated>
    <published>2025-06-04T17:57:53Z</published>
    <title>Pseudo-Simulation for Autonomous Driving</title>
    <summary>  Existing evaluation paradigms for Autonomous Vehicles (AVs) face critical
limitations. Real-world evaluation is often challenging due to safety concerns
and a lack of reproducibility, whereas closed-loop simulation can face
insufficient realism or high computational costs. Open-loop evaluation, while
being efficient and data-driven, relies on metrics that generally overlook
compounding errors. In this paper, we propose pseudo-simulation, a novel
paradigm that addresses these limitations. Pseudo-simulation operates on real
datasets, similar to open-loop evaluation, but augments them with synthetic
observations generated prior to evaluation using 3D Gaussian Splatting. Our key
idea is to approximate potential future states the AV might encounter by
generating a diverse set of observations that vary in position, heading, and
speed. Our method then assigns a higher importance to synthetic observations
that best match the AV's likely behavior using a novel proximity-based
weighting scheme. This enables evaluating error recovery and the mitigation of
causal confusion, as in closed-loop benchmarks, without requiring sequential
interactive simulation. We show that pseudo-simulation is better correlated
with closed-loop simulations (R^2=0.8) than the best existing open-loop
approach (R^2=0.7). We also establish a public leaderboard for the community to
benchmark new methodologies with pseudo-simulation. Our code is available at
https://github.com/autonomousvision/navsim.
</summary>
    <author>
      <name>Wei Cao</name>
    </author>
    <author>
      <name>Marcel Hallgarten</name>
    </author>
    <author>
      <name>Tianyu Li</name>
    </author>
    <author>
      <name>Daniel Dauner</name>
    </author>
    <author>
      <name>Xunjiang Gu</name>
    </author>
    <author>
      <name>Caojun Wang</name>
    </author>
    <author>
      <name>Yakov Miron</name>
    </author>
    <author>
      <name>Marco Aiello</name>
    </author>
    <author>
      <name>Hongyang Li</name>
    </author>
    <author>
      <name>Igor Gilitschenski</name>
    </author>
    <author>
      <name>Boris Ivanovic</name>
    </author>
    <author>
      <name>Marco Pavone</name>
    </author>
    <author>
      <name>Andreas Geiger</name>
    </author>
    <author>
      <name>Kashyap Chitta</name>
    </author>
    <link href="http://arxiv.org/abs/2506.04218v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.04218v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.04215v1</id>
    <updated>2025-06-04T17:57:30Z</updated>
    <published>2025-06-04T17:57:30Z</published>
    <title>Thinking Beyond Visibility: A Near-Optimal Policy Framework for Locally
  Interdependent Multi-Agent MDPs</title>
    <summary>  Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs) are
known to be NEXP-Complete and intractable to solve. However, for problems such
as cooperative navigation, obstacle avoidance, and formation control, basic
assumptions can be made about local visibility and local dependencies. The work
DeWeese and Qu 2024 formalized these assumptions in the construction of the
Locally Interdependent Multi-Agent MDP. In this setting, it establishes three
closed-form policies that are tractable to compute in various situations and
are exponentially close to optimal with respect to visibility. However, it is
also shown that these solutions can have poor performance when the visibility
is small and fixed, often getting stuck during simulations due to the so called
"Penalty Jittering" phenomenon. In this work, we establish the Extended Cutoff
Policy Class which is, to the best of our knowledge, the first non-trivial
class of near optimal closed-form partially observable policies that are
exponentially close to optimal with respect to the visibility for any Locally
Interdependent Multi-Agent MDP. These policies are able to remember agents
beyond their visibilities which allows them to perform significantly better in
many small and fixed visibility settings, resolve Penalty Jittering
occurrences, and under certain circumstances guarantee fully observable joint
optimal behavior despite the partial observability. We also propose a
generalized form of the Locally Interdependent Multi-Agent MDP that allows for
transition dependence and extended reward dependence, then replicate our
theoretical results in this setting.
</summary>
    <author>
      <name>Alex DeWeese</name>
    </author>
    <author>
      <name>Guannan Qu</name>
    </author>
    <link href="http://arxiv.org/abs/2506.04215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.04215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.04214v1</id>
    <updated>2025-06-04T17:57:26Z</updated>
    <published>2025-06-04T17:57:26Z</published>
    <title>Sounding that Object: Interactive Object-Aware Image to Audio Generation</title>
    <summary>  Generating accurate sounds for complex audio-visual scenes is challenging,
especially in the presence of multiple objects and sound sources. In this
paper, we propose an {\em interactive object-aware audio generation} model that
grounds sound generation in user-selected visual objects within images. Our
method integrates object-centric learning into a conditional latent diffusion
model, which learns to associate image regions with their corresponding sounds
through multi-modal attention. At test time, our model employs image
segmentation to allow users to interactively generate sounds at the {\em
object} level. We theoretically validate that our attention mechanism
functionally approximates test-time segmentation masks, ensuring the generated
audio aligns with selected objects. Quantitative and qualitative evaluations
show that our model outperforms baselines, achieving better alignment between
objects and their associated sounds. Project page:
https://tinglok.netlify.app/files/avobject/
</summary>
    <author>
      <name>Tingle Li</name>
    </author>
    <author>
      <name>Baihe Huang</name>
    </author>
    <author>
      <name>Xiaobin Zhuang</name>
    </author>
    <author>
      <name>Dongya Jia</name>
    </author>
    <author>
      <name>Jiawei Chen</name>
    </author>
    <author>
      <name>Yuping Wang</name>
    </author>
    <author>
      <name>Zhuo Chen</name>
    </author>
    <author>
      <name>Gopala Anumanchipalli</name>
    </author>
    <author>
      <name>Yuxuan Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.04214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.04214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.04211v1</id>
    <updated>2025-06-04T17:56:46Z</updated>
    <published>2025-06-04T17:56:46Z</published>
    <title>Diffusion Domain Teacher: Diffusion Guided Domain Adaptive Object
  Detector</title>
    <summary>  Object detectors often suffer a decrease in performance due to the large
domain gap between the training data (source domain) and real-world data
(target domain). Diffusion-based generative models have shown remarkable
abilities in generating high-quality and diverse images, suggesting their
potential for extracting valuable feature from various domains. To effectively
leverage the cross-domain feature representation of diffusion models, in this
paper, we train a detector with frozen-weight diffusion model on the source
domain, then employ it as a teacher model to generate pseudo labels on the
unlabeled target domain, which are used to guide the supervised learning of the
student model on the target domain. We refer to this approach as Diffusion
Domain Teacher (DDT). By employing this straightforward yet potent framework,
we significantly improve cross-domain object detection performance without
compromising the inference speed. Our method achieves an average mAP
improvement of 21.2% compared to the baseline on 6 datasets from three common
cross-domain detection benchmarks (Cross-Camera, Syn2Real, Real2Artistic},
surpassing the current state-of-the-art (SOTA) methods by an average of 5.7%
mAP. Furthermore, extensive experiments demonstrate that our method
consistently brings improvements even in more powerful and complex models,
highlighting broadly applicable and effective domain adaptation capability of
our DDT. The code is available at
https://github.com/heboyong/Diffusion-Domain-Teacher.
</summary>
    <author>
      <name>Boyong He</name>
    </author>
    <author>
      <name>Yuxiang Ji</name>
    </author>
    <author>
      <name>Zhuoyue Tan</name>
    </author>
    <author>
      <name>Liaoni Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MM2024 poster, with appendix and codes</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.04211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.04211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
