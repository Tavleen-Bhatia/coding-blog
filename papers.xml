<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-04-21T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">419735</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.13837v1</id>
    <updated>2025-04-18T17:59:56Z</updated>
    <published>2025-04-18T17:59:56Z</published>
    <title>Does Reinforcement Learning Really Incentivize Reasoning Capacity in
  LLMs Beyond the Base Model?</title>
    <summary>  Reinforcement Learning with Verifiable Rewards (RLVR) has recently
demonstrated notable success in enhancing the reasoning capabilities of LLMs,
particularly in mathematics and programming tasks. It is widely believed that
RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning
abilities that exceed corresponding base models' capacity. In this study,
however, we critically re-examines this assumption by measuring the
pass@\textit{k} metric with large values of \textit{k} to explore the reasoning
capability boundary of the models across a wide range of model families and
benchmarks. Surprisingly, the RL does \emph{not}, in fact, elicit fundamentally
new reasoning patterns. While RL-trained models outperform their base models at
smaller values of $k$ (\eg, $k$=1), base models can achieve a comparable or
even higher pass@$k$ score compared to their RL counterparts at large $k$
values. The reasoning paths generated by RL-trained models are already included
in the base models' sampling distribution, suggesting that most reasoning
abilities manifested in RL-trained models are already obtained by base models.
Further analysis shows that RL training boosts the performance by biasing the
model's output distribution toward paths that are more likely to yield rewards,
therefore sampling correct responses more efficiently. But this also results in
a narrower reasoning capability boundary compared to base models. Similar
results are observed in visual reasoning tasks trained with RLVR. Moreover, we
find that distillation can genuinely introduce new knowledge into the model,
different from RLVR. These findings underscore a critical limitation of RLVR in
advancing LLM reasoning abilities which requires us to fundamentally rethink
the impact of RL training in reasoning LLMs and the need of a better paradigm.
Project Page: https://limit-of-RLVR.github.io
</summary>
    <author>
      <name>Yang Yue</name>
    </author>
    <author>
      <name>Zhiqi Chen</name>
    </author>
    <author>
      <name>Rui Lu</name>
    </author>
    <author>
      <name>Andrew Zhao</name>
    </author>
    <author>
      <name>Zhaokai Wang</name>
    </author>
    <author>
      <name>Yang Yue</name>
    </author>
    <author>
      <name>Shiji Song</name>
    </author>
    <author>
      <name>Gao Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.13837v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.13837v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.13825v1</id>
    <updated>2025-04-18T17:54:33Z</updated>
    <published>2025-04-18T17:54:33Z</published>
    <title>Feature Alignment and Representation Transfer in Knowledge Distillation
  for Large Language Models</title>
    <summary>  Knowledge distillation (KD) is a technique for transferring knowledge from
complex teacher models to simpler student models, significantly enhancing model
efficiency and accuracy. It has demonstrated substantial advancements in
various applications including image classification, object detection, language
modeling, text classification, and sentiment analysis. Recent innovations in KD
methods, such as attention-based approaches, block-wise logit distillation, and
decoupling distillation, have notably improved student model performance. These
techniques focus on stimulus complexity, attention mechanisms, and global
information capture to optimize knowledge transfer. In addition, KD has proven
effective in compressing large language models while preserving accuracy,
reducing computational overhead, and improving inference speed. This survey
synthesizes the latest literature, highlighting key findings, contributions,
and future directions in knowledge distillation to provide insights for
researchers and practitioners on its evolving role in artificial intelligence
and machine learning.
</summary>
    <author>
      <name>Junjie Yang</name>
    </author>
    <author>
      <name>Junhao Song</name>
    </author>
    <author>
      <name>Xudong Han</name>
    </author>
    <author>
      <name>Ziqian Bi</name>
    </author>
    <author>
      <name>Tianyang Wang</name>
    </author>
    <author>
      <name>Chia Xin Liang</name>
    </author>
    <author>
      <name>Xinyuan Song</name>
    </author>
    <author>
      <name>Yichao Zhang</name>
    </author>
    <author>
      <name>Qian Niu</name>
    </author>
    <author>
      <name>Benji Peng</name>
    </author>
    <author>
      <name>Keyu Chen</name>
    </author>
    <author>
      <name>Ming Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2504.13825v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.13825v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.13822v1</id>
    <updated>2025-04-18T17:51:51Z</updated>
    <published>2025-04-18T17:51:51Z</published>
    <title>Parameter-Efficient Continual Fine-Tuning: A Survey</title>
    <summary>  The emergence of large pre-trained networks has revolutionized the AI field,
unlocking new possibilities and achieving unprecedented performance. However,
these models inherit a fundamental limitation from traditional Machine Learning
approaches: their strong dependence on the \textit{i.i.d.} assumption hinders
their adaptability to dynamic learning scenarios. We believe the next
breakthrough in AI lies in enabling efficient adaptation to evolving
environments -- such as the real world -- where new data and tasks arrive
sequentially. This challenge defines the field of Continual Learning (CL), a
Machine Learning paradigm focused on developing lifelong learning neural
models. One alternative to efficiently adapt these large-scale models is known
Parameter-Efficient Fine-Tuning (PEFT). These methods tackle the issue of
adapting the model to a particular data or scenario by performing small and
efficient modifications, achieving similar performance to full fine-tuning.
However, these techniques still lack the ability to adjust the model to
multiple tasks continually, as they suffer from the issue of Catastrophic
Forgetting. In this survey, we first provide an overview of CL algorithms and
PEFT methods before reviewing the state-of-the-art on Parameter-Efficient
Continual Fine-Tuning (PECFT). We examine various approaches, discuss
evaluation metrics, and explore potential future research directions. Our goal
is to highlight the synergy between CL and Parameter-Efficient Fine-Tuning,
guide researchers in this field, and pave the way for novel future research
directions.
</summary>
    <author>
      <name>Eric Nuertey Coleman</name>
    </author>
    <author>
      <name>Luigi Quarantiello</name>
    </author>
    <author>
      <name>Ziyue Liu</name>
    </author>
    <author>
      <name>Qinwen Yang</name>
    </author>
    <author>
      <name>Samrat Mukherjee</name>
    </author>
    <author>
      <name>Julio Hurtado</name>
    </author>
    <author>
      <name>Vincenzo Lomonaco</name>
    </author>
    <link href="http://arxiv.org/abs/2504.13822v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.13822v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.13820v1</id>
    <updated>2025-04-18T17:50:43Z</updated>
    <published>2025-04-18T17:50:43Z</published>
    <title>CheXWorld: Exploring Image World Modeling for Radiograph Representation
  Learning</title>
    <summary>  Humans can develop internal world models that encode common sense knowledge,
telling them how the world works and predicting the consequences of their
actions. This concept has emerged as a promising direction for establishing
general-purpose machine-learning models in recent preliminary works, e.g., for
visual representation learning. In this paper, we present CheXWorld, the first
effort towards a self-supervised world model for radiographic images.
Specifically, our work develops a unified framework that simultaneously models
three aspects of medical knowledge essential for qualified radiologists,
including 1) local anatomical structures describing the fine-grained
characteristics of local tissues (e.g., architectures, shapes, and textures);
2) global anatomical layouts describing the global organization of the human
body (e.g., layouts of organs and skeletons); and 3) domain variations that
encourage CheXWorld to model the transitions across different appearance
domains of radiographs (e.g., varying clarity, contrast, and exposure caused by
collecting radiographs from different hospitals, devices, or patients).
Empirically, we design tailored qualitative and quantitative analyses,
revealing that CheXWorld successfully captures these three dimensions of
medical knowledge. Furthermore, transfer learning experiments across eight
medical image classification and segmentation benchmarks showcase that
CheXWorld significantly outperforms existing SSL methods and large-scale
medical foundation models. Code &amp; pre-trained models are available at
https://github.com/LeapLabTHU/CheXWorld.
</summary>
    <author>
      <name>Yang Yue</name>
    </author>
    <author>
      <name>Yulin Wang</name>
    </author>
    <author>
      <name>Chenxin Tao</name>
    </author>
    <author>
      <name>Pan Liu</name>
    </author>
    <author>
      <name>Shiji Song</name>
    </author>
    <author>
      <name>Gao Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by CVPR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.13820v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.13820v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.13818v1</id>
    <updated>2025-04-18T17:49:55Z</updated>
    <published>2025-04-18T17:49:55Z</published>
    <title>Not All Rollouts are Useful: Down-Sampling Rollouts in LLM Reinforcement
  Learning</title>
    <summary>  Reinforcement learning (RL) has emerged as a powerful paradigm for enhancing
reasoning capabilities in large language models, but faces a fundamental
asymmetry in computation and memory requirements: inference is embarrassingly
parallel with a minimal memory footprint, while policy updates require
extensive synchronization and are memory-intensive. To address this asymmetry,
we introduce PODS (Policy Optimization with Down-Sampling), a framework that
strategically decouples these phases by generating numerous rollouts in
parallel but updating only on an informative subset. Within this framework, we
develop max-variance down-sampling, a theoretically motivated method that
selects rollouts with maximally diverse reward signals. We prove that this
approach has an efficient algorithmic solution, and empirically demonstrate
that GRPO with PODS using max-variance down-sampling achieves superior
performance over standard GRPO on the GSM8K benchmark.
</summary>
    <author>
      <name>Yixuan Even Xu</name>
    </author>
    <author>
      <name>Yash Savani</name>
    </author>
    <author>
      <name>Fei Fang</name>
    </author>
    <author>
      <name>Zico Kolter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.13818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.13818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
