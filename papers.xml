<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-03-21T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">413608</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.16429v1</id>
    <updated>2025-03-20T17:59:59Z</updated>
    <published>2025-03-20T17:59:59Z</published>
    <title>Sonata: Self-Supervised Learning of Reliable Point Representations</title>
    <summary>  In this paper, we question whether we have a reliable self-supervised point
cloud model that can be used for diverse 3D tasks via simple linear probing,
even with limited data and minimal computation. We find that existing 3D
self-supervised learning approaches fall short when evaluated on representation
quality through linear probing. We hypothesize that this is due to what we term
the "geometric shortcut", which causes representations to collapse to low-level
spatial features. This challenge is unique to 3D and arises from the sparse
nature of point cloud data. We address it through two key strategies: obscuring
spatial information and enhancing the reliance on input features, ultimately
composing a Sonata of 140k point clouds through self-distillation. Sonata is
simple and intuitive, yet its learned representations are strong and reliable:
zero-shot visualizations demonstrate semantic grouping, alongside strong
spatial reasoning through nearest-neighbor relationships. Sonata demonstrates
exceptional parameter and data efficiency, tripling linear probing accuracy
(from 21.8% to 72.5%) on ScanNet and nearly doubling performance with only 1%
of the data compared to previous approaches. Full fine-tuning further advances
SOTA across both 3D indoor and outdoor perception tasks.
</summary>
    <author>
      <name>Xiaoyang Wu</name>
    </author>
    <author>
      <name>Daniel DeTone</name>
    </author>
    <author>
      <name>Duncan Frost</name>
    </author>
    <author>
      <name>Tianwei Shen</name>
    </author>
    <author>
      <name>Chris Xie</name>
    </author>
    <author>
      <name>Nan Yang</name>
    </author>
    <author>
      <name>Jakob Engel</name>
    </author>
    <author>
      <name>Richard Newcombe</name>
    </author>
    <author>
      <name>Hengshuang Zhao</name>
    </author>
    <author>
      <name>Julian Straub</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2025, produced by Pointcept x Meta, project page:
  https://xywu.me/sonata/</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.16429v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16429v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16426v1</id>
    <updated>2025-03-20T17:59:54Z</updated>
    <published>2025-03-20T17:59:54Z</published>
    <title>DynamicVis: An Efficient and General Visual Foundation Model for Remote
  Sensing Image Understanding</title>
    <summary>  The advancement of remote sensing technology has improved the spatial
resolution of satellite imagery, facilitating more detailed visual
representations for diverse interpretations. However, existing methods exhibit
limited generalization capabilities across varied applications. While some
contemporary foundation models demonstrate potential, they are hindered by
insufficient cross-task adaptability and primarily process low-resolution
imagery of restricted sizes, thus failing to fully exploit high-resolution data
or leverage comprehensive large-scene semantics. Crucially, remote sensing
imagery differs fundamentally from natural images, as key foreground targets
(eg., maritime objects, artificial structures) often occupy minimal spatial
proportions (~1%) and exhibit sparse distributions. Efficiently modeling
cross-task generalizable knowledge from lengthy 2D tokens (~100,000) poses a
significant challenge yet remains critical for remote sensing image
understanding. Motivated by the selective attention mechanisms inherent to the
human visual system, we propose DynamicVis, a dynamic visual perception
foundation model for remote sensing imagery. The framework integrates a novel
dynamic region perception backbone based on the selective state space model,
which strategically balances localized detail extraction with global contextual
integration, enabling computationally efficient encoding of large-scale data
while maintaining architectural scalability. To enhance cross-task knowledge
transferring, we introduce a multi-instance learning paradigm utilizing
meta-embedding representations, trained on million-scale region-level
annotations. Evaluations across nine downstream tasks demonstrate the model's
versatility. DynamicVis achieves multi-level feature modeling with exceptional
efficiency, processing (2048x2048) pixels with 97 ms latency (6% of ViT's) and
833 MB GPU memory (3% of ViT's).
</summary>
    <author>
      <name>Keyan Chen</name>
    </author>
    <author>
      <name>Chenyang Liu</name>
    </author>
    <author>
      <name>Bowen Chen</name>
    </author>
    <author>
      <name>Wenyuan Li</name>
    </author>
    <author>
      <name>Zhengxia Zou</name>
    </author>
    <author>
      <name>Zhenwei Shi</name>
    </author>
    <link href="http://arxiv.org/abs/2503.16426v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16426v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16423v1</id>
    <updated>2025-03-20T17:59:47Z</updated>
    <published>2025-03-20T17:59:47Z</published>
    <title>GAEA: A Geolocation Aware Conversational Model</title>
    <summary>  Image geolocalization, in which, traditionally, an AI model predicts the
precise GPS coordinates of an image is a challenging task with many downstream
applications. However, the user cannot utilize the model to further their
knowledge other than the GPS coordinate; the model lacks an understanding of
the location and the conversational ability to communicate with the user. In
recent days, with tremendous progress of large multimodal models (LMMs)
proprietary and open-source researchers have attempted to geolocalize images
via LMMs. However, the issues remain unaddressed; beyond general tasks, for
more specialized downstream tasks, one of which is geolocalization, LMMs
struggle. In this work, we propose to solve this problem by introducing a
conversational model GAEA that can provide information regarding the location
of an image, as required by a user. No large-scale dataset enabling the
training of such a model exists. Thus we propose a comprehensive dataset GAEA
with 800K images and around 1.6M question answer pairs constructed by
leveraging OpenStreetMap (OSM) attributes and geographical context clues. For
quantitative evaluation, we propose a diverse benchmark comprising 4K
image-text pairs to evaluate conversational capabilities equipped with diverse
question types. We consider 11 state-of-the-art open-source and proprietary
LMMs and demonstrate that GAEA significantly outperforms the best open-source
model, LLaVA-OneVision by 25.69% and the best proprietary model, GPT-4o by
8.28%. Our dataset, model and codes are available
</summary>
    <author>
      <name>Ron Campos</name>
    </author>
    <author>
      <name>Ashmal Vayani</name>
    </author>
    <author>
      <name>Parth Parag Kulkarni</name>
    </author>
    <author>
      <name>Rohit Gupta</name>
    </author>
    <author>
      <name>Aritra Dutta</name>
    </author>
    <author>
      <name>Mubarak Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The dataset and code used in this submission is available at:
  https://ucf-crcv.github.io/GAEA/</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.16423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4; I.2.7; I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16421v1</id>
    <updated>2025-03-20T17:59:42Z</updated>
    <published>2025-03-20T17:59:42Z</published>
    <title>MagicMotion: Controllable Video Generation with Dense-to-Sparse
  Trajectory Guidance</title>
    <summary>  Recent advances in video generation have led to remarkable improvements in
visual quality and temporal coherence. Upon this, trajectory-controllable video
generation has emerged to enable precise object motion control through
explicitly defined spatial paths. However, existing methods struggle with
complex object movements and multi-object motion control, resulting in
imprecise trajectory adherence, poor object consistency, and compromised visual
quality. Furthermore, these methods only support trajectory control in a single
format, limiting their applicability in diverse scenarios. Additionally, there
is no publicly available dataset or benchmark specifically tailored for
trajectory-controllable video generation, hindering robust training and
systematic evaluation. To address these challenges, we introduce MagicMotion, a
novel image-to-video generation framework that enables trajectory control
through three levels of conditions from dense to sparse: masks, bounding boxes,
and sparse boxes. Given an input image and trajectories, MagicMotion seamlessly
animates objects along defined trajectories while maintaining object
consistency and visual quality. Furthermore, we present MagicData, a
large-scale trajectory-controlled video dataset, along with an automated
pipeline for annotation and filtering. We also introduce MagicBench, a
comprehensive benchmark that assesses both video quality and trajectory control
accuracy across different numbers of objects. Extensive experiments demonstrate
that MagicMotion outperforms previous methods across various metrics. Our
project page are publicly available at
https://quanhaol.github.io/magicmotion-site.
</summary>
    <author>
      <name>Quanhao Li</name>
    </author>
    <author>
      <name>Zhen Xing</name>
    </author>
    <author>
      <name>Rui Wang</name>
    </author>
    <author>
      <name>Hui Zhang</name>
    </author>
    <author>
      <name>Qi Dai</name>
    </author>
    <author>
      <name>Zuxuan Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2503.16421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.16419v1</id>
    <updated>2025-03-20T17:59:38Z</updated>
    <published>2025-03-20T17:59:38Z</published>
    <title>Stop Overthinking: A Survey on Efficient Reasoning for Large Language
  Models</title>
    <summary>  Large Language Models (LLMs) have demonstrated remarkable capabilities in
complex tasks. Recent advancements in Large Reasoning Models (LRMs), such as
OpenAI o1 and DeepSeek-R1, have further improved performance in System-2
reasoning domains like mathematics and programming by harnessing supervised
fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance the
Chain-of-Thought (CoT) reasoning. However, while longer CoT reasoning sequences
improve performance, they also introduce significant computational overhead due
to verbose and redundant outputs, known as the "overthinking phenomenon". In
this paper, we provide the first structured survey to systematically
investigate and explore the current progress toward achieving efficient
reasoning in LLMs. Overall, relying on the inherent mechanism of LLMs, we
categorize existing works into several key directions: (1) model-based
efficient reasoning, which considers optimizing full-length reasoning models
into more concise reasoning models or directly training efficient reasoning
models; (2) reasoning output-based efficient reasoning, which aims to
dynamically reduce reasoning steps and length during inference; (3) input
prompts-based efficient reasoning, which seeks to enhance reasoning efficiency
based on input prompt properties such as difficulty or length control.
Additionally, we introduce the use of efficient data for training reasoning
models, explore the reasoning capabilities of small language models, and
discuss evaluation methods and benchmarking.
</summary>
    <author>
      <name>Yang Sui</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name>Yu-Neng Chuang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name>Guanchu Wang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name>Jiamu Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name>Tianyi Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name>Jiayi Yuan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name>Hongyi Liu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Wen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name> Shaochen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Henry</arxiv:affiliation>
    </author>
    <author>
      <name> Zhong</name>
    </author>
    <author>
      <name>Hanjie Chen</name>
    </author>
    <author>
      <name>Xia Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Website:
  https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.16419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.16419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
