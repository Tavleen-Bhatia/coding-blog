<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-02-19T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">405924</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2502.13146v1</id>
    <updated>2025-02-18T18:59:57Z</updated>
    <published>2025-02-18T18:59:57Z</published>
    <title>Re-Align: Aligning Vision Language Models via Retrieval-Augmented Direct
  Preference Optimization</title>
    <summary>  The emergence of large Vision Language Models (VLMs) has broadened the scope
and capabilities of single-modal Large Language Models (LLMs) by integrating
visual modalities, thereby unlocking transformative cross-modal applications in
a variety of real-world scenarios. Despite their impressive performance, VLMs
are prone to significant hallucinations, particularly in the form of
cross-modal inconsistencies. Building on the success of Reinforcement Learning
from Human Feedback (RLHF) in aligning LLMs, recent advancements have focused
on applying direct preference optimization (DPO) on carefully curated datasets
to mitigate these issues. Yet, such approaches typically introduce preference
signals in a brute-force manner, neglecting the crucial role of visual
information in the alignment process. In this paper, we introduce Re-Align, a
novel alignment framework that leverages image retrieval to construct a
dual-preference dataset, effectively incorporating both textual and visual
preference signals. We further introduce rDPO, an extension of the standard
direct preference optimization that incorporates an additional visual
preference objective during fine-tuning. Our experimental results demonstrate
that Re-Align not only mitigates hallucinations more effectively than previous
methods but also yields significant performance gains in general visual
question-answering (VQA) tasks. Moreover, we show that Re-Align maintains
robustness and scalability across a wide range of VLM sizes and architectures.
This work represents a significant step forward in aligning multimodal LLMs,
paving the way for more reliable and effective cross-modal applications. We
release all the code in https://github.com/taco-group/Re-Align.
</summary>
    <author>
      <name>Shuo Xing</name>
    </author>
    <author>
      <name>Yuping Wang</name>
    </author>
    <author>
      <name>Peiran Li</name>
    </author>
    <author>
      <name>Ruizheng Bai</name>
    </author>
    <author>
      <name>Yueqi Wang</name>
    </author>
    <author>
      <name>Chengxuan Qian</name>
    </author>
    <author>
      <name>Huaxiu Yao</name>
    </author>
    <author>
      <name>Zhengzhong Tu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.13146v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.13146v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.13144v1</id>
    <updated>2025-02-18T18:59:21Z</updated>
    <published>2025-02-18T18:59:21Z</published>
    <title>RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based
  Reinforcement Learning</title>
    <summary>  Existing end-to-end autonomous driving (AD) algorithms typically follow the
Imitation Learning (IL) paradigm, which faces challenges such as causal
confusion and the open-loop gap. In this work, we establish a 3DGS-based
closed-loop Reinforcement Learning (RL) training paradigm. By leveraging 3DGS
techniques, we construct a photorealistic digital replica of the real physical
world, enabling the AD policy to extensively explore the state space and learn
to handle out-of-distribution scenarios through large-scale trial and error. To
enhance safety, we design specialized rewards that guide the policy to
effectively respond to safety-critical events and understand real-world causal
relationships. For better alignment with human driving behavior, IL is
incorporated into RL training as a regularization term. We introduce a
closed-loop evaluation benchmark consisting of diverse, previously unseen 3DGS
environments. Compared to IL-based methods, RAD achieves stronger performance
in most closed-loop metrics, especially 3x lower collision rate. Abundant
closed-loop results are presented at https://hgao-cv.github.io/RAD.
</summary>
    <author>
      <name>Hao Gao</name>
    </author>
    <author>
      <name>Shaoyu Chen</name>
    </author>
    <author>
      <name>Bo Jiang</name>
    </author>
    <author>
      <name>Bencheng Liao</name>
    </author>
    <author>
      <name>Yiang Shi</name>
    </author>
    <author>
      <name>Xiaoyang Guo</name>
    </author>
    <author>
      <name>Yuechuan Pu</name>
    </author>
    <author>
      <name>Haoran Yin</name>
    </author>
    <author>
      <name>Xiangyu Li</name>
    </author>
    <author>
      <name>Xinbang Zhang</name>
    </author>
    <author>
      <name>Ying Zhang</name>
    </author>
    <author>
      <name>Wenyu Liu</name>
    </author>
    <author>
      <name>Qian Zhang</name>
    </author>
    <author>
      <name>Xinggang Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://hgao-cv.github.io/RAD</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.13144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.13144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.13142v1</id>
    <updated>2025-02-18T18:59:01Z</updated>
    <published>2025-02-18T18:59:01Z</published>
    <title>Pre-training Auto-regressive Robotic Models with 4D Representations</title>
    <summary>  Foundation models pre-trained on massive unlabeled datasets have
revolutionized natural language and computer vision, exhibiting remarkable
generalization capabilities, thus highlighting the importance of pre-training.
Yet, efforts in robotics have struggled to achieve similar success, limited by
either the need for costly robotic annotations or the lack of representations
that effectively model the physical world. In this paper, we introduce ARM4R,
an Auto-regressive Robotic Model that leverages low-level 4D Representations
learned from human video data to yield a better pre-trained robotic model.
Specifically, we focus on utilizing 3D point tracking representations from
videos derived by lifting 2D representations into 3D space via monocular depth
estimation across time. These 4D representations maintain a shared geometric
structure between the points and robot state representations up to a linear
transformation, enabling efficient transfer learning from human video data to
low-level robotic control. Our experiments show that ARM4R can transfer
efficiently from human video data to robotics and consistently improves
performance on tasks across various robot environments and configurations.
</summary>
    <author>
      <name>Dantong Niu</name>
    </author>
    <author>
      <name>Yuvan Sharma</name>
    </author>
    <author>
      <name>Haoru Xue</name>
    </author>
    <author>
      <name>Giscard Biamby</name>
    </author>
    <author>
      <name>Junyi Zhang</name>
    </author>
    <author>
      <name>Ziteng Ji</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>Roei Herzig</name>
    </author>
    <link href="http://arxiv.org/abs/2502.13142v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.13142v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.13141v1</id>
    <updated>2025-02-18T18:59:00Z</updated>
    <published>2025-02-18T18:59:00Z</published>
    <title>UniGuardian: A Unified Defense for Detecting Prompt Injection, Backdoor
  Attacks and Adversarial Attacks in Large Language Models</title>
    <summary>  Large Language Models (LLMs) are vulnerable to attacks like prompt injection,
backdoor attacks, and adversarial attacks, which manipulate prompts or models
to generate harmful outputs. In this paper, departing from traditional deep
learning attack paradigms, we explore their intrinsic relationship and
collectively term them Prompt Trigger Attacks (PTA). This raises a key
question: Can we determine if a prompt is benign or poisoned? To address this,
we propose UniGuardian, the first unified defense mechanism designed to detect
prompt injection, backdoor attacks, and adversarial attacks in LLMs.
Additionally, we introduce a single-forward strategy to optimize the detection
pipeline, enabling simultaneous attack detection and text generation within a
single forward pass. Our experiments confirm that UniGuardian accurately and
efficiently identifies malicious prompts in LLMs.
</summary>
    <author>
      <name>Huawei Lin</name>
    </author>
    <author>
      <name>Yingjie Lao</name>
    </author>
    <author>
      <name>Tong Geng</name>
    </author>
    <author>
      <name>Tan Yu</name>
    </author>
    <author>
      <name>Weijie Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 Pages, 8 Figures, 5 Tables, Keywords: Attack Defending, Security,
  Prompt Injection, Backdoor Attacks, Adversarial Attacks, Prompt Trigger
  Attacks</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.13141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.13141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.13140v1</id>
    <updated>2025-02-18T18:58:35Z</updated>
    <published>2025-02-18T18:58:35Z</published>
    <title>Towards Quantum Tensor Decomposition in Biomedical Applications</title>
    <summary>  Tensor decomposition has emerged as a powerful framework for feature
extraction in multi-modal biomedical data. In this review, we present a
comprehensive analysis of tensor decomposition methods such as Tucker,
CANDECOMP/PARAFAC, spiked tensor decomposition, etc. and their diverse
applications across biomedical domains such as imaging, multi-omics, and
spatial transcriptomics. To systematically investigate the literature, we
applied a topic modeling-based approach that identifies and groups distinct
thematic sub-areas in biomedicine where tensor decomposition has been used,
thereby revealing key trends and research directions. We evaluated challenges
related to the scalability of latent spaces along with obtaining the optimal
rank of the tensor, which often hinder the extraction of meaningful features
from increasingly large and complex datasets. Additionally, we discuss recent
advances in quantum algorithms for tensor decomposition, exploring how quantum
computing can be leveraged to address these challenges. Our study includes a
preliminary resource estimation analysis for quantum computing platforms and
examines the feasibility of implementing quantum-enhanced tensor decomposition
methods on near-term quantum devices. Collectively, this review not only
synthesizes current applications and challenges of tensor decomposition in
biomedical analyses but also outlines promising quantum computing strategies to
enhance its impact on deriving actionable insights from complex biomedical
data.
</summary>
    <author>
      <name>Myson Burch</name>
    </author>
    <author>
      <name>Jiasen Zhang</name>
    </author>
    <author>
      <name>Gideon Idumah</name>
    </author>
    <author>
      <name>Hakan Doga</name>
    </author>
    <author>
      <name>Richard Lartey</name>
    </author>
    <author>
      <name>Lamis Yehia</name>
    </author>
    <author>
      <name>Mingrui Yang</name>
    </author>
    <author>
      <name>Murat Yildirim</name>
    </author>
    <author>
      <name>Mihriban Karaayvaz</name>
    </author>
    <author>
      <name>Omar Shehab</name>
    </author>
    <author>
      <name>Weihong Guo</name>
    </author>
    <author>
      <name>Ying Ni</name>
    </author>
    <author>
      <name>Laxmi Parida</name>
    </author>
    <author>
      <name>Xiaojuan Li</name>
    </author>
    <author>
      <name>Aritra Bose</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.13140v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.13140v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
