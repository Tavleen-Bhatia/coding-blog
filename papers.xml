<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-02-20T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">406166</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2502.13967v1</id>
    <updated>2025-02-19T18:59:44Z</updated>
    <published>2025-02-19T18:59:44Z</published>
    <title>FlexTok: Resampling Images into 1D Token Sequences of Flexible Length</title>
    <summary>  Image tokenization has enabled major advances in autoregressive image
generation by providing compressed, discrete representations that are more
efficient to process than raw pixels. While traditional approaches use 2D grid
tokenization, recent methods like TiTok have shown that 1D tokenization can
achieve high generation quality by eliminating grid redundancies. However,
these methods typically use a fixed number of tokens and thus cannot adapt to
an image's inherent complexity. We introduce FlexTok, a tokenizer that projects
2D images into variable-length, ordered 1D token sequences. For example, a
256x256 image can be resampled into anywhere from 1 to 256 discrete tokens,
hierarchically and semantically compressing its information. By training a
rectified flow model as the decoder and using nested dropout, FlexTok produces
plausible reconstructions regardless of the chosen token sequence length. We
evaluate our approach in an autoregressive generation setting using a simple
GPT-style Transformer. On ImageNet, this approach achieves an FID&lt;2 across 8 to
128 tokens, outperforming TiTok and matching state-of-the-art methods with far
fewer tokens. We further extend the model to support to text-conditioned image
generation and examine how FlexTok relates to traditional 2D tokenization. A
key finding is that FlexTok enables next-token prediction to describe images in
a coarse-to-fine "visual vocabulary", and that the number of tokens to generate
depends on the complexity of the generation task.
</summary>
    <author>
      <name>Roman Bachmann</name>
    </author>
    <author>
      <name>Jesse Allardice</name>
    </author>
    <author>
      <name>David Mizrahi</name>
    </author>
    <author>
      <name>Enrico Fini</name>
    </author>
    <author>
      <name>OÄŸuzhan Fatih Kar</name>
    </author>
    <author>
      <name>Elmira Amirloo</name>
    </author>
    <author>
      <name>Alaaeldin El-Nouby</name>
    </author>
    <author>
      <name>Amir Zamir</name>
    </author>
    <author>
      <name>Afshin Dehghan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page at https://flextok.epfl.ch/</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.13967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.13967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.13966v2</id>
    <updated>2025-02-20T02:29:19Z</updated>
    <published>2025-02-19T18:59:32Z</published>
    <title>Where's the Bug? Attention Probing for Scalable Fault Localization</title>
    <summary>  Ensuring code correctness remains a challenging problem even as large
language models (LLMs) become increasingly capable at code-related tasks. While
LLM-based program repair systems can propose bug fixes using only a user's bug
report, their effectiveness is fundamentally limited by their ability to
perform fault localization (FL), a challenging problem for both humans and
LLMs. Existing FL approaches rely on executable test cases, require training on
costly and often noisy line-level annotations, or demand resource-intensive
LLMs. In this paper, we present Bug Attention Probe (BAP), a method which
learns state-of-the-art fault localization without any direct localization
labels, outperforming traditional FL baselines and prompting of large-scale
LLMs. We evaluate our approach across a variety of code settings, including
real-world Java bugs from the standard Defects4J dataset as well as seven other
datasets which span a diverse set of bug types and languages. Averaged across
all eight datasets, BAP improves by 34.6% top-1 accuracy compared to the
strongest baseline and 93.4% over zero-shot prompting GPT-4o. BAP is also
significantly more efficient than prompting, outperforming large open-weight
models at a small fraction of the computational cost.
</summary>
    <author>
      <name>Adam Stein</name>
    </author>
    <author>
      <name>Arthur Wayne</name>
    </author>
    <author>
      <name>Aaditya Naik</name>
    </author>
    <author>
      <name>Mayur Naik</name>
    </author>
    <author>
      <name>Eric Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.13966v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.13966v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.13965v1</id>
    <updated>2025-02-19T18:59:30Z</updated>
    <published>2025-02-19T18:59:30Z</published>
    <title>Autellix: An Efficient Serving Engine for LLM Agents as General Programs</title>
    <summary>  Large language model (LLM) applications are evolving beyond simple chatbots
into dynamic, general-purpose agentic programs, which scale LLM calls and
output tokens to help AI agents reason, explore, and solve complex tasks.
However, existing LLM serving systems ignore dependencies between programs and
calls, missing significant opportunities for optimization. Our analysis reveals
that programs submitted to LLM serving engines experience long cumulative wait
times, primarily due to head-of-line blocking at both the individual LLM
request and the program. To address this, we introduce Autellix, an LLM serving
system that treats programs as first-class citizens to minimize their
end-to-end latencies. Autellix intercepts LLM calls submitted by programs,
enriching schedulers with program-level context. We propose two scheduling
algorithms-for single-threaded and distributed programs-that preempt and
prioritize LLM calls based on their programs' previously completed calls. Our
evaluation demonstrates that across diverse LLMs and agentic workloads,
Autellix improves throughput of programs by 4-15x at the same latency compared
to state-of-the-art systems, such as vLLM.
</summary>
    <author>
      <name>Michael Luo</name>
    </author>
    <author>
      <name>Xiaoxiang Shi</name>
    </author>
    <author>
      <name>Colin Cai</name>
    </author>
    <author>
      <name>Tianjun Zhang</name>
    </author>
    <author>
      <name>Justin Wong</name>
    </author>
    <author>
      <name>Yichuan Wang</name>
    </author>
    <author>
      <name>Chi Wang</name>
    </author>
    <author>
      <name>Yanping Huang</name>
    </author>
    <author>
      <name>Zhifeng Chen</name>
    </author>
    <author>
      <name>Joseph E. Gonzalez</name>
    </author>
    <author>
      <name>Ion Stoica</name>
    </author>
    <link href="http://arxiv.org/abs/2502.13965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.13965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.13964v1</id>
    <updated>2025-02-19T18:59:17Z</updated>
    <published>2025-02-19T18:59:17Z</published>
    <title>A Training-Free Framework for Precise Mobile Manipulation of Small
  Everyday Objects</title>
    <summary>  Many everyday mobile manipulation tasks require precise interaction with
small objects, such as grasping a knob to open a cabinet or pressing a light
switch. In this paper, we develop Servoing with Vision Models (SVM), a
closed-loop training-free framework that enables a mobile manipulator to tackle
such precise tasks involving the manipulation of small objects. SVM employs an
RGB-D wrist camera and uses visual servoing for control. Our novelty lies in
the use of state-of-the-art vision models to reliably compute 3D targets from
the wrist image for diverse tasks and under occlusion due to the end-effector.
To mitigate occlusion artifacts, we employ vision models to out-paint the
end-effector thereby significantly enhancing target localization. We
demonstrate that aided by out-painting methods, open-vocabulary object
detectors can serve as a drop-in module to identify semantic targets (e.g.
knobs) and point tracking methods can reliably track interaction sites
indicated by user clicks. This training-free method obtains an 85% zero-shot
success rate on manipulating unseen objects in novel environments in the real
world, outperforming an open-loop control method and an imitation learning
baseline trained on 1000+ demonstrations by an absolute success rate of 50%.
</summary>
    <author>
      <name>Arjun Gupta</name>
    </author>
    <author>
      <name>Rishik Sathua</name>
    </author>
    <author>
      <name>Saurabh Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project webpage: https://arjung128.github.io/svm</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.13964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.13964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.13963v1</id>
    <updated>2025-02-19T18:59:15Z</updated>
    <published>2025-02-19T18:59:15Z</published>
    <title>MuDAF: Long-Context Multi-Document Attention Focusing through
  Contrastive Learning on Attention Heads</title>
    <summary>  Large Language Models (LLMs) frequently show distracted attention due to
irrelevant information in the input, which severely impairs their long-context
capabilities. Inspired by recent studies on the effectiveness of retrieval
heads in long-context factutality, we aim at addressing this distraction issue
through improving such retrieval heads directly. We propose Multi-Document
Attention Focusing (MuDAF), a novel method that explicitly optimizes the
attention distribution at the head level through contrastive learning.
According to the experimental results, MuDAF can significantly improve the
long-context question answering performance of LLMs, especially in
multi-document question answering. Extensive evaluations on retrieval scores
and attention visualizations show that MuDAF possesses great potential in
making attention heads more focused on relevant information and reducing
attention distractions.
</summary>
    <author>
      <name>Weihao Liu</name>
    </author>
    <author>
      <name>Ning Wu</name>
    </author>
    <author>
      <name>Shiping Yang</name>
    </author>
    <author>
      <name>Wenbiao Ding</name>
    </author>
    <author>
      <name>Shining Liang</name>
    </author>
    <author>
      <name>Ming Gong</name>
    </author>
    <author>
      <name>Dongmei Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.13963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.13963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
