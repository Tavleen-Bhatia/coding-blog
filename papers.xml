<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-06-01T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">430042</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.23765v1</id>
    <updated>2025-05-29T17:59:55Z</updated>
    <published>2025-05-29T17:59:55Z</published>
    <title>From Chat Logs to Collective Insights: Aggregative Question Answering</title>
    <summary>  Conversational agents powered by large language models (LLMs) are rapidly
becoming integral to our daily interactions, generating unprecedented amounts
of conversational data. Such datasets offer a powerful lens into societal
interests, trending topics, and collective concerns. Yet, existing approaches
typically treat these interactions as independent and miss critical insights
that could emerge from aggregating and reasoning across large-scale
conversation logs. In this paper, we introduce Aggregative Question Answering,
a novel task requiring models to reason explicitly over thousands of
user-chatbot interactions to answer aggregative queries, such as identifying
emerging concerns among specific demographics. To enable research in this
direction, we construct a benchmark, WildChat-AQA, comprising 6,027 aggregative
questions derived from 182,330 real-world chatbot conversations. Experiments
show that existing methods either struggle to reason effectively or incur
prohibitive computational costs, underscoring the need for new approaches
capable of extracting collective insights from large-scale conversational data.
</summary>
    <author>
      <name>Wentao Zhang</name>
    </author>
    <author>
      <name>Woojeong Kim</name>
    </author>
    <author>
      <name>Yuntian Deng</name>
    </author>
    <link href="http://arxiv.org/abs/2505.23765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.23765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.23762v1</id>
    <updated>2025-05-29T17:59:51Z</updated>
    <published>2025-05-29T17:59:51Z</published>
    <title>ZeroGUI: Automating Online GUI Learning at Zero Human Cost</title>
    <summary>  The rapid advancement of large Vision-Language Models (VLMs) has propelled
the development of pure-vision-based GUI Agents, capable of perceiving and
operating Graphical User Interfaces (GUI) to autonomously fulfill user
instructions. However, existing approaches usually adopt an offline learning
framework, which faces two core limitations: (1) heavy reliance on high-quality
manual annotations for element grounding and action supervision, and (2)
limited adaptability to dynamic and interactive environments. To address these
limitations, we propose ZeroGUI, a scalable, online learning framework for
automating GUI Agent training at Zero human cost. Specifically, ZeroGUI
integrates (i) VLM-based automatic task generation to produce diverse training
goals from the current environment state, (ii) VLM-based automatic reward
estimation to assess task success without hand-crafted evaluation functions,
and (iii) two-stage online reinforcement learning to continuously interact with
and learn from GUI environments. Experiments on two advanced GUI Agents
(UI-TARS and Aguvis) demonstrate that ZeroGUI significantly boosts performance
across OSWorld and AndroidLab environments. The code is available at
https://github.com/OpenGVLab/ZeroGUI.
</summary>
    <author>
      <name>Chenyu Yang</name>
    </author>
    <author>
      <name>Shiqian Su</name>
    </author>
    <author>
      <name>Shi Liu</name>
    </author>
    <author>
      <name>Xuan Dong</name>
    </author>
    <author>
      <name>Yue Yu</name>
    </author>
    <author>
      <name>Weijie Su</name>
    </author>
    <author>
      <name>Xuehui Wang</name>
    </author>
    <author>
      <name>Zhaoyang Liu</name>
    </author>
    <author>
      <name>Jinguo Zhu</name>
    </author>
    <author>
      <name>Hao Li</name>
    </author>
    <author>
      <name>Wenhai Wang</name>
    </author>
    <author>
      <name>Yu Qiao</name>
    </author>
    <author>
      <name>Xizhou Zhu</name>
    </author>
    <author>
      <name>Jifeng Dai</name>
    </author>
    <link href="http://arxiv.org/abs/2505.23762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.23762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.23761v1</id>
    <updated>2025-05-29T17:59:50Z</updated>
    <published>2025-05-29T17:59:50Z</published>
    <title>Differential Information: An Information-Theoretic Perspective on
  Preference Optimization</title>
    <summary>  Direct Preference Optimization (DPO) has become a standard technique for
aligning language models with human preferences in a supervised manner. Despite
its empirical success, the theoretical justification behind its log-ratio
reward parameterization remains incomplete. In this work, we address this gap
by utilizing the Differential Information Distribution (DID): a distribution
over token sequences that captures the information gained during policy
updates. First, we show that when preference labels encode the differential
information required to transform a reference policy into a target policy, the
log-ratio reward in DPO emerges as the uniquely optimal form for learning the
target policy via preference optimization. This result naturally yields a
closed-form expression for the optimal sampling distribution over rejected
responses. Second, we find that the condition for preferences to encode
differential information is fundamentally linked to an implicit assumption
regarding log-margin ordered policies-an inductive bias widely used in
preference optimization yet previously unrecognized. Finally, by analyzing the
entropy of the DID, we characterize how learning low-entropy differential
information reinforces the policy distribution, while high-entropy differential
information induces a smoothing effect, which explains the log-likelihood
displacement phenomenon. We validate our theoretical findings in synthetic
experiments and extend them to real-world instruction-following datasets. Our
results suggest that learning high-entropy differential information is crucial
for general instruction-following, while learning low-entropy differential
information benefits knowledge-intensive question answering. Overall, our work
presents a unifying perspective on the DPO objective, the structure of
preference data, and resulting policy behaviors through the lens of
differential information.
</summary>
    <author>
      <name>Yunjae Won</name>
    </author>
    <author>
      <name>Hyunji Lee</name>
    </author>
    <author>
      <name>Hyeonbin Hwang</name>
    </author>
    <author>
      <name>Minjoon Seo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 13 figures; due to the 1,920-character limitation imposed
  on the abstract field by arXiv, the abstract included on the arXiv page is
  slightly abbreviated compared to the version presented in the PDF</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.23761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.23761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.23760v1</id>
    <updated>2025-05-29T17:59:48Z</updated>
    <published>2025-05-29T17:59:48Z</published>
    <title>Model Immunization from a Condition Number Perspective</title>
    <summary>  Model immunization aims to pre-train models that are difficult to fine-tune
on harmful tasks while retaining their utility on other non-harmful tasks.
Though prior work has shown empirical evidence for immunizing text-to-image
models, the key understanding of when immunization is possible and a precise
definition of an immunized model remain unclear. In this work, we propose a
framework, based on the condition number of a Hessian matrix, to analyze model
immunization for linear models. Building on this framework, we design an
algorithm with regularization terms to control the resulting condition numbers
after pre-training. Empirical results on linear models and non-linear deep-nets
demonstrate the effectiveness of the proposed algorithm on model immunization.
The code is available at
https://github.com/amberyzheng/model-immunization-cond-num.
</summary>
    <author>
      <name>Amber Yijia Zheng</name>
    </author>
    <author>
      <name>Cedar Site Bai</name>
    </author>
    <author>
      <name>Brian Bullins</name>
    </author>
    <author>
      <name>Raymond A. Yeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.23760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.23760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.23759v1</id>
    <updated>2025-05-29T17:59:47Z</updated>
    <published>2025-05-29T17:59:47Z</published>
    <title>Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint</title>
    <summary>  Rebus puzzles, visual riddles that encode language through imagery, spatial
arrangement, and symbolic substitution, pose a unique challenge to current
vision-language models (VLMs). Unlike traditional image captioning or question
answering tasks, rebus solving requires multi-modal abstraction, symbolic
reasoning, and a grasp of cultural, phonetic and linguistic puns. In this
paper, we investigate the capacity of contemporary VLMs to interpret and solve
rebus puzzles by constructing a hand-generated and annotated benchmark of
diverse English-language rebus puzzles, ranging from simple pictographic
substitutions to spatially-dependent cues ("head" over "heels"). We analyze how
different VLMs perform, and our findings reveal that while VLMs exhibit some
surprising capabilities in decoding simple visual clues, they struggle
significantly with tasks requiring abstract reasoning, lateral thinking, and
understanding visual metaphors.
</summary>
    <author>
      <name>Heekyung Lee</name>
    </author>
    <author>
      <name>Jiaxin Ge</name>
    </author>
    <author>
      <name>Tsung-Han Wu</name>
    </author>
    <author>
      <name>Minwoo Kang</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <author>
      <name>David M. Chan</name>
    </author>
    <link href="http://arxiv.org/abs/2505.23759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.23759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
