<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-04-16T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">419068</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.11457v1</id>
    <updated>2025-04-15T17:59:54Z</updated>
    <published>2025-04-15T17:59:54Z</published>
    <title>Aligning Generative Denoising with Discriminative Objectives Unleashes
  Diffusion for Visual Perception</title>
    <summary>  With the success of image generation, generative diffusion models are
increasingly adopted for discriminative tasks, as pixel generation provides a
unified perception interface. However, directly repurposing the generative
denoising process for discriminative objectives reveals critical gaps rarely
addressed previously. Generative models tolerate intermediate sampling errors
if the final distribution remains plausible, but discriminative tasks require
rigorous accuracy throughout, as evidenced in challenging multi-modal tasks
like referring image segmentation. Motivated by this gap, we analyze and
enhance alignment between generative diffusion processes and perception tasks,
focusing on how perception quality evolves during denoising. We find: (1)
earlier denoising steps contribute disproportionately to perception quality,
prompting us to propose tailored learning objectives reflecting varying
timestep contributions; (2) later denoising steps show unexpected perception
degradation, highlighting sensitivity to training-denoising distribution
shifts, addressed by our diffusion-tailored data augmentation; and (3)
generative processes uniquely enable interactivity, serving as controllable
user interfaces adaptable to correctional prompts in multi-round interactions.
Our insights significantly improve diffusion-based perception models without
architectural changes, achieving state-of-the-art performance on depth
estimation, referring image segmentation, and generalist perception tasks. Code
available at https://github.com/ziqipang/ADDP.
</summary>
    <author>
      <name>Ziqi Pang</name>
    </author>
    <author>
      <name>Xin Xu</name>
    </author>
    <author>
      <name>Yu-Xiong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2025</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2025</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2504.11457v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.11457v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.11456v1</id>
    <updated>2025-04-15T17:59:51Z</updated>
    <published>2025-04-15T17:59:51Z</published>
    <title>DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and
  Verifiable Mathematical Dataset for Advancing Reasoning</title>
    <summary>  The capacity for complex mathematical reasoning is a key benchmark for
artificial intelligence. While reinforcement learning (RL) applied to LLMs
shows promise, progress is significantly hindered by the lack of large-scale
training data that is sufficiently challenging, possesses verifiable answer
formats suitable for RL, and is free from contamination with evaluation
benchmarks. To address these limitations, we introduce DeepMath-103K, a new,
large-scale dataset comprising approximately 103K mathematical problems,
specifically designed to train advanced reasoning models via RL. DeepMath-103K
is curated through a rigorous pipeline involving source analysis, stringent
decontamination against numerous benchmarks, and filtering for high difficulty
(primarily Levels 5-9), significantly exceeding existing open resources in
challenge. Each problem includes a verifiable final answer, enabling rule-based
RL, and three distinct R1-generated solutions suitable for diverse training
paradigms like supervised fine-tuning or distillation. Spanning a wide range of
mathematical topics, DeepMath-103K promotes the development of generalizable
reasoning. We demonstrate that models trained on DeepMath-103K achieve
significant improvements on challenging mathematical benchmarks, validating its
effectiveness. We release DeepMath-103K publicly to facilitate community
progress in building more capable AI reasoning systems:
https://github.com/zwhe99/DeepMath.
</summary>
    <author>
      <name>Zhiwei He</name>
    </author>
    <author>
      <name>Tian Liang</name>
    </author>
    <author>
      <name>Jiahao Xu</name>
    </author>
    <author>
      <name>Qiuzhi Liu</name>
    </author>
    <author>
      <name>Xingyu Chen</name>
    </author>
    <author>
      <name>Yue Wang</name>
    </author>
    <author>
      <name>Linfeng Song</name>
    </author>
    <author>
      <name>Dian Yu</name>
    </author>
    <author>
      <name>Zhenwen Liang</name>
    </author>
    <author>
      <name>Wenxuan Wang</name>
    </author>
    <author>
      <name>Zhuosheng Zhang</name>
    </author>
    <author>
      <name>Rui Wang</name>
    </author>
    <author>
      <name>Zhaopeng Tu</name>
    </author>
    <author>
      <name>Haitao Mi</name>
    </author>
    <author>
      <name>Dong Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">WIP</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.11456v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.11456v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.11454v2</id>
    <updated>2025-04-16T02:35:11Z</updated>
    <published>2025-04-15T17:59:43Z</published>
    <title>Elucidating the Design Space of Multimodal Protein Language Models</title>
    <summary>  Multimodal protein language models (PLMs) integrate sequence and token-based
structural information, serving as a powerful foundation for protein modeling,
generation, and design. However, the reliance on tokenizing 3D structures into
discrete tokens causes substantial loss of fidelity about fine-grained
structural details and correlations. In this paper, we systematically elucidate
the design space of multimodal PLMs to overcome their limitations. We identify
tokenization loss and inaccurate structure token predictions by the PLMs as
major bottlenecks. To address these, our proposed design space covers improved
generative modeling, structure-aware architectures and representation learning,
and data exploration. Our advancements approach finer-grained supervision,
demonstrating that token-based multimodal PLMs can achieve robust structural
modeling. The effective design methods dramatically improve the structure
generation diversity, and notably, folding abilities of our 650M model by
reducing the RMSD from 5.52 to 2.36 on PDB testset, even outperforming 3B
baselines and on par with the specialized folding models.
</summary>
    <author>
      <name>Cheng-Yen Hsieh</name>
    </author>
    <author>
      <name>Xinyou Wang</name>
    </author>
    <author>
      <name>Daiheng Zhang</name>
    </author>
    <author>
      <name>Dongyu Xue</name>
    </author>
    <author>
      <name>Fei Ye</name>
    </author>
    <author>
      <name>Shujian Huang</name>
    </author>
    <author>
      <name>Zaixiang Zheng</name>
    </author>
    <author>
      <name>Quanquan Gu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://bytedance.github.io/dplm/dplm-2.1/</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.11454v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.11454v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.11453v1</id>
    <updated>2025-04-15T17:59:05Z</updated>
    <published>2025-04-15T17:59:05Z</published>
    <title>A Clean Slate for Offline Reinforcement Learning</title>
    <summary>  Progress in offline reinforcement learning (RL) has been impeded by ambiguous
problem definitions and entangled algorithmic designs, resulting in
inconsistent implementations, insufficient ablations, and unfair evaluations.
Although offline RL explicitly avoids environment interaction, prior methods
frequently employ extensive, undocumented online evaluation for hyperparameter
tuning, complicating method comparisons. Moreover, existing reference
implementations differ significantly in boilerplate code, obscuring their core
algorithmic contributions. We address these challenges by first introducing a
rigorous taxonomy and a transparent evaluation protocol that explicitly
quantifies online tuning budgets. To resolve opaque algorithmic design, we
provide clean, minimalistic, single-file implementations of various model-free
and model-based offline RL methods, significantly enhancing clarity and
achieving substantial speed-ups. Leveraging these streamlined implementations,
we propose Unifloral, a unified algorithm that encapsulates diverse prior
approaches within a single, comprehensive hyperparameter space, enabling
algorithm development in a shared hyperparameter space. Using Unifloral with
our rigorous evaluation protocol, we develop two novel algorithms - TD3-AWR
(model-free) and MoBRAC (model-based) - which substantially outperform
established baselines. Our implementation is publicly available at
https://github.com/EmptyJackson/unifloral.
</summary>
    <author>
      <name>Matthew Thomas Jackson</name>
    </author>
    <author>
      <name>Uljad Berdica</name>
    </author>
    <author>
      <name>Jarek Liesen</name>
    </author>
    <author>
      <name>Shimon Whiteson</name>
    </author>
    <author>
      <name>Jakob Nicolaus Foerster</name>
    </author>
    <link href="http://arxiv.org/abs/2504.11453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.11453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.11451v1</id>
    <updated>2025-04-15T17:58:16Z</updated>
    <published>2025-04-15T17:58:16Z</published>
    <title>PARTFIELD: Learning 3D Feature Fields for Part Segmentation and Beyond</title>
    <summary>  We propose PartField, a feedforward approach for learning part-based 3D
features, which captures the general concept of parts and their hierarchy
without relying on predefined templates or text-based names, and can be applied
to open-world 3D shapes across various modalities. PartField requires only a 3D
feedforward pass at inference time, significantly improving runtime and
robustness compared to prior approaches. Our model is trained by distilling 2D
and 3D part proposals from a mix of labeled datasets and image segmentations on
large unsupervised datasets, via a contrastive learning formulation. It
produces a continuous feature field which can be clustered to yield a
hierarchical part decomposition. Comparisons show that PartField is up to 20%
more accurate and often orders of magnitude faster than other recent
class-agnostic part-segmentation methods. Beyond single-shape part
decomposition, consistency in the learned field emerges across shapes, enabling
tasks such as co-segmentation and correspondence, which we demonstrate in
several applications of these general-purpose, hierarchical, and consistent 3D
feature fields. Check our Webpage!
https://research.nvidia.com/labs/toronto-ai/partfield-release/
</summary>
    <author>
      <name>Minghua Liu</name>
    </author>
    <author>
      <name>Mikaela Angelina Uy</name>
    </author>
    <author>
      <name>Donglai Xiang</name>
    </author>
    <author>
      <name>Hao Su</name>
    </author>
    <author>
      <name>Sanja Fidler</name>
    </author>
    <author>
      <name>Nicholas Sharp</name>
    </author>
    <author>
      <name>Jun Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">https://research.nvidia.com/labs/toronto-ai/partfield-release/</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.11451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.11451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
