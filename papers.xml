<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-04-06T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">416601</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.02827v1</id>
    <updated>2025-04-03T17:59:56Z</updated>
    <published>2025-04-03T17:59:56Z</published>
    <title>On Vanishing Variance in Transformer Length Generalization</title>
    <summary>  It is a widely known issue that Transformers, when trained on shorter
sequences, fail to generalize robustly to longer ones at test time. This raises
the question of whether Transformer models are real reasoning engines, despite
their impressive abilities in mathematical problem solving and code synthesis.
In this paper, we offer a vanishing variance perspective on this issue. To the
best of our knowledge, we are the first to demonstrate that even for today's
frontier models, a longer sequence length results in a decrease in variance in
the output of the multi-head attention modules. On the argmax retrieval and
dictionary lookup tasks, our experiments show that applying layer normalization
after the attention outputs leads to significantly better length
generalization. Our analyses attribute this improvement to a reduction-though
not a complete elimination-of the distribution shift caused by vanishing
variance.
</summary>
    <author>
      <name>Ruining Li</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Jinghao</arxiv:affiliation>
    </author>
    <author>
      <name>Gabrijel Boduljak</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Jinghao</arxiv:affiliation>
    </author>
    <author>
      <name> Jensen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Jinghao</arxiv:affiliation>
    </author>
    <author>
      <name> Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://ruiningli.com/vanishing-variance. The first two
  authors contributed equally to this work</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.02827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.02827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.02823v1</id>
    <updated>2025-04-03T17:59:12Z</updated>
    <published>2025-04-03T17:59:12Z</published>
    <title>STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage
  Security Inspection</title>
    <summary>  Advancements in Computer-Aided Screening (CAS) systems are essential for
improving the detection of security threats in X-ray baggage scans. However,
current datasets are limited in representing real-world, sophisticated threats
and concealment tactics, and existing approaches are constrained by a
closed-set paradigm with predefined labels. To address these challenges, we
introduce STCray, the first multimodal X-ray baggage security dataset,
comprising 46,642 image-caption paired scans across 21 threat categories,
generated using an X-ray scanner for airport security. STCray is meticulously
developed with our specialized protocol that ensures domain-aware, coherent
captions, that lead to the multi-modal instruction following data in X-ray
baggage security. This allows us to train a domain-aware visual AI assistant
named STING-BEE that supports a range of vision-language tasks, including scene
comprehension, referring threat localization, visual grounding, and visual
question answering (VQA), establishing novel baselines for multi-modal learning
in X-ray baggage security. Further, STING-BEE shows state-of-the-art
generalization in cross-domain settings. Code, data, and models are available
at https://divs1159.github.io/STING-BEE/.
</summary>
    <author>
      <name>Divya Velayudhan</name>
    </author>
    <author>
      <name>Abdelfatah Ahmed</name>
    </author>
    <author>
      <name>Mohamad Alansari</name>
    </author>
    <author>
      <name>Neha Gour</name>
    </author>
    <author>
      <name>Abderaouf Behouch</name>
    </author>
    <author>
      <name>Taimur Hassan</name>
    </author>
    <author>
      <name>Syed Talal Wasim</name>
    </author>
    <author>
      <name>Nabil Maalej</name>
    </author>
    <author>
      <name>Muzammal Naseer</name>
    </author>
    <author>
      <name>Juergen Gall</name>
    </author>
    <author>
      <name>Mohammed Bennamoun</name>
    </author>
    <author>
      <name>Ernesto Damiani</name>
    </author>
    <author>
      <name>Naoufel Werghi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at CVPR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.02823v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.02823v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.02822v1</id>
    <updated>2025-04-03T17:58:44Z</updated>
    <published>2025-04-03T17:58:44Z</published>
    <title>Do Two AI Scientists Agree?</title>
    <summary>  When two AI models are trained on the same scientific task, do they learn the
same theory or two different theories? Throughout history of science, we have
witnessed the rise and fall of theories driven by experimental validation or
falsification: many theories may co-exist when experimental data is lacking,
but the space of survived theories become more constrained with more
experimental data becoming available. We show the same story is true for AI
scientists. With increasingly more systems provided in training data, AI
scientists tend to converge in the theories they learned, although sometimes
they form distinct groups corresponding to different theories. To
mechanistically interpret what theories AI scientists learn and quantify their
agreement, we propose MASS, Hamiltonian-Lagrangian neural networks as AI
Scientists, trained on standard problems in physics, aggregating training
results across many seeds simulating the different configurations of AI
scientists. Our findings suggests for AI scientists switch from learning a
Hamiltonian theory in simple setups to a Lagrangian formulation when more
complex systems are introduced. We also observe strong seed dependence of the
training dynamics and final learned weights, controlling the rise and fall of
relevant theories. We finally demonstrate that not only can our neural networks
aid interpretability, it can also be applied to higher dimensional problems.
</summary>
    <author>
      <name>Xinghong Fu</name>
    </author>
    <author>
      <name>Ziming Liu</name>
    </author>
    <author>
      <name>Max Tegmark</name>
    </author>
    <link href="http://arxiv.org/abs/2504.02822v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.02822v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.02821v1</id>
    <updated>2025-04-03T17:58:35Z</updated>
    <published>2025-04-03T17:58:35Z</published>
    <title>Sparse Autoencoders Learn Monosemantic Features in Vision-Language
  Models</title>
    <summary>  Sparse Autoencoders (SAEs) have recently been shown to enhance
interpretability and steerability in Large Language Models (LLMs). In this
work, we extend the application of SAEs to Vision-Language Models (VLMs), such
as CLIP, and introduce a comprehensive framework for evaluating monosemanticity
in vision representations. Our experimental results reveal that SAEs trained on
VLMs significantly enhance the monosemanticity of individual neurons while also
exhibiting hierarchical representations that align well with expert-defined
structures (e.g., iNaturalist taxonomy). Most notably, we demonstrate that
applying SAEs to intervene on a CLIP vision encoder, directly steer output from
multimodal LLMs (e.g., LLaVA) without any modifications to the underlying
model. These findings emphasize the practicality and efficacy of SAEs as an
unsupervised approach for enhancing both the interpretability and control of
VLMs.
</summary>
    <author>
      <name>Mateusz Pach</name>
    </author>
    <author>
      <name>Shyamgopal Karthik</name>
    </author>
    <author>
      <name>Quentin Bouniot</name>
    </author>
    <author>
      <name>Serge Belongie</name>
    </author>
    <author>
      <name>Zeynep Akata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint. The code is available at
  https://github.com/ExplainableML/sae-for-vlm</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.02821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.02821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.02819v1</id>
    <updated>2025-04-03T17:58:18Z</updated>
    <published>2025-04-03T17:58:18Z</published>
    <title>GMR-Conv: An Efficient Rotation and Reflection Equivariant Convolution
  Kernel Using Gaussian Mixture Rings</title>
    <summary>  Symmetry, where certain features remain invariant under geometric
transformations, can often serve as a powerful prior in designing convolutional
neural networks (CNNs). While conventional CNNs inherently support
translational equivariance, extending this property to rotation and reflection
has proven challenging, often forcing a compromise between equivariance,
efficiency, and information loss. In this work, we introduce Gaussian Mixture
Ring Convolution (GMR-Conv), an efficient convolution kernel that smooths
radial symmetry using a mixture of Gaussian-weighted rings. This design
mitigates discretization errors of circular kernels, thereby preserving robust
rotation and reflection equivariance without incurring computational overhead.
We further optimize both the space and speed efficiency of GMR-Conv via a novel
parameterization and computation strategy, allowing larger kernels at an
acceptable cost. Extensive experiments on eight classification and one
segmentation datasets demonstrate that GMR-Conv not only matches conventional
CNNs' performance but can also surpass it in applications with orientation-less
data. GMR-Conv is also proven to be more robust and efficient than the
state-of-the-art equivariant learning methods. Our work provides inspiring
empirical evidence that carefully applied radial symmetry can alleviate the
challenges of information loss, marking a promising advance in equivariant
network architectures. The code is available at
https://github.com/XYPB/GMR-Conv.
</summary>
    <author>
      <name>Yuexi Du</name>
    </author>
    <author>
      <name>Jiazhen Zhang</name>
    </author>
    <author>
      <name>Nicha C. Dvornek</name>
    </author>
    <author>
      <name>John A. Onofrey</name>
    </author>
    <link href="http://arxiv.org/abs/2504.02819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.02819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
