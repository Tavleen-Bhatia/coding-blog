<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-03-25T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">414338</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.18948v1</id>
    <updated>2025-03-24T17:59:57Z</updated>
    <published>2025-03-24T17:59:57Z</published>
    <title>Equivariant Image Modeling</title>
    <summary>  Current generative models, such as autoregressive and diffusion approaches,
decompose high-dimensional data distribution learning into a series of simpler
subtasks. However, inherent conflicts arise during the joint optimization of
these subtasks, and existing solutions fail to resolve such conflicts without
sacrificing efficiency or scalability. We propose a novel equivariant image
modeling framework that inherently aligns optimization targets across subtasks
by leveraging the translation invariance of natural visual signals. Our method
introduces (1) column-wise tokenization which enhances translational symmetry
along the horizontal axis, and (2) windowed causal attention which enforces
consistent contextual relationships across positions. Evaluated on
class-conditioned ImageNet generation at 256x256 resolution, our approach
achieves performance comparable to state-of-the-art AR models while using fewer
computational resources. Systematic analysis demonstrates that enhanced
equivariance reduces inter-task conflicts, significantly improving zero-shot
generalization and enabling ultra-long image synthesis. This work establishes
the first framework for task-aligned decomposition in generative modeling,
offering insights into efficient parameter sharing and conflict-free
optimization. The code and models are publicly available at
https://github.com/drx-code/EquivariantModeling.
</summary>
    <author>
      <name>Ruixiao Dong</name>
    </author>
    <author>
      <name>Mengde Xu</name>
    </author>
    <author>
      <name>Zigang Geng</name>
    </author>
    <author>
      <name>Li Li</name>
    </author>
    <author>
      <name>Han Hu</name>
    </author>
    <author>
      <name>Shuyang Gu</name>
    </author>
    <link href="http://arxiv.org/abs/2503.18948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.18948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.18947v1</id>
    <updated>2025-03-24T17:59:56Z</updated>
    <published>2025-03-24T17:59:56Z</published>
    <title>Tuning-Free Amodal Segmentation via the Occlusion-Free Bias of
  Inpainting Models</title>
    <summary>  Amodal segmentation aims to predict segmentation masks for both the visible
and occluded regions of an object. Most existing works formulate this as a
supervised learning problem, requiring manually annotated amodal masks or
synthetic training data. Consequently, their performance depends on the quality
of the datasets, which often lack diversity and scale. This work introduces a
tuning-free approach that repurposes pretrained diffusion-based inpainting
models for amodal segmentation. Our approach is motivated by the
"occlusion-free bias" of inpainting models, i.e., the inpainted objects tend to
be complete objects without occlusions. Specifically, we reconstruct the
occluded regions of an object via inpainting and then apply segmentation, all
without additional training or fine-tuning. Experiments on five datasets
demonstrate the generalizability and robustness of our approach. On average,
our approach achieves 5.3% more accurate masks over the state-of-the-art.
</summary>
    <author>
      <name>Jae Joong Lee</name>
    </author>
    <author>
      <name>Bedrich Benes</name>
    </author>
    <author>
      <name>Raymond A. Yeh</name>
    </author>
    <link href="http://arxiv.org/abs/2503.18947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.18947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.18945v2</id>
    <updated>2025-03-25T15:31:25Z</updated>
    <published>2025-03-24T17:59:51Z</published>
    <title>Aether: Geometric-Aware Unified World Modeling</title>
    <summary>  The integration of geometric reconstruction and generative modeling remains a
critical challenge in developing AI systems capable of human-like spatial
reasoning. This paper proposes Aether, a unified framework that enables
geometry-aware reasoning in world models by jointly optimizing three core
capabilities: (1) 4D dynamic reconstruction, (2) action-conditioned video
prediction, and (3) goal-conditioned visual planning. Through task-interleaved
feature learning, Aether achieves synergistic knowledge sharing across
reconstruction, prediction, and planning objectives. Building upon video
generation models, our framework demonstrates unprecedented synthetic-to-real
generalization despite never observing real-world data during training.
Furthermore, our approach achieves zero-shot generalization in both action
following and reconstruction tasks, thanks to its intrinsic geometric modeling.
Remarkably, even without real-world data, its reconstruction performance is
comparable with or even better than that of domain-specific models.
Additionally, Aether employs camera trajectories as geometry-informed action
spaces, enabling effective action-conditioned prediction and visual planning.
We hope our work inspires the community to explore new frontiers in
physically-reasonable world modeling and its applications.
</summary>
    <author>
      <name> Aether Team</name>
    </author>
    <author>
      <name>Haoyi Zhu</name>
    </author>
    <author>
      <name>Yifan Wang</name>
    </author>
    <author>
      <name>Jianjun Zhou</name>
    </author>
    <author>
      <name>Wenzheng Chang</name>
    </author>
    <author>
      <name>Yang Zhou</name>
    </author>
    <author>
      <name>Zizun Li</name>
    </author>
    <author>
      <name>Junyi Chen</name>
    </author>
    <author>
      <name>Chunhua Shen</name>
    </author>
    <author>
      <name>Jiangmiao Pang</name>
    </author>
    <author>
      <name>Tong He</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://aether-world.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.18945v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.18945v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.18938v1</id>
    <updated>2025-03-24T17:58:15Z</updated>
    <published>2025-03-24T17:58:15Z</published>
    <title>AdaWorld: Learning Adaptable World Models with Latent Actions</title>
    <summary>  World models aim to learn action-controlled prediction models and have proven
essential for the development of intelligent agents. However, most existing
world models rely heavily on substantial action-labeled data and costly
training, making it challenging to adapt to novel environments with
heterogeneous actions through limited interactions. This limitation can hinder
their applicability across broader domains. To overcome this challenge, we
propose AdaWorld, an innovative world model learning approach that enables
efficient adaptation. The key idea is to incorporate action information during
the pretraining of world models. This is achieved by extracting latent actions
from videos in a self-supervised manner, capturing the most critical
transitions between frames. We then develop an autoregressive world model that
conditions on these latent actions. This learning paradigm enables highly
adaptable world models, facilitating efficient transfer and learning of new
actions even with limited interactions and finetuning. Our comprehensive
experiments across multiple environments demonstrate that AdaWorld achieves
superior performance in both simulation quality and visual planning.
</summary>
    <author>
      <name>Shenyuan Gao</name>
    </author>
    <author>
      <name>Siyuan Zhou</name>
    </author>
    <author>
      <name>Yilun Du</name>
    </author>
    <author>
      <name>Jun Zhang</name>
    </author>
    <author>
      <name>Chuang Gan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://adaptable-world-model.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.18938v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.18938v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.18929v1</id>
    <updated>2025-03-24T17:51:39Z</updated>
    <published>2025-03-24T17:51:39Z</published>
    <title>Trajectory Balance with Asynchrony: Decoupling Exploration and Learning
  for Fast, Scalable LLM Post-Training</title>
    <summary>  Reinforcement learning (RL) is a critical component of large language model
(LLM) post-training. However, existing on-policy algorithms used for
post-training are inherently incompatible with the use of experience replay
buffers, which can be populated scalably by distributed off-policy actors to
enhance exploration as compute increases. We propose efficiently obtaining this
benefit of replay buffers via Trajectory Balance with Asynchrony (TBA), a
massively scalable LLM RL system. In contrast to existing approaches, TBA uses
a larger fraction of compute on search, constantly generating off-policy data
for a central replay buffer. A training node simultaneously samples data from
this buffer based on reward or recency to update the policy using Trajectory
Balance (TB), a diversity-seeking RL objective introduced for GFlowNets. TBA
offers three key advantages: (1) decoupled training and search, speeding up
training wall-clock time by 4x or more; (2) improved diversity through
large-scale off-policy sampling; and (3) scalable search for sparse reward
settings. On mathematical reasoning, preference-tuning, and automated
red-teaming (diverse and representative post-training tasks), TBA produces
speed and performance improvements over strong baselines.
</summary>
    <author>
      <name>Brian R. Bartoldson</name>
    </author>
    <author>
      <name>Siddarth Venkatraman</name>
    </author>
    <author>
      <name>James Diffenderfer</name>
    </author>
    <author>
      <name>Moksh Jain</name>
    </author>
    <author>
      <name>Tal Ben-Nun</name>
    </author>
    <author>
      <name>Seanie Lee</name>
    </author>
    <author>
      <name>Minsu Kim</name>
    </author>
    <author>
      <name>Johan Obando-Ceron</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <author>
      <name>Bhavya Kailkhura</name>
    </author>
    <link href="http://arxiv.org/abs/2503.18929v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.18929v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
