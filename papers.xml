<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-04-11T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">418087</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.07965v1</id>
    <updated>2025-04-10T17:59:57Z</updated>
    <published>2025-04-10T17:59:57Z</published>
    <title>Cat, Rat, Meow: On the Alignment of Language Model and Human
  Term-Similarity Judgments</title>
    <summary>  Small and mid-sized generative language models have gained increasing
attention. Their size and availability make them amenable to being analyzed at
a behavioral as well as a representational level, allowing investigations of
how these levels interact. We evaluate 32 publicly available language models
for their representational and behavioral alignment with human similarity
judgments on a word triplet task. This provides a novel evaluation setting to
probe semantic associations in language beyond common pairwise comparisons. We
find that (1) even the representations of small language models can achieve
human-level alignment, (2) instruction-tuned model variants can exhibit
substantially increased agreement, (3) the pattern of alignment across layers
is highly model dependent, and (4) alignment based on models' behavioral
responses is highly dependent on model size, matching their representational
alignment only for the largest evaluated models.
</summary>
    <author>
      <name>Lorenz Linhardt</name>
    </author>
    <author>
      <name>Tom Neuhäuser</name>
    </author>
    <author>
      <name>Lenka Tětková</name>
    </author>
    <author>
      <name>Oliver Eberle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2025 Workshop on Representational Alignment (Re-Align)</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.07965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.07964v1</id>
    <updated>2025-04-10T17:59:56Z</updated>
    <published>2025-04-10T17:59:56Z</published>
    <title>C3PO: Critical-Layer, Core-Expert, Collaborative Pathway Optimization
  for Test-Time Expert Re-Mixing</title>
    <summary>  Mixture-of-Experts (MoE) Large Language Models (LLMs) suffer from severely
sub-optimal expert pathways-our study reveals that naive expert selection
learned from pretraining leaves a surprising 10-20% accuracy gap for
improvement. Motivated by this observation, we develop a novel class of
test-time optimization methods to re-weight or "re-mixing" the experts in
different layers jointly for each test sample. Since the test sample's ground
truth is unknown, we propose to optimize a surrogate objective defined by the
sample's "successful neighbors" from a reference set of samples. We introduce
three surrogates and algorithms based on mode-finding, kernel regression, and
the average loss of similar reference samples/tasks. To reduce the cost of
optimizing whole pathways, we apply our algorithms merely to the core experts'
mixing weights in critical layers, which enjoy similar performance but save
significant computation. This leads to "Critical-Layer, Core-Expert,
Collaborative Pathway Optimization (C3PO)". We apply C3PO to two recent MoE
LLMs and examine it on six widely-used benchmarks. It consistently improves the
base model by 7-15% in accuracy and outperforms widely used test-time learning
baselines, e.g., in-context learning and prompt/prefix tuning, by a large
margin. Moreover, C3PO enables MoE LLMs with 1-3B active parameters to
outperform LLMs of 7-9B parameters, hence improving MoE's advantages on
efficiency. Our thorough ablation study further sheds novel insights on
achieving test-time improvement on MoE.
</summary>
    <author>
      <name>Zhongyang Li</name>
    </author>
    <author>
      <name>Ziyue Li</name>
    </author>
    <author>
      <name>Tianyi Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2504.07964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.07962v1</id>
    <updated>2025-04-10T17:59:55Z</updated>
    <published>2025-04-10T17:59:55Z</published>
    <title>GLUS: Global-Local Reasoning Unified into A Single Large Language Model
  for Video Segmentation</title>
    <summary>  This paper proposes a novel framework utilizing multi-modal large language
models (MLLMs) for referring video object segmentation (RefVOS). Previous
MLLM-based methods commonly struggle with the dilemma between "Ref" and "VOS":
they either specialize in understanding a few key frames (global reasoning) or
tracking objects on continuous frames (local reasoning), and rely on external
VOS or frame selectors to mitigate the other end of the challenge. However, our
framework GLUS shows that global and local consistency can be unified into a
single video segmentation MLLM: a set of sparse "context frames" provides
global information, while a stream of continuous "query frames" conducts local
object tracking. This is further supported by jointly training the MLLM with a
pre-trained VOS memory bank to simultaneously digest short-range and long-range
temporal information. To improve the information efficiency within the limited
context window of MLLMs, we introduce object contrastive learning to
distinguish hard false-positive objects and a self-refined framework to
identify crucial frames and perform propagation. By collectively integrating
these insights, our GLUS delivers a simple yet effective baseline, achieving
new state-of-the-art for MLLMs on the MeViS and Ref-Youtube-VOS benchmark. Our
project page is at https://glus-video.github.io/.
</summary>
    <author>
      <name>Lang Lin</name>
    </author>
    <author>
      <name>Xueyang Yu</name>
    </author>
    <author>
      <name>Ziqi Pang</name>
    </author>
    <author>
      <name>Yu-Xiong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.07962v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07962v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.07960v1</id>
    <updated>2025-04-10T17:59:42Z</updated>
    <published>2025-04-10T17:59:42Z</published>
    <title>VisualCloze: A Universal Image Generation Framework via Visual
  In-Context Learning</title>
    <summary>  Recent progress in diffusion models significantly advances various image
generation tasks. However, the current mainstream approach remains focused on
building task-specific models, which have limited efficiency when supporting a
wide range of different needs. While universal models attempt to address this
limitation, they face critical challenges, including generalizable task
instruction, appropriate task distributions, and unified architectural design.
To tackle these challenges, we propose VisualCloze, a universal image
generation framework, which supports a wide range of in-domain tasks,
generalization to unseen ones, unseen unification of multiple tasks, and
reverse generation. Unlike existing methods that rely on language-based task
instruction, leading to task ambiguity and weak generalization, we integrate
visual in-context learning, allowing models to identify tasks from visual
demonstrations. Meanwhile, the inherent sparsity of visual task distributions
hampers the learning of transferable knowledge across tasks. To this end, we
introduce Graph200K, a graph-structured dataset that establishes various
interrelated tasks, enhancing task density and transferable knowledge.
Furthermore, we uncover that our unified image generation formulation shared a
consistent objective with image infilling, enabling us to leverage the strong
generative priors of pre-trained infilling models without modifying the
architectures.
</summary>
    <author>
      <name>Zhong-Yu Li</name>
    </author>
    <author>
      <name>Ruoyi Du</name>
    </author>
    <author>
      <name>Juncheng Yan</name>
    </author>
    <author>
      <name>Le Zhuo</name>
    </author>
    <author>
      <name>Zhen Li</name>
    </author>
    <author>
      <name>Peng Gao</name>
    </author>
    <author>
      <name>Zhanyu Ma</name>
    </author>
    <author>
      <name>Ming-Ming Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://visualcloze.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.07960v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07960v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.07959v1</id>
    <updated>2025-04-10T17:59:31Z</updated>
    <published>2025-04-10T17:59:31Z</published>
    <title>CCMNet: Leveraging Calibrated Color Correction Matrices for Cross-Camera
  Color Constancy</title>
    <summary>  Computational color constancy, or white balancing, is a key module in a
camera's image signal processor (ISP) that corrects color casts from scene
lighting. Because this operation occurs in the camera-specific raw color space,
white balance algorithms must adapt to different cameras. This paper introduces
a learning-based method for cross-camera color constancy that generalizes to
new cameras without retraining. Our method leverages pre-calibrated color
correction matrices (CCMs) available on ISPs that map the camera's raw color
space to a standard space (e.g., CIE XYZ). Our method uses these CCMs to
transform predefined illumination colors (i.e., along the Planckian locus) into
the test camera's raw space. The mapped illuminants are encoded into a compact
camera fingerprint embedding (CFE) that enables the network to adapt to unseen
cameras. To prevent overfitting due to limited cameras and CCMs during
training, we introduce a data augmentation technique that interpolates between
cameras and their CCMs. Experimental results across multiple datasets and
backbones show that our method achieves state-of-the-art cross-camera color
constancy while remaining lightweight and relying only on data readily
available in camera ISPs.
</summary>
    <author>
      <name>Dongyoung Kim</name>
    </author>
    <author>
      <name>Mahmoud Afifi</name>
    </author>
    <author>
      <name>Dongyun Kim</name>
    </author>
    <author>
      <name>Michael S. Brown</name>
    </author>
    <author>
      <name>Seon Joo Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2504.07959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
