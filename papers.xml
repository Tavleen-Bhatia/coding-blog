<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-03-10T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">410356</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.05696v1</id>
    <updated>2025-03-07T18:58:23Z</updated>
    <published>2025-03-07T18:58:23Z</published>
    <title>Multi-Fidelity Policy Gradient Algorithms</title>
    <summary>  Many reinforcement learning (RL) algorithms require large amounts of data,
prohibiting their use in applications where frequent interactions with
operational systems are infeasible, or high-fidelity simulations are expensive
or unavailable. Meanwhile, low-fidelity simulators--such as reduced-order
models, heuristic reward functions, or generative world models--can cheaply
provide useful data for RL training, even if they are too coarse for direct
sim-to-real transfer. We propose multi-fidelity policy gradients (MFPGs), an RL
framework that mixes a small amount of data from the target environment with a
large volume of low-fidelity simulation data to form unbiased, reduced-variance
estimators (control variates) for on-policy policy gradients. We instantiate
the framework by developing multi-fidelity variants of two policy gradient
algorithms: REINFORCE and proximal policy optimization. Experimental results
across a suite of simulated robotics benchmark problems demonstrate that when
target-environment samples are limited, MFPG achieves up to 3.9x higher reward
and improves training stability when compared to baselines that only use
high-fidelity data. Moreover, even when the baselines are given more
high-fidelity samples--up to 10x as many interactions with the target
environment--MFPG continues to match or outperform them. Finally, we observe
that MFPG is capable of training effective policies even when the low-fidelity
environment is drastically different from the target environment. MFPG thus not
only offers a novel paradigm for efficient sim-to-real transfer but also
provides a principled approach to managing the trade-off between policy
performance and data collection costs.
</summary>
    <author>
      <name>Xinjie Liu</name>
    </author>
    <author>
      <name>Cyrus Neary</name>
    </author>
    <author>
      <name>Kushagra Gupta</name>
    </author>
    <author>
      <name>Christian Ellis</name>
    </author>
    <author>
      <name>Ufuk Topcu</name>
    </author>
    <author>
      <name>David Fridovich-Keil</name>
    </author>
    <link href="http://arxiv.org/abs/2503.05696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.05696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.05684v1</id>
    <updated>2025-03-07T18:49:57Z</updated>
    <published>2025-03-07T18:49:57Z</published>
    <title>Fairness-Aware Low-Rank Adaptation Under Demographic Privacy Constraints</title>
    <summary>  Pre-trained foundation models can be adapted for specific tasks using
Low-Rank Adaptation (LoRA). However, the fairness properties of these adapted
classifiers remain underexplored. Existing fairness-aware fine-tuning methods
rely on direct access to sensitive attributes or their predictors, but in
practice, these sensitive attributes are often held under strict consumer
privacy controls, and neither the attributes nor their predictors are available
to model developers, hampering the development of fair models. To address this
issue, we introduce a set of LoRA-based fine-tuning methods that can be trained
in a distributed fashion, where model developers and fairness auditors
collaborate without sharing sensitive attributes or predictors. In this paper,
we evaluate three such methods - sensitive unlearning, adversarial training,
and orthogonality loss - against a fairness-unaware baseline, using experiments
on the CelebA and UTK-Face datasets with an ImageNet pre-trained ViT-Base
model. We find that orthogonality loss consistently reduces bias while
maintaining or improving utility, whereas adversarial training improves False
Positive Rate Parity and Demographic Parity in some cases, and sensitive
unlearning provides no clear benefit. In tasks where significant biases are
present, distributed fairness-aware fine-tuning methods can effectively
eliminate bias without compromising consumer privacy and, in most cases,
improve model utility.
</summary>
    <author>
      <name>Parameswaran Kamalaruban</name>
    </author>
    <author>
      <name>Mark Anderson</name>
    </author>
    <author>
      <name>Stuart Burrell</name>
    </author>
    <author>
      <name>Maeve Madigan</name>
    </author>
    <author>
      <name>Piotr Skalski</name>
    </author>
    <author>
      <name>David Sutton</name>
    </author>
    <link href="http://arxiv.org/abs/2503.05684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.05684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.05683v1</id>
    <updated>2025-03-07T18:45:42Z</updated>
    <published>2025-03-07T18:45:42Z</published>
    <title>Understanding the Limits of Lifelong Knowledge Editing in LLMs</title>
    <summary>  Keeping large language models factually up-to-date is crucial for deployment,
yet costly retraining remains a challenge. Knowledge editing offers a promising
alternative, but methods are only tested on small-scale or synthetic edit
benchmarks. In this work, we aim to bridge research into lifelong knowledge
editing to real-world edits at practically relevant scale. We first introduce
WikiBigEdit; a large-scale benchmark of real-world Wikidata edits, built to
automatically extend lifelong for future-proof benchmarking. In its first
instance, it includes over 500K question-answer pairs for knowledge editing
alongside a comprehensive evaluation pipeline. Finally, we use WikiBigEdit to
study existing knowledge editing techniques' ability to incorporate large
volumes of real-world facts and contrast their capabilities to generic
modification techniques such as retrieval augmentation and continual finetuning
to acquire a complete picture of the practical extent of current lifelong
knowledge editing.
</summary>
    <author>
      <name>Lukas Thede</name>
    </author>
    <author>
      <name>Karsten Roth</name>
    </author>
    <author>
      <name>Matthias Bethge</name>
    </author>
    <author>
      <name>Zeynep Akata</name>
    </author>
    <author>
      <name>Tom Hartvigsen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.05683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.05683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.05682v1</id>
    <updated>2025-03-07T18:44:53Z</updated>
    <published>2025-03-07T18:44:53Z</published>
    <title>Task-oriented Uncertainty Collaborative Learning for Label-Efficient
  Brain Tumor Segmentation</title>
    <summary>  Multi-contrast magnetic resonance imaging (MRI) plays a vital role in brain
tumor segmentation and diagnosis by leveraging complementary information from
different contrasts. Each contrast highlights specific tumor characteristics,
enabling a comprehensive understanding of tumor morphology, edema, and
pathological heterogeneity. However, existing methods still face the challenges
of multi-level specificity perception across different contrasts, especially
with limited annotations. These challenges include data heterogeneity,
granularity differences, and interference from redundant information. To
address these limitations, we propose a Task-oriented Uncertainty Collaborative
Learning (TUCL) framework for multi-contrast MRI segmentation. TUCL introduces
a task-oriented prompt attention (TPA) module with intra-prompt and
cross-prompt attention mechanisms to dynamically model feature interactions
across contrasts and tasks. Additionally, a cyclic process is designed to map
the predictions back to the prompt to ensure that the prompts are effectively
utilized. In the decoding stage, the TUCL framework proposes a dual-path
uncertainty refinement (DUR) strategy which ensures robust segmentation by
refining predictions iteratively. Extensive experimental results on limited
labeled data demonstrate that TUCL significantly improves segmentation accuracy
(88.2\% in Dice and 10.853 mm in HD95). It shows that TUCL has the potential to
extract multi-contrast information and reduce the reliance on extensive
annotations. The code is available at:
https://github.com/Zhenxuan-Zhang/TUCL_BrainSeg.
</summary>
    <author>
      <name>Zhenxuan Zhang</name>
    </author>
    <author>
      <name>Hongjie Wu</name>
    </author>
    <author>
      <name>Jiahao Huang</name>
    </author>
    <author>
      <name>Baihong Xie</name>
    </author>
    <author>
      <name>Zhifan Gao</name>
    </author>
    <author>
      <name>Junxian Du</name>
    </author>
    <author>
      <name>Pete Lally</name>
    </author>
    <author>
      <name>Guang Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2503.05682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.05682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.05681v1</id>
    <updated>2025-03-07T18:43:45Z</updated>
    <published>2025-03-07T18:43:45Z</published>
    <title>Defining Determinism</title>
    <summary>  Determinism is the thesis that the past determines the future, but efforts to
define it precisely have exposed deep methodological disagreements. Standard
possible-worlds formulations of determinism presuppose an "agreement" relation
between worlds, but this relation can be understood in multiple ways -- none of
which is particularly clear. We critically examine the proliferation of
definitions of determinism in the recent literature, arguing that these
definitions fail to deliver clear verdicts about actual scientific theories. We
advocate a return to a formal approach, in the logical tradition of Carnap,
that treats determinism as a property of scientific theories, rather than an
elusive metaphysical doctrine.
  We highlight two key distinctions: (1) the difference between qualitative and
"full" determinism, as emphasized in recent discussions of physics and
metaphysics, and (2) the distinction between weak and strong formal conditions
on the uniqueness of world extensions. We argue that defining determinism in
terms of metaphysical notions such as haecceities is unhelpful, whereas
rigorous formal criteria -- such as Belot's D1 and D3 -- offer a tractable and
scientifically relevant account. By clarifying what it means for a theory to be
deterministic, we set the stage for a fruitful interaction between physics and
metaphysics.
</summary>
    <author>
      <name>Hans Halvorson</name>
    </author>
    <author>
      <name>JB Manchak</name>
    </author>
    <author>
      <name>James Owen Weatherall</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">46 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.05681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.05681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.hist-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.hist-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
