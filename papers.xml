<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-02-24T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">406723</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2502.15681v1</id>
    <updated>2025-02-21T18:59:20Z</updated>
    <published>2025-02-21T18:59:20Z</published>
    <title>One-step Diffusion Models with $f$-Divergence Distribution Matching</title>
    <summary>  Sampling from diffusion models involves a slow iterative process that hinders
their practical deployment, especially for interactive applications. To
accelerate generation speed, recent approaches distill a multi-step diffusion
model into a single-step student generator via variational score distillation,
which matches the distribution of samples generated by the student to the
teacher's distribution. However, these approaches use the reverse
Kullback-Leibler (KL) divergence for distribution matching which is known to be
mode seeking. In this paper, we generalize the distribution matching approach
using a novel $f$-divergence minimization framework, termed $f$-distill, that
covers different divergences with different trade-offs in terms of mode
coverage and training variance. We derive the gradient of the $f$-divergence
between the teacher and student distributions and show that it is expressed as
the product of their score differences and a weighting function determined by
their density ratio. This weighting function naturally emphasizes samples with
higher density in the teacher distribution, when using a less mode-seeking
divergence. We observe that the popular variational score distillation approach
using the reverse-KL divergence is a special case within our framework.
Empirically, we demonstrate that alternative $f$-divergences, such as
forward-KL and Jensen-Shannon divergences, outperform the current best
variational score distillation methods across image generation tasks. In
particular, when using Jensen-Shannon divergence, $f$-distill achieves current
state-of-the-art one-step generation performance on ImageNet64 and zero-shot
text-to-image generation on MS-COCO. Project page:
https://research.nvidia.com/labs/genair/f-distill
</summary>
    <author>
      <name>Yilun Xu</name>
    </author>
    <author>
      <name>Weili Nie</name>
    </author>
    <author>
      <name>Arash Vahdat</name>
    </author>
    <link href="http://arxiv.org/abs/2502.15681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.15681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.15679v1</id>
    <updated>2025-02-21T18:58:57Z</updated>
    <published>2025-02-21T18:58:57Z</published>
    <title>BOSS: Benchmark for Observation Space Shift in Long-Horizon Task</title>
    <summary>  Robotics has long sought to develop visual-servoing robots capable of
completing previously unseen long-horizon tasks. Hierarchical approaches offer
a pathway for achieving this goal by executing skill combinations arranged by a
task planner, with each visuomotor skill pre-trained using a specific imitation
learning (IL) algorithm. However, even in simple long-horizon tasks like skill
chaining, hierarchical approaches often struggle due to a problem we identify
as Observation Space Shift (OSS), where the sequential execution of preceding
skills causes shifts in the observation space, disrupting the performance of
subsequent individually trained skill policies. To validate OSS and evaluate
its impact on long-horizon tasks, we introduce BOSS (a Benchmark for
Observation Space Shift). BOSS comprises three distinct challenges: "Single
Predicate Shift", "Accumulated Predicate Shift", and "Skill Chaining", each
designed to assess a different aspect of OSS's negative effect. We evaluated
several recent popular IL algorithms on BOSS, including three Behavioral
Cloning methods and the Visual Language Action model OpenVLA. Even on the
simplest challenge, we observed average performance drops of 67%, 35%, 34%, and
54%, respectively, when comparing skill performance with and without OSS.
Additionally, we investigate a potential solution to OSS that scales up the
training data for each skill with a larger and more visually diverse set of
demonstrations, with our results showing it is not sufficient to resolve OSS.
The project page is: https://boss-benchmark.github.io/
</summary>
    <author>
      <name>Yue Yang</name>
    </author>
    <author>
      <name>Linfeng Zhao</name>
    </author>
    <author>
      <name>Mingyu Ding</name>
    </author>
    <author>
      <name>Gedas Bertasius</name>
    </author>
    <author>
      <name>Daniel Szafir</name>
    </author>
    <link href="http://arxiv.org/abs/2502.15679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.15679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.15678v1</id>
    <updated>2025-02-21T18:58:30Z</updated>
    <published>2025-02-21T18:58:30Z</published>
    <title>Testing the limits of fine-tuning to improve reasoning in vision
  language models</title>
    <summary>  Pre-trained vision language models still fall short of human visual
cognition. In an effort to improve visual cognition and align models with human
behavior, we introduce visual stimuli and human judgments on visual cognition
tasks, allowing us to systematically evaluate performance across cognitive
domains under a consistent environment. We fine-tune models on ground truth
data for intuitive physics and causal reasoning and find that this improves
model performance in the respective fine-tuning domain. Furthermore, it can
improve model alignment with human behavior. However, we find that fine-tuning
does not contribute to robust human-like generalization to data with other
visual characteristics or to tasks in other cognitive domains.
</summary>
    <author>
      <name>Luca M. Schulze Buschoff</name>
    </author>
    <author>
      <name>Konstantinos Voudouris</name>
    </author>
    <author>
      <name>Elif Akata</name>
    </author>
    <author>
      <name>Matthias Bethge</name>
    </author>
    <author>
      <name>Joshua B. Tenenbaum</name>
    </author>
    <author>
      <name>Eric Schulz</name>
    </author>
    <link href="http://arxiv.org/abs/2502.15678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.15678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.15677v1</id>
    <updated>2025-02-21T18:58:06Z</updated>
    <published>2025-02-21T18:58:06Z</published>
    <title>FLEKE: Federated Locate-then-Edit Knowledge Editing</title>
    <summary>  Locate-then-Edit Knowledge Editing (LEKE) is a key technique for updating
large language models (LLMs) without full retraining. However, existing methods
assume a single-user setting and become inefficient in real-world multi-client
scenarios, where decentralized organizations (e.g., hospitals, financial
institutions) independently update overlapping knowledge, leading to redundant
mediator knowledge vector (MKV) computations and privacy concerns. To address
these challenges, we introduce Federated Locate-then-Edit Knowledge Editing
(FLEKE), a novel task that enables multiple clients to collaboratively perform
LEKE while preserving privacy and reducing computational overhead. To achieve
this, we propose FedEdit, a two-stage framework that optimizes MKV selection
and reuse. In the first stage, clients locally apply LEKE and upload the
computed MKVs. In the second stage, rather than relying solely on server-based
MKV sharing, FLEKE allows clients retrieve relevant MKVs based on cosine
similarity, enabling knowledge re-edit and minimizing redundant computations.
Experimental results on two benchmark datasets demonstrate that FedEdit retains
over 96% of the performance of non-federated LEKE while significantly
outperforming a FedAvg-based baseline by approximately twofold. Besides, we
find that MEMIT performs more consistently than PMET in the FLEKE task with our
FedEdit framework. Our code is available at https://github.com/zongkaiz/FLEKE.
</summary>
    <author>
      <name>Zongkai Zhao</name>
    </author>
    <author>
      <name>Guozeng Xu</name>
    </author>
    <author>
      <name>Xiuhua Li</name>
    </author>
    <author>
      <name>Kaiwen Wei</name>
    </author>
    <author>
      <name>Jiang Zhong</name>
    </author>
    <link href="http://arxiv.org/abs/2502.15677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.15677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.15672v1</id>
    <updated>2025-02-21T18:56:02Z</updated>
    <published>2025-02-21T18:56:02Z</published>
    <title>VaViM and VaVAM: Autonomous Driving through Video Generative Modeling</title>
    <summary>  We explore the potential of large-scale generative video models for
autonomous driving, introducing an open-source auto-regressive video model
(VaViM) and its companion video-action model (VaVAM) to investigate how video
pre-training transfers to real-world driving. VaViM is a simple auto-regressive
video model that predicts frames using spatio-temporal token sequences. We show
that it captures the semantics and dynamics of driving scenes. VaVAM, the
video-action model, leverages the learned representations of VaViM to generate
driving trajectories through imitation learning. Together, the models form a
complete perception-to-action pipeline. We evaluate our models in open- and
closed-loop driving scenarios, revealing that video-based pre-training holds
promise for autonomous driving. Key insights include the semantic richness of
the learned representations, the benefits of scaling for video synthesis, and
the complex relationship between model size, data, and safety metrics in
closed-loop evaluations. We release code and model weights at
https://github.com/valeoai/VideoActionModel
</summary>
    <author>
      <name>Florent Bartoccioni</name>
    </author>
    <author>
      <name>Elias Ramzi</name>
    </author>
    <author>
      <name>Victor Besnier</name>
    </author>
    <author>
      <name>Shashanka Venkataramanan</name>
    </author>
    <author>
      <name>Tuan-Hung Vu</name>
    </author>
    <author>
      <name>Yihong Xu</name>
    </author>
    <author>
      <name>Loick Chambon</name>
    </author>
    <author>
      <name>Spyros Gidaris</name>
    </author>
    <author>
      <name>Serkan Odabas</name>
    </author>
    <author>
      <name>David Hurych</name>
    </author>
    <author>
      <name>Renaud Marlet</name>
    </author>
    <author>
      <name>Alexandre Boulch</name>
    </author>
    <author>
      <name>Mickael Chen</name>
    </author>
    <author>
      <name>Éloi Zablocki</name>
    </author>
    <author>
      <name>Andrei Bursuc</name>
    </author>
    <author>
      <name>Eduardo Valle</name>
    </author>
    <author>
      <name>Matthieu Cord</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code and model: https://github.com/valeoai/VideoActionModel, project
  page: https://valeoai.github.io/vavim-vavam/</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.15672v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.15672v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
