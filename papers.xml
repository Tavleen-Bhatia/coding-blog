<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-02-25T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">407321</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2502.17437v1</id>
    <updated>2025-02-24T18:59:56Z</updated>
    <published>2025-02-24T18:59:56Z</published>
    <title>Fractal Generative Models</title>
    <summary>  Modularization is a cornerstone of computer science, abstracting complex
functions into atomic building blocks. In this paper, we introduce a new level
of modularization by abstracting generative models into atomic generative
modules. Analogous to fractals in mathematics, our method constructs a new type
of generative model by recursively invoking atomic generative modules,
resulting in self-similar fractal architectures that we call fractal generative
models. As a running example, we instantiate our fractal framework using
autoregressive models as the atomic generative modules and examine it on the
challenging task of pixel-by-pixel image generation, demonstrating strong
performance in both likelihood estimation and generation quality. We hope this
work could open a new paradigm in generative modeling and provide a fertile
ground for future research. Code is available at
https://github.com/LTH14/fractalgen.
</summary>
    <author>
      <name>Tianhong Li</name>
    </author>
    <author>
      <name>Qinyi Sun</name>
    </author>
    <author>
      <name>Lijie Fan</name>
    </author>
    <author>
      <name>Kaiming He</name>
    </author>
    <link href="http://arxiv.org/abs/2502.17437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.17437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.17436v1</id>
    <updated>2025-02-24T18:59:55Z</updated>
    <published>2025-02-24T18:59:55Z</published>
    <title>Towards Hierarchical Rectified Flow</title>
    <summary>  We formulate a hierarchical rectified flow to model data distributions. It
hierarchically couples multiple ordinary differential equations (ODEs) and
defines a time-differentiable stochastic process that generates a data
distribution from a known source distribution. Each ODE resembles the ODE that
is solved in a classic rectified flow, but differs in its domain, i.e.,
location, velocity, acceleration, etc. Unlike the classic rectified flow
formulation, which formulates a single ODE in the location domain and only
captures the expected velocity field (sufficient to capture a multi-modal data
distribution), the hierarchical rectified flow formulation models the
multi-modal random velocity field, acceleration field, etc., in their entirety.
This more faithful modeling of the random velocity field enables integration
paths to intersect when the underlying ODE is solved during data generation.
Intersecting paths in turn lead to integration trajectories that are more
straight than those obtained in the classic rectified flow formulation, where
integration paths cannot intersect. This leads to modeling of data
distributions with fewer neural function evaluations. We empirically verify
this on synthetic 1D and 2D data as well as MNIST, CIFAR-10, and ImageNet-32
data. Code is available at: https://riccizz.github.io/HRF/.
</summary>
    <author>
      <name>Yichi Zhang</name>
    </author>
    <author>
      <name>Yici Yan</name>
    </author>
    <author>
      <name>Alex Schwing</name>
    </author>
    <author>
      <name>Zhizhen Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2025; Project Page: https://riccizz.github.io/HRF/</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.17436v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.17436v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.17433v1</id>
    <updated>2025-02-24T18:59:10Z</updated>
    <published>2025-02-24T18:59:10Z</published>
    <title>Dynamical phases of short-term memory mechanisms in RNNs</title>
    <summary>  Short-term memory is essential for cognitive processing, yet our
understanding of its neural mechanisms remains unclear. A key focus in
neuroscience has been the study of sequential activity patterns, where neurons
fire one after another within large networks, to explain how information is
maintained. While recurrent connections were shown to drive sequential
dynamics, a mechanistic understanding of this process still remains unknown. In
this work, we first introduce two unique mechanisms that can subserve
short-term memory: slow-point manifolds generating direct sequences or limit
cycles providing temporally localized approximations. Then, through analytical
models, we identify fundamental properties that govern the selection of these
mechanisms, \textit{i.e.}, we derive theoretical scaling laws for critical
learning rates as a function of the delay period length, beyond which no
learning is possible. We empirically verify these observations by training and
evaluating more than 35,000 recurrent neural networks (RNNs) that we will
publicly release upon publication. Overall, our work provides new insights into
short-term memory mechanisms and proposes experimentally testable predictions
for systems neuroscience.
</summary>
    <author>
      <name>Bariscan Kurtkaya</name>
    </author>
    <author>
      <name>Fatih Dinc</name>
    </author>
    <author>
      <name>Mert Yuksekgonul</name>
    </author>
    <author>
      <name>Marta Blanco-Pozo</name>
    </author>
    <author>
      <name>Ege Cirakman</name>
    </author>
    <author>
      <name>Mark Schnitzer</name>
    </author>
    <author>
      <name>Yucel Yemez</name>
    </author>
    <author>
      <name>Hidenori Tanaka</name>
    </author>
    <author>
      <name>Peng Yuan</name>
    </author>
    <author>
      <name>Nina Miolane</name>
    </author>
    <link href="http://arxiv.org/abs/2502.17433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.17433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.17432v1</id>
    <updated>2025-02-24T18:59:07Z</updated>
    <published>2025-02-24T18:59:07Z</published>
    <title>FACTR: Force-Attending Curriculum Training for Contact-Rich Policy
  Learning</title>
    <summary>  Many contact-rich tasks humans perform, such as box pickup or rolling dough,
rely on force feedback for reliable execution. However, this force information,
which is readily available in most robot arms, is not commonly used in
teleoperation and policy learning. Consequently, robot behavior is often
limited to quasi-static kinematic tasks that do not require intricate
force-feedback. In this paper, we first present a low-cost, intuitive,
bilateral teleoperation setup that relays external forces of the follower arm
back to the teacher arm, facilitating data collection for complex, contact-rich
tasks. We then introduce FACTR, a policy learning method that employs a
curriculum which corrupts the visual input with decreasing intensity throughout
training. The curriculum prevents our transformer-based policy from
over-fitting to the visual input and guides the policy to properly attend to
the force modality. We demonstrate that by fully utilizing the force
information, our method significantly improves generalization to unseen objects
by 43\% compared to baseline approaches without a curriculum. Video results and
instructions at https://jasonjzliu.com/factr/
</summary>
    <author>
      <name>Jason Jingzhou Liu</name>
    </author>
    <author>
      <name>Yulong Li</name>
    </author>
    <author>
      <name>Kenneth Shaw</name>
    </author>
    <author>
      <name>Tony Tao</name>
    </author>
    <author>
      <name>Ruslan Salakhutdinov</name>
    </author>
    <author>
      <name>Deepak Pathak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Website at https://jasonjzliu.com/factr/</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.17432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.17432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.17429v1</id>
    <updated>2025-02-24T18:58:58Z</updated>
    <published>2025-02-24T18:58:58Z</published>
    <title>CLIMB-3D: Continual Learning for Imbalanced 3D Instance Segmentation</title>
    <summary>  While 3D instance segmentation has made significant progress, current methods
struggle to address realistic scenarios where new categories emerge over time
with natural class imbalance. This limitation stems from existing datasets,
which typically feature few well-balanced classes. Although few datasets
include unbalanced class annotations, they lack the diverse incremental
scenarios necessary for evaluating methods under incremental settings.
Addressing these challenges requires frameworks that handle both incremental
learning and class imbalance. However, existing methods for 3D incremental
segmentation rely heavily on large exemplar replay, focusing only on
incremental learning while neglecting class imbalance. Moreover,
frequency-based tuning for balanced learning is impractical in these setups due
to the lack of prior class statistics. To overcome these limitations, we
propose a framework to tackle both \textbf{C}ontinual \textbf{L}earning and
class \textbf{Imb}alance for \textbf{3D} instance segmentation
(\textbf{CLIMB-3D}). Our proposed approach combines Exemplar Replay (ER),
Knowledge Distillation (KD), and a novel Imbalance Correction (IC) module.
Unlike prior methods, our framework minimizes ER usage, with KD preventing
forgetting and supporting the IC module in compiling past class statistics to
balance learning of rare classes during incremental updates. To evaluate our
framework, we design three incremental scenarios based on class frequency,
semantic similarity, and random grouping that aim to mirror real-world dynamics
in 3D environments. Experimental results show that our proposed framework
achieves state-of-the-art performance, with an increase of up to 16.76\% in mAP
compared to the baseline. Code will be available at:
\href{https://github.com/vgthengane/CLIMB3D}{https://github.com/vgthengane/CLIMB3D}
</summary>
    <author>
      <name>Vishal Thengane</name>
    </author>
    <author>
      <name>Jean Lahoud</name>
    </author>
    <author>
      <name>Hisham Cholakkal</name>
    </author>
    <author>
      <name>Rao Muhammad Anwer</name>
    </author>
    <author>
      <name>Lu Yin</name>
    </author>
    <author>
      <name>Xiatian Zhu</name>
    </author>
    <author>
      <name>Salman Khan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code: https://github.com/vgthengane/CLIMB3D</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.17429v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.17429v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
