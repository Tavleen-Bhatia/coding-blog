<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-03-04T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">408552</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2502.21321v1</id>
    <updated>2025-02-28T18:59:54Z</updated>
    <published>2025-02-28T18:59:54Z</published>
    <title>LLM Post-Training: A Deep Dive into Reasoning Large Language Models</title>
    <summary>  Large Language Models (LLMs) have transformed the natural language processing
landscape and brought to life diverse applications. Pretraining on vast
web-scale data has laid the foundation for these models, yet the research
community is now increasingly shifting focus toward post-training techniques to
achieve further breakthroughs. While pretraining provides a broad linguistic
foundation, post-training methods enable LLMs to refine their knowledge,
improve reasoning, enhance factual accuracy, and align more effectively with
user intents and ethical considerations. Fine-tuning, reinforcement learning,
and test-time scaling have emerged as critical strategies for optimizing LLMs
performance, ensuring robustness, and improving adaptability across various
real-world tasks. This survey provides a systematic exploration of
post-training methodologies, analyzing their role in refining LLMs beyond
pretraining, addressing key challenges such as catastrophic forgetting, reward
hacking, and inference-time trade-offs. We highlight emerging directions in
model alignment, scalable adaptation, and inference-time reasoning, and outline
future research directions. We also provide a public repository to continually
track developments in this fast-evolving field:
https://github.com/mbzuai-oryx/Awesome-LLM-Post-training.
</summary>
    <author>
      <name>Komal Kumar</name>
    </author>
    <author>
      <name>Tajamul Ashraf</name>
    </author>
    <author>
      <name>Omkar Thawakar</name>
    </author>
    <author>
      <name>Rao Muhammad Anwer</name>
    </author>
    <author>
      <name>Hisham Cholakkal</name>
    </author>
    <author>
      <name>Mubarak Shah</name>
    </author>
    <author>
      <name>Ming-Hsuan Yang</name>
    </author>
    <author>
      <name>Phillip H. S. Torr</name>
    </author>
    <author>
      <name>Salman Khan</name>
    </author>
    <author>
      <name>Fahad Shahbaz Khan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 7 figures, 3 tables, 375 references</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.21321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.21321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.21320v1</id>
    <updated>2025-02-28T18:59:52Z</updated>
    <published>2025-02-28T18:59:52Z</published>
    <title>TomoSelfDEQ: Self-Supervised Deep Equilibrium Learning for Sparse-Angle
  CT Reconstruction</title>
    <summary>  Deep learning has emerged as a powerful tool for solving inverse problems in
imaging, including computed tomography (CT). However, most approaches require
paired training data with ground truth images, which can be difficult to
obtain, e.g., in medical applications. We present TomoSelfDEQ, a
self-supervised Deep Equilibrium (DEQ) framework for sparse-angle CT
reconstruction that trains directly on undersampled measurements. We establish
theoretical guarantees showing that, under suitable assumptions, our
self-supervised updates match those of fully-supervised training with a loss
including the (possibly non-unitary) forward operator like the CT forward map.
Numerical experiments on sparse-angle CT data confirm this finding, also
demonstrating that TomoSelfDEQ outperforms existing self-supervised methods,
achieving state-of-the-art results with as few as 16 projection angles.
</summary>
    <author>
      <name>Tatiana A. Bubba</name>
    </author>
    <author>
      <name>Matteo Santacesaria</name>
    </author>
    <author>
      <name>Andrea Sebastiani</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">proceedings of Scale Space and Variational Methods in Computer
  Vision, SSVM 2025</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2502.21320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.21320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.21317v1</id>
    <updated>2025-02-28T18:59:28Z</updated>
    <published>2025-02-28T18:59:28Z</published>
    <title>Assessing zero-shot generalisation behaviour in graph-neural-network
  interatomic potentials</title>
    <summary>  With the rapidly growing availability of machine-learned interatomic
potential (MLIP) models for chemistry, much current research focuses on the
development of generally applicable and ``foundational'' MLIPs. An important
question in this context is whether, and how well, such models can transfer
from one application domain to another. Here, we assess this transferability
for an MLIP model at the interface of materials and molecular chemistry.
Specifically, we study GO-MACE-23, a model designed for the extended covalent
network of graphene oxide, and quantify its zero-shot performance for small,
isolated molecules and chemical reactions outside its direct scope--in direct
comparison with a state-of-the-art model which has been trained in-domain. Our
work provides quantitative insight into the transfer and generalisation ability
of graph-neural-network potentials and, more generally, makes a step towards
the more widespread applicability of MLIPs in chemistry.
</summary>
    <author>
      <name>Chiheb Ben Mahmoud</name>
    </author>
    <author>
      <name>Zakariya El-Machachi</name>
    </author>
    <author>
      <name>Krystian A. Gierczak</name>
    </author>
    <author>
      <name>John L. A. Gardner</name>
    </author>
    <author>
      <name>Volker L. Deringer</name>
    </author>
    <link href="http://arxiv.org/abs/2502.21317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.21317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.21313v1</id>
    <updated>2025-02-28T18:54:51Z</updated>
    <published>2025-02-28T18:54:51Z</published>
    <title>Unsupervised Parameter Efficient Source-free Post-pretraining</title>
    <summary>  Following the success in NLP, the best vision models are now in the billion
parameter ranges. Adapting these large models to a target distribution has
become computationally and economically prohibitive. Addressing this challenge,
we introduce UpStep, an Unsupervised Parameter-efficient Source-free
post-pretraining approach, designed to efficiently adapt a base model from a
source domain to a target domain: i) we design a self-supervised training
scheme to adapt a pretrained model on an unlabeled target domain in a setting
where source domain data is unavailable. Such source-free setting comes with
the risk of catastrophic forgetting, hence, ii) we propose center vector
regularization (CVR), a set of auxiliary operations that minimize catastrophic
forgetting and additionally reduces the computational cost by skipping
backpropagation in 50\% of the training iterations. Finally iii) we perform
this adaptation process in a parameter-efficient way by adapting the pretrained
model through low-rank adaptation methods, resulting in a fraction of
parameters to optimize. We utilize various general backbone architectures, both
supervised and unsupervised, trained on Imagenet as our base model and adapt
them to a diverse set of eight target domains demonstrating the adaptability
and generalizability of our proposed approach.
</summary>
    <author>
      <name>Abhishek Jha</name>
    </author>
    <author>
      <name>Tinne Tuytelaars</name>
    </author>
    <author>
      <name>Yuki M. Asano</name>
    </author>
    <link href="http://arxiv.org/abs/2502.21313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.21313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.21311v1</id>
    <updated>2025-02-28T18:53:32Z</updated>
    <published>2025-02-28T18:53:32Z</published>
    <title>AutoComb: Automated Comb Sign Detector for 3D CTE Scans</title>
    <summary>  Comb Sign is an important imaging biomarker to detect multiple
gastrointestinal diseases. It shows up as increased blood flow along the
intestinal wall indicating potential abnormality, which helps doctors diagnose
inflammatory conditions. Despite its clinical significance, current detection
methods are manual, time-intensive, and prone to subjective interpretation due
to the need for multi-planar image-orientation. To the best of our knowledge,
we are the first to propose a fully automated technique for the detection of
Comb Sign from CTE scans. Our novel approach is based on developing a
probabilistic map that shows areas of pathological hypervascularity by
identifying fine vascular bifurcations and wall enhancement via processing
through stepwise algorithmic modules. These modules include utilising deep
learning segmentation model, a Gaussian Mixture Model (GMM), vessel extraction
using vesselness filter, iterative probabilistic enhancement of vesselness via
neighborhood maximization and a distance-based weighting scheme over the
vessels. Experimental results demonstrate that our pipeline effectively
identifies Comb Sign, offering an objective, accurate, and reliable tool to
enhance diagnostic accuracy in Crohn's disease and related hypervascular
conditions where Comb Sign is considered as one of the important biomarkers.
</summary>
    <author>
      <name>Shashwat Gupta</name>
    </author>
    <author>
      <name>Sarthak Gupta</name>
    </author>
    <author>
      <name>Akshan Agrawal</name>
    </author>
    <author>
      <name>Mahim Naaz</name>
    </author>
    <author>
      <name>Rajanikanth Yadav</name>
    </author>
    <author>
      <name>Priyanka Bagade</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.21311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.21311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
