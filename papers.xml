<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-04-23T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">420409</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.16084v1</id>
    <updated>2025-04-22T17:59:56Z</updated>
    <published>2025-04-22T17:59:56Z</published>
    <title>TTRL: Test-Time Reinforcement Learning</title>
    <summary>  This paper investigates Reinforcement Learning (RL) on data without explicit
labels for reasoning tasks in Large Language Models (LLMs). The core challenge
of the problem is reward estimation during inference while not having access to
ground-truth information. While this setting appears elusive, we find that
common practices in Test-Time Scaling (TTS), such as majority voting, yield
surprisingly effective rewards suitable for driving RL training. In this work,
we introduce Test-Time Reinforcement Learning (TTRL), a novel method for
training LLMs using RL on unlabeled data. TTRL enables self-evolution of LLMs
by utilizing the priors in the pre-trained models. Our experiments demonstrate
that TTRL consistently improves performance across a variety of tasks and
models. Notably, TTRL boosts the pass@1 performance of Qwen-2.5-Math-7B by
approximately 159% on the AIME 2024 with only unlabeled test data. Furthermore,
although TTRL is only supervised by the Maj@N metric, TTRL has demonstrated
performance to consistently surpass the upper limit of the initial model, and
approach the performance of models trained directly on test data with
ground-truth labels. Our experimental findings validate the general
effectiveness of TTRL across various tasks, and highlight TTRL's potential for
broader tasks and domains. GitHub: https://github.com/PRIME-RL/TTRL
</summary>
    <author>
      <name>Yuxin Zuo</name>
    </author>
    <author>
      <name>Kaiyan Zhang</name>
    </author>
    <author>
      <name>Shang Qu</name>
    </author>
    <author>
      <name>Li Sheng</name>
    </author>
    <author>
      <name>Xuekai Zhu</name>
    </author>
    <author>
      <name>Biqing Qi</name>
    </author>
    <author>
      <name>Youbang Sun</name>
    </author>
    <author>
      <name>Ganqu Cui</name>
    </author>
    <author>
      <name>Ning Ding</name>
    </author>
    <author>
      <name>Bowen Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2504.16084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.16084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.16083v1</id>
    <updated>2025-04-22T17:59:51Z</updated>
    <published>2025-04-22T17:59:51Z</published>
    <title>MMInference: Accelerating Pre-filling for Long-Context VLMs via
  Modality-Aware Permutation Sparse Attention</title>
    <summary>  The integration of long-context capabilities with visual understanding
unlocks unprecedented potential for Vision Language Models (VLMs). However, the
quadratic attention complexity during the pre-filling phase remains a
significant obstacle to real-world deployment. To overcome this limitation, we
introduce MMInference (Multimodality Million tokens Inference), a dynamic
sparse attention method that accelerates the prefilling stage for long-context
multi-modal inputs. First, our analysis reveals that the temporal and spatial
locality of video input leads to a unique sparse pattern, the Grid pattern.
Simultaneously, VLMs exhibit markedly different sparse distributions across
different modalities. We introduce a permutation-based method to leverage the
unique Grid pattern and handle modality boundary issues. By offline search the
optimal sparse patterns for each head, MMInference constructs the sparse
distribution dynamically based on the input. We also provide optimized GPU
kernels for efficient sparse computations. Notably, MMInference integrates
seamlessly into existing VLM pipelines without any model modifications or
fine-tuning. Experiments on multi-modal benchmarks-including Video QA,
Captioning, VisionNIAH, and Mixed-Modality NIAH-with state-of-the-art
long-context VLMs (LongVila, LlavaVideo, VideoChat-Flash, Qwen2.5-VL) show that
MMInference accelerates the pre-filling stage by up to 8.3x at 1M tokens while
maintaining accuracy. Our code is available at https://aka.ms/MMInference.
</summary>
    <author>
      <name>Yucheng Li</name>
    </author>
    <author>
      <name>Huiqiang Jiang</name>
    </author>
    <author>
      <name>Chengruidong Zhang</name>
    </author>
    <author>
      <name>Qianhui Wu</name>
    </author>
    <author>
      <name>Xufang Luo</name>
    </author>
    <author>
      <name>Surin Ahn</name>
    </author>
    <author>
      <name>Amir H. Abdi</name>
    </author>
    <author>
      <name>Dongsheng Li</name>
    </author>
    <author>
      <name>Jianfeng Gao</name>
    </author>
    <author>
      <name>Yuqing Yang</name>
    </author>
    <author>
      <name>Lili Qiu</name>
    </author>
    <link href="http://arxiv.org/abs/2504.16083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.16083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.16081v1</id>
    <updated>2025-04-22T17:59:17Z</updated>
    <published>2025-04-22T17:59:17Z</published>
    <title>Survey of Video Diffusion Models: Foundations, Implementations, and
  Applications</title>
    <summary>  Recent advances in diffusion models have revolutionized video generation,
offering superior temporal consistency and visual quality compared to
traditional generative adversarial networks-based approaches. While this
emerging field shows tremendous promise in applications, it faces significant
challenges in motion consistency, computational efficiency, and ethical
considerations. This survey provides a comprehensive review of diffusion-based
video generation, examining its evolution, technical foundations, and practical
applications. We present a systematic taxonomy of current methodologies,
analyze architectural innovations and optimization strategies, and investigate
applications across low-level vision tasks such as denoising and
super-resolution. Additionally, we explore the synergies between diffusionbased
video generation and related domains, including video representation learning,
question answering, and retrieval. Compared to the existing surveys (Lei et
al., 2024a;b; Melnik et al., 2024; Cao et al., 2023; Xing et al., 2024c) which
focus on specific aspects of video generation, such as human video synthesis
(Lei et al., 2024a) or long-form content generation (Lei et al., 2024b), our
work provides a broader, more updated, and more fine-grained perspective on
diffusion-based approaches with a special section for evaluation metrics,
industry solutions, and training engineering techniques in video generation.
This survey serves as a foundational resource for researchers and practitioners
working at the intersection of diffusion models and video generation, providing
insights into both the theoretical frameworks and practical implementations
that drive this rapidly evolving field. A structured list of related works
involved in this survey is also available on
https://github.com/Eyeline-Research/Survey-Video-Diffusion.
</summary>
    <author>
      <name>Yimu Wang</name>
    </author>
    <author>
      <name>Xuye Liu</name>
    </author>
    <author>
      <name>Wei Pang</name>
    </author>
    <author>
      <name>Li Ma</name>
    </author>
    <author>
      <name>Shuai Yuan</name>
    </author>
    <author>
      <name>Paul Debevec</name>
    </author>
    <author>
      <name>Ning Yu</name>
    </author>
    <link href="http://arxiv.org/abs/2504.16081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.16081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.16078v1</id>
    <updated>2025-04-22T17:57:14Z</updated>
    <published>2025-04-22T17:57:14Z</published>
    <title>LLMs are Greedy Agents: Effects of RL Fine-tuning on Decision-Making
  Abilities</title>
    <summary>  The success of Large Language Models (LLMs) has sparked interest in various
agentic applications. A key hypothesis is that LLMs, leveraging common sense
and Chain-of-Thought (CoT) reasoning, can effectively explore and efficiently
solve complex domains. However, LLM agents have been found to suffer from
sub-optimal exploration and the knowing-doing gap, the inability to effectively
act on knowledge present in the model. In this work, we systematically study
why LLMs perform sub-optimally in decision-making scenarios. In particular, we
closely examine three prevalent failure modes: greediness, frequency bias, and
the knowing-doing gap. We propose mitigation of these shortcomings by
fine-tuning via Reinforcement Learning (RL) on self-generated CoT rationales.
Our experiments across multi-armed bandits, contextual bandits, and
Tic-tac-toe, demonstrate that RL fine-tuning enhances the decision-making
abilities of LLMs by increasing exploration and narrowing the knowing-doing
gap. Finally, we study both classic exploration mechanisms, such as
$\epsilon$-greedy, and LLM-specific approaches, such as self-correction and
self-consistency, to enable more effective fine-tuning of LLMs for
decision-making.
</summary>
    <author>
      <name>Thomas Schmied</name>
    </author>
    <author>
      <name>Jörg Bornschein</name>
    </author>
    <author>
      <name>Jordi Grau-Moya</name>
    </author>
    <author>
      <name>Markus Wulfmeier</name>
    </author>
    <author>
      <name>Razvan Pascanu</name>
    </author>
    <link href="http://arxiv.org/abs/2504.16078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.16078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.16077v1</id>
    <updated>2025-04-22T17:55:56Z</updated>
    <published>2025-04-22T17:55:56Z</published>
    <title>Intent-aware Diffusion with Contrastive Learning for Sequential
  Recommendation</title>
    <summary>  Contrastive learning has proven effective in training sequential
recommendation models by incorporating self-supervised signals from augmented
views. Most existing methods generate multiple views from the same interaction
sequence through stochastic data augmentation, aiming to align their
representations in the embedding space. However, users typically have specific
intents when purchasing items (e.g., buying clothes as gifts or cosmetics for
beauty). Random data augmentation used in existing methods may introduce noise,
disrupting the latent intent information implicit in the original interaction
sequence. Moreover, using noisy augmented sequences in contrastive learning may
mislead the model to focus on irrelevant features, distorting the embedding
space and failing to capture users' true behavior patterns and intents. To
address these issues, we propose Intent-aware Diffusion with contrastive
learning for sequential Recommendation (InDiRec). The core idea is to generate
item sequences aligned with users' purchasing intents, thus providing more
reliable augmented views for contrastive learning. Specifically, InDiRec first
performs intent clustering on sequence representations using K-means to build
intent-guided signals. Next, it retrieves the intent representation of the
target interaction sequence to guide a conditional diffusion model, generating
positive views that share the same underlying intent. Finally, contrastive
learning is applied to maximize representation consistency between these
intent-aligned views and the original sequence. Extensive experiments on five
public datasets demonstrate that InDiRec achieves superior performance compared
to existing baselines, learning more robust representations even under noisy
and sparse data conditions.
</summary>
    <author>
      <name>Yuanpeng Qu</name>
    </author>
    <author>
      <name>Hajime Nobuhara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at SIGIR 2025. 10 pages, 6 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.16077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.16077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
