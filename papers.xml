<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-04-29T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">421483</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.20041v1</id>
    <updated>2025-04-28T17:59:54Z</updated>
    <published>2025-04-28T17:59:54Z</published>
    <title>Learning Streaming Video Representation via Multitask Training</title>
    <summary>  Understanding continuous video streams plays a fundamental role in real-time
applications including embodied AI and autonomous driving. Unlike offline video
understanding, streaming video understanding requires the ability to process
video streams frame by frame, preserve historical information, and make
low-latency decisions.To address these challenges, our main contributions are
three-fold. (i) We develop a novel streaming video backbone, termed as
StreamFormer, by incorporating causal temporal attention into a pre-trained
vision transformer. This enables efficient streaming video processing while
maintaining image representation capability.(ii) To train StreamFormer, we
propose to unify diverse spatial-temporal video understanding tasks within a
multitask visual-language alignment framework. Hence, StreamFormer learns
global semantics, temporal dynamics, and fine-grained spatial relationships
simultaneously. (iii) We conduct extensive experiments on online action
detection, online video instance segmentation, and video question answering.
StreamFormer achieves competitive results while maintaining efficiency,
demonstrating its potential for real-time applications.
</summary>
    <author>
      <name>Yibin Yan</name>
    </author>
    <author>
      <name>Jilan Xu</name>
    </author>
    <author>
      <name>Shangzhe Di</name>
    </author>
    <author>
      <name>Yikun Liu</name>
    </author>
    <author>
      <name>Yudi Shi</name>
    </author>
    <author>
      <name>Qirui Chen</name>
    </author>
    <author>
      <name>Zeqian Li</name>
    </author>
    <author>
      <name>Yifei Huang</name>
    </author>
    <author>
      <name>Weidi Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report. Project Page:
  https://go2heart.github.io/streamformer</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.20041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.20041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.20040v1</id>
    <updated>2025-04-28T17:59:52Z</updated>
    <published>2025-04-28T17:59:52Z</published>
    <title>MP-SfM: Monocular Surface Priors for Robust Structure-from-Motion</title>
    <summary>  While Structure-from-Motion (SfM) has seen much progress over the years,
state-of-the-art systems are prone to failure when facing extreme viewpoint
changes in low-overlap, low-parallax or high-symmetry scenarios. Because
capturing images that avoid these pitfalls is challenging, this severely limits
the wider use of SfM, especially by non-expert users. We overcome these
limitations by augmenting the classical SfM paradigm with monocular depth and
normal priors inferred by deep neural networks. Thanks to a tight integration
of monocular and multi-view constraints, our approach significantly outperforms
existing ones under extreme viewpoint changes, while maintaining strong
performance in standard conditions. We also show that monocular priors can help
reject faulty associations due to symmetries, which is a long-standing problem
for SfM. This makes our approach the first capable of reliably reconstructing
challenging indoor environments from few images. Through principled uncertainty
propagation, it is robust to errors in the priors, can handle priors inferred
by different models with little tuning, and will thus easily benefit from
future progress in monocular depth and normal estimation. Our code is publicly
available at https://github.com/cvg/mpsfm.
</summary>
    <author>
      <name>Zador Pataki</name>
    </author>
    <author>
      <name>Paul-Edouard Sarlin</name>
    </author>
    <author>
      <name>Johannes L. Sch√∂nberger</name>
    </author>
    <author>
      <name>Marc Pollefeys</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.20040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.20040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.20039v1</id>
    <updated>2025-04-28T17:59:28Z</updated>
    <published>2025-04-28T17:59:28Z</published>
    <title>AutoJudge: Judge Decoding Without Manual Annotation</title>
    <summary>  We introduce AutoJudge, a framework that accelerates large language model
(LLM) inference with task-specific lossy speculative decoding. Instead of
matching the original model output distribution token-by-token, we identify
which of the generated tokens affect the downstream quality of the generated
response, relaxing the guarantee so that the "unimportant" tokens can be
generated faster. Our approach relies on a semi-greedy search algorithm to test
which of the mismatches between target and draft model should be corrected to
preserve quality, and which ones may be skipped. We then train a lightweight
classifier based on existing LLM embeddings to predict, at inference time,
which mismatching tokens can be safely accepted without compromising the final
answer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B
(target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more
accepted tokens per verification cycle with under 1% degradation in answer
accuracy compared to standard speculative decoding and over 2x with small loss
in accuracy. When applied to the LiveCodeBench benchmark, our approach
automatically detects other, programming-specific important tokens and shows
similar speedups, demonstrating its ability to generalize across tasks.
</summary>
    <author>
      <name>Roman Garipov</name>
    </author>
    <author>
      <name>Fedor Velikonivtsev</name>
    </author>
    <author>
      <name>Ruslan Svirschevski</name>
    </author>
    <author>
      <name>Vage Egiazarian</name>
    </author>
    <author>
      <name>Max Ryabinin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint, Work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.20039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.20039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.20033v1</id>
    <updated>2025-04-28T17:56:04Z</updated>
    <published>2025-04-28T17:56:04Z</published>
    <title>Mitigating Catastrophic Forgetting in the Incremental Learning of
  Medical Images</title>
    <summary>  This paper proposes an Incremental Learning (IL) approach to enhance the
accuracy and efficiency of deep learning models in analyzing T2-weighted (T2w)
MRI medical images prostate cancer detection using the PI-CAI dataset. We used
multiple health centers' artificial intelligence and radiology data, focused on
different tasks that looked at prostate cancer detection using MRI (PI-CAI). We
utilized Knowledge Distillation (KD), as it employs generated images from past
tasks to guide the training of models for subsequent tasks. The approach
yielded improved performance and faster convergence of the models. To
demonstrate the versatility and robustness of our approach, we evaluated it on
the PI-CAI dataset, a diverse set of medical imaging modalities including OCT
and PathMNIST, and the benchmark continual learning dataset CIFAR-10. Our
results indicate that KD can be a promising technique for IL in medical image
analysis in which data is sourced from individual health centers and the
storage of large datasets is not feasible. By using generated images from prior
tasks, our method enables the model to retain and apply previously acquired
knowledge without direct access to the original data.
</summary>
    <author>
      <name>Sara Yavari</name>
    </author>
    <author>
      <name>Jacob Furst</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 Pages, 3 Figures, 3 Tables, 1 Algorithm, This paper will be
  updated</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.20033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.20033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.20024v1</id>
    <updated>2025-04-28T17:48:43Z</updated>
    <published>2025-04-28T17:48:43Z</published>
    <title>SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning</title>
    <summary>  Recent studies in 3D spatial reasoning explore data-driven approaches and
achieve enhanced spatial reasoning performance with reinforcement learning
(RL). However, these methods typically perform spatial reasoning in an implicit
manner, and it remains underexplored whether the acquired 3D knowledge
generalizes to unseen question types at any stage of the training. In this work
we introduce SpatialReasoner, a novel large vision-language model (LVLM) that
address 3D spatial reasoning with explicit 3D representations shared between
stages -- 3D perception, computation, and reasoning. Explicit 3D
representations provide a coherent interface that supports advanced 3D spatial
reasoning and enable us to study the factual errors made by LVLMs. Results show
that our SpatialReasoner achieve improved performance on a variety of spatial
reasoning benchmarks and generalizes better when evaluating on novel 3D spatial
reasoning questions. Our study bridges the 3D parsing capabilities of prior
visual foundation models with the powerful reasoning abilities of large
language models, opening new directions for 3D spatial reasoning.
</summary>
    <author>
      <name>Wufei Ma</name>
    </author>
    <author>
      <name>Yu-Cheng Chou</name>
    </author>
    <author>
      <name>Qihao Liu</name>
    </author>
    <author>
      <name>Xingrui Wang</name>
    </author>
    <author>
      <name>Celso de Melo</name>
    </author>
    <author>
      <name>Jieneng Chen</name>
    </author>
    <author>
      <name>Jianwen Xie</name>
    </author>
    <author>
      <name>Alan Yuille</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://spatial-reasoner.github.io</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.20024v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.20024v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
