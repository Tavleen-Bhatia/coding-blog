<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-05-27T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">428723</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.20152v1</id>
    <updated>2025-05-26T15:55:28Z</updated>
    <published>2025-05-26T15:55:28Z</published>
    <title>Hard Negative Contrastive Learning for Fine-Grained Geometric
  Understanding in Large Multimodal Models</title>
    <summary>  Benefiting from contrastively trained visual encoders on large-scale natural
scene images, Large Multimodal Models (LMMs) have achieved remarkable
performance across various visual perception tasks. However, the inherent
limitations of contrastive learning upon summarized descriptions fundamentally
restrict the capabilities of models in meticulous reasoning, particularly in
crucial scenarios of geometric problem-solving. To enhance geometric
understanding, we propose a novel hard negative contrastive learning framework
for the vision encoder, which combines image-based contrastive learning using
generation-based hard negatives created by perturbing diagram generation code,
and text-based contrastive learning using rule-based negatives derived from
modified geometric descriptions and retrieval-based negatives selected based on
caption similarity. We train CLIP using our strong negative learning method,
namely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for
geometric problem-solving. Experiments show that our trained model, MMGeoLM,
significantly outperforms other open-source models on three geometric reasoning
benchmarks. Even with a size of 7B, it can rival powerful closed-source models
like GPT-4o. We further study the impact of different negative sample
construction methods and the number of negative samples on the geometric
reasoning performance of LMM, yielding fruitful conclusions. The code and
dataset are available at https://github.com/THU-KEG/MMGeoLM.
</summary>
    <author>
      <name>Kai Sun</name>
    </author>
    <author>
      <name>Yushi Bai</name>
    </author>
    <author>
      <name>Zhen Yang</name>
    </author>
    <author>
      <name>Jiajie Zhang</name>
    </author>
    <author>
      <name>Ji Qi</name>
    </author>
    <author>
      <name>Lei Hou</name>
    </author>
    <author>
      <name>Juanzi Li</name>
    </author>
    <link href="http://arxiv.org/abs/2505.20152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.20152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.20150v1</id>
    <updated>2025-05-26T15:53:09Z</updated>
    <published>2025-05-26T15:53:09Z</published>
    <title>On the (Non) Injectivity of Piecewise Linear Janossy Pooling</title>
    <summary>  Multiset functions, which are functions that map multisets to vectors, are a
fundamental tool in the construction of neural networks for multisets and
graphs. To guarantee that the vector representation of the multiset is
faithful, it is often desirable to have multiset mappings that are both
injective and bi-Lipschitz. Currently, there are several constructions of
multiset functions achieving both these guarantees, leading to improved
performance in some tasks but often also to higher compute time than standard
constructions. Accordingly, it is natural to inquire whether simpler multiset
functions achieving the same guarantees are available. In this paper, we make a
large step towards giving a negative answer to this question. We consider the
family of k-ary Janossy pooling, which includes many of the most popular
multiset models, and prove that no piecewise linear Janossy pooling function
can be injective. On the positive side, we show that when restricted to
multisets without multiplicities, even simple deep-sets models suffice for
injectivity and bi-Lipschitzness.
</summary>
    <author>
      <name>Ilai Reshef</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technion - Israel Institute of Technology</arxiv:affiliation>
    </author>
    <author>
      <name>Nadav Dym</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Technion - Israel Institute of Technology</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.20150v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.20150v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.20149v1</id>
    <updated>2025-05-26T15:49:44Z</updated>
    <published>2025-05-26T15:49:44Z</published>
    <title>Improvement Strategies for Few-Shot Learning in OCT Image Classification
  of Rare Retinal Diseases</title>
    <summary>  This paper focuses on using few-shot learning to improve the accuracy of
classifying OCT diagnosis images with major and rare classes. We used the
GAN-based augmentation strategy as a baseline and introduced several novel
methods to further enhance our model. The proposed strategy contains U-GAT-IT
for improving the generative part and uses the data balance technique to narrow
down the skew of accuracy between all categories. The best model obtained was
built with CBAM attention mechanism and fine-tuned InceptionV3, and achieved an
overall accuracy of 97.85%, representing a significant improvement over the
original baseline.
</summary>
    <author>
      <name>Cheng-Yu Tai</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dixson</arxiv:affiliation>
    </author>
    <author>
      <name>Ching-Wen Chen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dixson</arxiv:affiliation>
    </author>
    <author>
      <name>Chi-Chin Wu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dixson</arxiv:affiliation>
    </author>
    <author>
      <name>Bo-Chen Chiu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dixson</arxiv:affiliation>
    </author>
    <author>
      <name> Cheng-Hung</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dixson</arxiv:affiliation>
    </author>
    <author>
      <name> Lin</name>
    </author>
    <author>
      <name>Cheng-Kai Lu</name>
    </author>
    <author>
      <name>Jia-Kang Wang</name>
    </author>
    <author>
      <name>Tzu-Lun Huang</name>
    </author>
    <link href="http://arxiv.org/abs/2505.20149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.20149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.20147v1</id>
    <updated>2025-05-26T15:46:53Z</updated>
    <published>2025-05-26T15:46:53Z</published>
    <title>FUDOKI: Discrete Flow-based Unified Understanding and Generation via
  Kinetic-Optimal Velocities</title>
    <summary>  The rapid progress of large language models (LLMs) has catalyzed the
emergence of multimodal large language models (MLLMs) that unify visual
understanding and image generation within a single framework. However, most
existing MLLMs rely on autoregressive (AR) architectures, which impose inherent
limitations on future development, such as the raster-scan order in image
generation and restricted reasoning abilities in causal context modeling. In
this work, we challenge the dominance of AR-based approaches by introducing
FUDOKI, a unified multimodal model purely based on discrete flow matching, as
an alternative to conventional AR paradigms. By leveraging metric-induced
probability paths with kinetic optimal velocities, our framework goes beyond
the previous masking-based corruption process, enabling iterative refinement
with self-correction capability and richer bidirectional context integration
during generation. To mitigate the high cost of training from scratch, we
initialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to
the discrete flow matching paradigm. Experimental results show that FUDOKI
achieves performance comparable to state-of-the-art AR-based MLLMs across both
visual understanding and image generation tasks, highlighting its potential as
a foundation for next-generation unified multimodal models. Furthermore, we
show that applying test-time scaling techniques to FUDOKI yields significant
performance gains, further underscoring its promise for future enhancement
through reinforcement learning.
</summary>
    <author>
      <name>Jin Wang</name>
    </author>
    <author>
      <name>Yao Lai</name>
    </author>
    <author>
      <name>Aoxue Li</name>
    </author>
    <author>
      <name>Shifeng Zhang</name>
    </author>
    <author>
      <name>Jiacheng Sun</name>
    </author>
    <author>
      <name>Ning Kang</name>
    </author>
    <author>
      <name>Chengyue Wu</name>
    </author>
    <author>
      <name>Zhenguo Li</name>
    </author>
    <author>
      <name>Ping Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.20147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.20147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.20144v1</id>
    <updated>2025-05-26T15:45:56Z</updated>
    <published>2025-05-26T15:45:56Z</published>
    <title>SeMe: Training-Free Language Model Merging via Semantic Alignment</title>
    <summary>  Despite the remarkable capabilities of Language Models (LMs) across diverse
tasks, no single model consistently outperforms others, necessitating efficient
methods to combine their strengths without expensive retraining. Existing model
merging techniques, such as parameter averaging and task-guided fusion, often
rely on data-dependent computations or fail to preserve internal knowledge,
limiting their robustness and scalability. We introduce SeMe (Semantic-based
Merging), a novel, data-free, and training-free approach that leverages latent
semantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike
prior work, SeMe not only preserves model behaviors but also explicitly
stabilizes internal knowledge, addressing a critical gap in LM fusion. Through
extensive experiments across diverse architectures and tasks, we demonstrate
that SeMe outperforms existing methods in both performance and efficiency while
eliminating reliance on external data. Our work establishes a new paradigm for
knowledge-aware model merging and provides insights into the semantic structure
of LMs, paving the way for more scalable and interpretable model composition.
</summary>
    <author>
      <name>Jian Gu</name>
    </author>
    <author>
      <name>Aldeida Aleti</name>
    </author>
    <author>
      <name>Chunyang Chen</name>
    </author>
    <author>
      <name>Hongyu Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">an early-stage version</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.20144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.20144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
