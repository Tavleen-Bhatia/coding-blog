<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-04-01T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">415844</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.24388v1</id>
    <updated>2025-03-31T17:59:52Z</updated>
    <published>2025-03-31T17:59:52Z</published>
    <title>RIG: Synergizing Reasoning and Imagination in End-to-End Generalist
  Policy</title>
    <summary>  Reasoning before action and imagining potential outcomes (i.e., world models)
are essential for embodied agents operating in complex open-world environments.
Yet, prior work either incorporates only one of these abilities in an
end-to-end agent or integrates multiple specialized models into an agent
system, limiting the learning efficiency and generalization of the policy.
Thus, this paper makes the first attempt to synergize Reasoning and Imagination
in an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end
manner, we construct a data pipeline that progressively integrates and enriches
the content of imagination and reasoning in the trajectories collected from
existing agents. The joint learning of reasoning and next image generation
explicitly models the inherent correlation between reasoning, action, and
dynamics of environments, and thus exhibits more than $17\times$ sample
efficiency improvements and generalization in comparison with previous works.
During inference, RIG first reasons about the next action, produces potential
action, and then predicts the action outcomes, which offers the agent a chance
to review and self-correct based on the imagination before taking real actions.
Experimental results show that the synergy of reasoning and imagination not
only improves the robustness, generalization, and interoperability of
generalist policy but also enables test-time scaling to enhance overall
performance.
</summary>
    <author>
      <name>Zhonghan Zhao</name>
    </author>
    <author>
      <name>Wenwei Zhang</name>
    </author>
    <author>
      <name>Haian Huang</name>
    </author>
    <author>
      <name>Kuikun Liu</name>
    </author>
    <author>
      <name>Jianfei Gao</name>
    </author>
    <author>
      <name>Gaoang Wang</name>
    </author>
    <author>
      <name>Kai Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2503.24388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.24388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.24387v1</id>
    <updated>2025-03-31T17:59:51Z</updated>
    <published>2025-03-31T17:59:51Z</published>
    <title>Consistent Subject Generation via Contrastive Instantiated Concepts</title>
    <summary>  While text-to-image generative models can synthesize diverse and faithful
contents, subject variation across multiple creations limits the application in
long content generation. Existing approaches require time-consuming tuning,
references for all subjects, or access to other creations. We introduce
Contrastive Concept Instantiation (CoCoIns) to effectively synthesize
consistent subjects across multiple independent creations. The framework
consists of a generative model and a mapping network, which transforms input
latent codes into pseudo-words associated with certain instances of concepts.
Users can generate consistent subjects with the same latent codes. To construct
such associations, we propose a contrastive learning approach that trains the
network to differentiate the combination of prompts and latent codes. Extensive
evaluations of human faces with a single subject show that CoCoIns performs
comparably to existing methods while maintaining higher flexibility. We also
demonstrate the potential of extending CoCoIns to multiple subjects and other
object categories.
</summary>
    <author>
      <name>Lee Hsin-Ying</name>
    </author>
    <author>
      <name>Kelvin C. K. Chan</name>
    </author>
    <author>
      <name>Ming-Hsuan Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://contrastive-concept-instantiation.github.io</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.24387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.24387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.24381v1</id>
    <updated>2025-03-31T17:59:24Z</updated>
    <published>2025-03-31T17:59:24Z</published>
    <title>UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in
  Autonomous Driving</title>
    <summary>  We introduce UniOcc, a comprehensive, unified benchmark for occupancy
forecasting (i.e., predicting future occupancies based on historical
information) and current-frame occupancy prediction from camera images. UniOcc
unifies data from multiple real-world datasets (i.e., nuScenes, Waymo) and
high-fidelity driving simulators (i.e., CARLA, OpenCOOD), which provides 2D/3D
occupancy labels with per-voxel flow annotations and support for cooperative
autonomous driving. In terms of evaluation, unlike existing studies that rely
on suboptimal pseudo labels for evaluation, UniOcc incorporates novel metrics
that do not depend on ground-truth occupancy, enabling robust assessment of
additional aspects of occupancy quality. Through extensive experiments on
state-of-the-art models, we demonstrate that large-scale, diverse training data
and explicit flow information significantly enhance occupancy prediction and
forecasting performance.
</summary>
    <author>
      <name>Yuping Wang</name>
    </author>
    <author>
      <name>Xiangyu Huang</name>
    </author>
    <author>
      <name>Xiaokang Sun</name>
    </author>
    <author>
      <name>Mingxuan Yan</name>
    </author>
    <author>
      <name>Shuo Xing</name>
    </author>
    <author>
      <name>Zhengzhong Tu</name>
    </author>
    <author>
      <name>Jiachen Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages; Dataset: https://huggingface.co/datasets/tasl-lab/uniocc;
  Code: https://github.com/tasl-lab/UniOcc</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.24381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.24381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.24377v1</id>
    <updated>2025-03-31T17:58:07Z</updated>
    <published>2025-03-31T17:58:07Z</published>
    <title>Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for
  Large Language Models</title>
    <summary>  Recent advancements in Large Language Models (LLMs) have significantly
enhanced their ability to perform complex reasoning tasks, transitioning from
fast and intuitive thinking (System 1) to slow and deep reasoning (System 2).
While System 2 reasoning improves task accuracy, it often incurs substantial
computational costs due to its slow thinking nature and inefficient or
unnecessary reasoning behaviors. In contrast, System 1 reasoning is
computationally efficient but leads to suboptimal performance. Consequently, it
is critical to balance the trade-off between performance (benefits) and
computational costs (budgets), giving rise to the concept of reasoning economy.
In this survey, we provide a comprehensive analysis of reasoning economy in
both the post-training and test-time inference stages of LLMs, encompassing i)
the cause of reasoning inefficiency, ii) behavior analysis of different
reasoning patterns, and iii) potential solutions to achieve reasoning economy.
By offering actionable insights and highlighting open challenges, we aim to
shed light on strategies for improving the reasoning economy of LLMs, thereby
serving as a valuable resource for advancing research in this evolving area. We
also provide a public repository to continually track developments in this
fast-evolving field.
</summary>
    <author>
      <name>Rui Wang</name>
    </author>
    <author>
      <name>Hongru Wang</name>
    </author>
    <author>
      <name>Boyang Xue</name>
    </author>
    <author>
      <name>Jianhui Pang</name>
    </author>
    <author>
      <name>Shudong Liu</name>
    </author>
    <author>
      <name>Yi Chen</name>
    </author>
    <author>
      <name>Jiahao Qiu</name>
    </author>
    <author>
      <name>Derek Fai Wong</name>
    </author>
    <author>
      <name>Heng Ji</name>
    </author>
    <author>
      <name>Kam-Fai Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Progress; Paper list Repo:
  https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.24377v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.24377v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.24376v1</id>
    <updated>2025-03-31T17:55:23Z</updated>
    <published>2025-03-31T17:55:23Z</published>
    <title>Exploring the Effect of Reinforcement Learning on Video Understanding:
  Insights from SEED-Bench-R1</title>
    <summary>  Recent advancements in Chain of Thought (COT) generation have significantly
improved the reasoning capabilities of Large Language Models (LLMs), with
reinforcement learning (RL) emerging as an effective post-training approach.
Multimodal Large Language Models (MLLMs) inherit this reasoning potential but
remain underexplored in tasks requiring both perception and logical reasoning.
To address this, we introduce SEED-Bench-R1, a benchmark designed to
systematically evaluate post-training methods for MLLMs in video understanding.
It includes intricate real-world videos and complex everyday planning tasks in
the format of multiple-choice questions, requiring sophisticated perception and
reasoning. SEED-Bench-R1 assesses generalization through a three-level
hierarchy: in-distribution, cross-environment, and cross-environment-task
scenarios, equipped with a large-scale training dataset with easily verifiable
ground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RL
with supervised fine-tuning (SFT), demonstrating RL's data efficiency and
superior performance on both in-distribution and out-of-distribution tasks,
even outperforming SFT on general video understanding benchmarks like
LongVideoBench. Our detailed analysis reveals that RL enhances visual
perception but often produces less logically coherent reasoning chains. We
identify key limitations such as inconsistent reasoning and overlooked visual
cues, and suggest future improvements in base model reasoning, reward modeling,
and RL robustness against noisy signals.
</summary>
    <author>
      <name>Yi Chen</name>
    </author>
    <author>
      <name>Yuying Ge</name>
    </author>
    <author>
      <name>Rui Wang</name>
    </author>
    <author>
      <name>Yixiao Ge</name>
    </author>
    <author>
      <name>Lu Qiu</name>
    </author>
    <author>
      <name>Ying Shan</name>
    </author>
    <author>
      <name>Xihui Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report (In Progress); Code released at:
  https://github.com/TencentARC/SEED-Bench-R1</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.24376v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.24376v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
