<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-04-17T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">419302</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.12299v1</id>
    <updated>2025-04-16T17:59:54Z</updated>
    <published>2025-04-16T17:59:54Z</published>
    <title>Adapting a World Model for Trajectory Following in a 3D Game</title>
    <summary>  Imitation learning is a powerful tool for training agents by leveraging
expert knowledge, and being able to replicate a given trajectory is an integral
part of it. In complex environments, like modern 3D video games, distribution
shift and stochasticity necessitate robust approaches beyond simple action
replay. In this study, we apply Inverse Dynamics Models (IDM) with different
encoders and policy heads to trajectory following in a modern 3D video game --
Bleeding Edge. Additionally, we investigate several future alignment strategies
that address the distribution shift caused by the aleatoric uncertainty and
imperfections of the agent. We measure both the trajectory deviation distance
and the first significant deviation point between the reference and the agent's
trajectory and show that the optimal configuration depends on the chosen
setting. Our results show that in a diverse data setting, a GPT-style policy
head with an encoder trained from scratch performs the best, DINOv2 encoder
with the GPT-style policy head gives the best results in the low data regime,
and both GPT-style and MLP-style policy heads had comparable results when
pre-trained on a diverse setting and fine-tuned for a specific behaviour
setting.
</summary>
    <author>
      <name>Marko Tot</name>
    </author>
    <author>
      <name>Shu Ishida</name>
    </author>
    <author>
      <name>Abdelhak Lemkhenter</name>
    </author>
    <author>
      <name>David Bignell</name>
    </author>
    <author>
      <name>Pallavi Choudhury</name>
    </author>
    <author>
      <name>Chris Lovett</name>
    </author>
    <author>
      <name>Luis França</name>
    </author>
    <author>
      <name>Matheus Ribeiro Furtado de Mendonça</name>
    </author>
    <author>
      <name>Tarun Gupta</name>
    </author>
    <author>
      <name>Darren Gehring</name>
    </author>
    <author>
      <name>Sam Devlin</name>
    </author>
    <author>
      <name>Sergio Valcarcel Macua</name>
    </author>
    <author>
      <name>Raluca Georgescu</name>
    </author>
    <link href="http://arxiv.org/abs/2504.12299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.12299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.12292v1</id>
    <updated>2025-04-16T17:55:02Z</updated>
    <published>2025-04-16T17:55:02Z</published>
    <title>SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians</title>
    <summary>  Accurate, real-time 3D reconstruction of human heads from monocular images
and videos underlies numerous visual applications. As 3D ground truth data is
hard to come by at scale, previous methods have sought to learn from abundant
2D videos in a self-supervised manner. Typically, this involves the use of
differentiable mesh rendering, which is effective but faces limitations. To
improve on this, we propose SHeaP (Self-supervised Head Geometry Predictor
Learned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a
set of Gaussians that are rigged to this mesh. We then reanimate this rigged
head avatar to match a target frame, and backpropagate photometric losses to
both the 3DMM and Gaussian prediction networks. We find that using Gaussians
for rendering substantially improves the effectiveness of this self-supervised
approach. Training solely on 2D data, our method surpasses existing
self-supervised approaches in geometric evaluations on the NoW benchmark for
neutral faces and a new benchmark for non-neutral expressions. Our method also
produces highly expressive meshes, outperforming state-of-the-art in emotion
classification.
</summary>
    <author>
      <name>Liam Schoneveld</name>
    </author>
    <author>
      <name>Zhe Chen</name>
    </author>
    <author>
      <name>Davide Davoli</name>
    </author>
    <author>
      <name>Jiapeng Tang</name>
    </author>
    <author>
      <name>Saimon Terazawa</name>
    </author>
    <author>
      <name>Ko Nishino</name>
    </author>
    <author>
      <name>Matthias Nießner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">For video demonstrations and additional materials please see
  https://nlml.github.io/sheap/</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.12292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.12292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.12287v1</id>
    <updated>2025-04-16T17:51:59Z</updated>
    <published>2025-04-16T17:51:59Z</published>
    <title>Trend Filtered Mixture of Experts for Automated Gating of High-Frequency
  Flow Cytometry Data</title>
    <summary>  Ocean microbes are critical to both ocean ecosystems and the global climate.
Flow cytometry, which measures cell optical properties in fluid samples, is
routinely used in oceanographic research. Despite decades of accumulated data,
identifying key microbial populations (a process known as ``gating'') remains a
significant analytical challenge. To address this, we focus on gating
multidimensional, high-frequency flow cytometry data collected {\it
continuously} on board oceanographic research vessels, capturing time- and
space-wise variations in the dynamic ocean. Our paper proposes a novel
mixture-of-experts model in which both the gating function and the experts are
given by trend filtering. The model leverages two key assumptions: (1) Each
snapshot of flow cytometry data is a mixture of multivariate Gaussians and (2)
the parameters of these Gaussians vary smoothly over time. Our method uses
regularization and a constraint to ensure smoothness and that cluster means
match biologically distinct microbe types. We demonstrate, using flow cytometry
data from the North Pacific Ocean, that our proposed model accurately matches
human-annotated gating and corrects significant errors.
</summary>
    <author>
      <name>Sangwon Hyun</name>
    </author>
    <author>
      <name>Tim Coleman</name>
    </author>
    <author>
      <name>Francois Ribalet</name>
    </author>
    <author>
      <name>Jacob Bien</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 page (including supplement), 9 figures (including supplement)</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.12287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.12287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H30 (Primary) 62G08, 92B10, 62J07 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.12285v1</id>
    <updated>2025-04-16T17:51:43Z</updated>
    <published>2025-04-16T17:51:43Z</published>
    <title>BitNet b1.58 2B4T Technical Report</title>
    <summary>  We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large
Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4
trillion tokens, the model has been rigorously evaluated across benchmarks
covering language understanding, mathematical reasoning, coding proficiency,
and conversational ability. Our results demonstrate that BitNet b1.58 2B4T
achieves performance on par with leading open-weight, full-precision LLMs of
similar size, while offering significant advantages in computational
efficiency, including substantially reduced memory footprint, energy
consumption, and decoding latency. To facilitate further research and adoption,
the model weights are released via Hugging Face along with open-source
inference implementations for both GPU and CPU architectures.
</summary>
    <author>
      <name>Shuming Ma</name>
    </author>
    <author>
      <name>Hongyu Wang</name>
    </author>
    <author>
      <name>Shaohan Huang</name>
    </author>
    <author>
      <name>Xingxing Zhang</name>
    </author>
    <author>
      <name>Ying Hu</name>
    </author>
    <author>
      <name>Ting Song</name>
    </author>
    <author>
      <name>Yan Xia</name>
    </author>
    <author>
      <name>Furu Wei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.12285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.12285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.12284v1</id>
    <updated>2025-04-16T17:48:12Z</updated>
    <published>2025-04-16T17:48:12Z</published>
    <title>How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday
  Interactions</title>
    <summary>  We tackle the novel problem of predicting 3D hand motion and contact maps (or
Interaction Trajectories) given a single RGB view, action text, and a 3D
contact point on the object as input. Our approach consists of (1) Interaction
Codebook: a VQVAE model to learn a latent codebook of hand poses and contact
points, effectively tokenizing interaction trajectories, (2) Interaction
Predictor: a transformer-decoder module to predict the interaction trajectory
from test time inputs by using an indexer module to retrieve a latent
affordance from the learned codebook. To train our model, we develop a data
engine that extracts 3D hand poses and contact trajectories from the diverse
HoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger
than existing works, in terms of diversity of objects and interactions
observed, and test for generalization of the model across object categories,
action categories, tasks, and scenes. Experimental results show the
effectiveness of our approach over transformer &amp; diffusion baselines across all
settings.
</summary>
    <author>
      <name>Aditya Prakash</name>
    </author>
    <author>
      <name>Benjamin Lundell</name>
    </author>
    <author>
      <name>Dmitry Andreychuk</name>
    </author>
    <author>
      <name>David Forsyth</name>
    </author>
    <author>
      <name>Saurabh Gupta</name>
    </author>
    <author>
      <name>Harpreet Sawhney</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2025, Project page:
  https://ap229997.github.io/projects/latentact</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.12284v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.12284v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
