<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-04-10T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">417865</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.07097v1</id>
    <updated>2025-04-09T17:59:42Z</updated>
    <published>2025-04-09T17:59:42Z</published>
    <title>Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual
  Learning</title>
    <summary>  Continual learning in large language models (LLMs) is prone to catastrophic
forgetting, where adapting to new tasks significantly degrades performance on
previously learned ones. Existing methods typically rely on low-rank,
parameter-efficient updates that limit the model's expressivity and introduce
additional parameters per task, leading to scalability issues. To address these
limitations, we propose a novel continual full fine-tuning approach leveraging
adaptive singular value decomposition (SVD). Our method dynamically identifies
task-specific low-rank parameter subspaces and constrains updates to be
orthogonal to critical directions associated with prior tasks, thus effectively
minimizing interference without additional parameter overhead or storing
previous task gradients. We evaluate our approach extensively on standard
continual learning benchmarks using both encoder-decoder (T5-Large) and
decoder-only (LLaMA-2 7B) models, spanning diverse tasks including
classification, generation, and reasoning. Empirically, our method achieves
state-of-the-art results, up to 7% higher average accuracy than recent
baselines like O-LoRA, and notably maintains the model's general linguistic
capabilities, instruction-following accuracy, and safety throughout the
continual learning process by reducing forgetting to near-negligible levels.
Our adaptive SVD framework effectively balances model plasticity and knowledge
retention, providing a practical, theoretically grounded, and computationally
scalable solution for continual learning scenarios in large language models.
</summary>
    <author>
      <name>Nikhil Shivakumar Nayak</name>
    </author>
    <author>
      <name>Krishnateja Killamsetty</name>
    </author>
    <author>
      <name>Ligong Han</name>
    </author>
    <author>
      <name>Abhishek Bhandwaldar</name>
    </author>
    <author>
      <name>Prateek Chanda</name>
    </author>
    <author>
      <name>Kai Xu</name>
    </author>
    <author>
      <name>Hao Wang</name>
    </author>
    <author>
      <name>Aldo Pareja</name>
    </author>
    <author>
      <name>Oleg Silkin</name>
    </author>
    <author>
      <name>Mustafa Eyceoz</name>
    </author>
    <author>
      <name>Akash Srivastava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 13 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.07097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.07095v1</id>
    <updated>2025-04-09T17:59:32Z</updated>
    <published>2025-04-09T17:59:32Z</published>
    <title>Neural Motion Simulator: Pushing the Limit of World Models in
  Reinforcement Learning</title>
    <summary>  An embodied system must not only model the patterns of the external world but
also understand its own motion dynamics. A motion dynamic model is essential
for efficient skill acquisition and effective planning. In this work, we
introduce the neural motion simulator (MoSim), a world model that predicts the
future physical state of an embodied system based on current observations and
actions. MoSim achieves state-of-the-art performance in physical state
prediction and provides competitive performance across a range of downstream
tasks. This works shows that when a world model is accurate enough and performs
precise long-horizon predictions, it can facilitate efficient skill acquisition
in imagined worlds and even enable zero-shot reinforcement learning.
Furthermore, MoSim can transform any model-free reinforcement learning (RL)
algorithm into a model-based approach, effectively decoupling physical
environment modeling from RL algorithm development. This separation allows for
independent advancements in RL algorithms and world modeling, significantly
improving sample efficiency and enhancing generalization capabilities. Our
findings highlight that world models for motion dynamics is a promising
direction for developing more versatile and capable embodied systems.
</summary>
    <author>
      <name>Chenjie Hao</name>
    </author>
    <author>
      <name>Weyl Lu</name>
    </author>
    <author>
      <name>Yifan Xu</name>
    </author>
    <author>
      <name>Yubei Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages (main), 2-page appendix, 8 figures, accepted by CVPR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.07095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.07092v1</id>
    <updated>2025-04-09T17:59:05Z</updated>
    <published>2025-04-09T17:59:05Z</published>
    <title>Are We Done with Object-Centric Learning?</title>
    <summary>  Object-centric learning (OCL) seeks to learn representations that only encode
an object, isolated from other objects or background cues in a scene. This
approach underpins various aims, including out-of-distribution (OOD)
generalization, sample-efficient composition, and modeling of structured
environments. Most research has focused on developing unsupervised mechanisms
that separate objects into discrete slots in the representation space,
evaluated using unsupervised object discovery. However, with recent
sample-efficient segmentation models, we can separate objects in the pixel
space and encode them independently. This achieves remarkable zero-shot
performance on OOD object discovery benchmarks, is scalable to foundation
models, and can handle a variable number of slots out-of-the-box. Hence, the
goal of OCL methods to obtain object-centric representations has been largely
achieved. Despite this progress, a key question remains: How does the ability
to separate objects within a scene contribute to broader OCL objectives, such
as OOD generalization? We address this by investigating the OOD generalization
challenge caused by spurious background cues through the lens of OCL. We
propose a novel, training-free probe called $\textbf{Object-Centric
Classification with Applied Masks (OCCAM)}$, demonstrating that
segmentation-based encoding of individual objects significantly outperforms
slot-based OCL methods. However, challenges in real-world applications remain.
We provide the toolbox for the OCL community to use scalable object-centric
representations, and focus on practical applications and fundamental questions,
such as understanding object perception in human cognition. Our code is
available $\href{https://github.com/AlexanderRubinstein/OCCAM}{here}$.
</summary>
    <author>
      <name>Alexander Rubinstein</name>
    </author>
    <author>
      <name>Ameya Prabhu</name>
    </author>
    <author>
      <name>Matthias Bethge</name>
    </author>
    <author>
      <name>Seong Joon Oh</name>
    </author>
    <link href="http://arxiv.org/abs/2504.07092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.07090v1</id>
    <updated>2025-04-09T17:59:03Z</updated>
    <published>2025-04-09T17:59:03Z</published>
    <title>A Differentiable, End-to-End Forward Model for 21 cm Cosmology:
  Estimating the Foreground, Instrument, and Signal Joint Posterior</title>
    <summary>  We present a differentiable, end-to-end Bayesian forward modeling framework
for line intensity mapping cosmology experiments, with a specific focus on
low-frequency radio telescopes targeting the redshifted 21 cm line from neutral
hydrogen as a cosmological probe. Our framework is capable of posterior density
estimation of the cosmological signal jointly with foreground and telescope
parameters at the field level. Our key aim is to be able to optimize the
model's high-dimensional, non-linear, and ill-conditioned parameter space,
while also sampling from it to perform robust uncertainty quantification within
a Bayesian framework. We show how a differentiable programming paradigm,
accelerated by recent advances in machine learning software and hardware, can
make this computationally-demanding, end-to-end Bayesian approach feasible. We
demonstrate a proof-of-concept on a simplified signal recovery problem for the
Hydrogen Epoch of Reionization Array experiment, highlighting the framework's
ability to build confidence in early 21 cm signal detections even in the
presence of poorly understood foregrounds and instrumental systematics. We use
a Hessian-preconditioned Hamiltonian Monte Carlo algorithm to efficiently
sample our parameter space with a dimensionality approaching $N\sim10^5$, which
enables joint, end-to-end nuisance parameter marginalization over foreground
and instrumental terms. Lastly, we introduce a new spherical harmonic formalism
that is a complete and orthogonal basis on the cut sky relevant to drift-scan
radio surveys, which we call the spherical stripe harmonic formalism, and it's
associated three-dimensional basis, the spherical stripe Fourier-Bessel
formalism.
</summary>
    <author>
      <name>Nicholas Kern</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 15 figures; Submitted to MNRAS</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.07090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.IM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.07091v1</id>
    <updated>2025-04-09T17:59:03Z</updated>
    <published>2025-04-09T17:59:03Z</published>
    <title>AssistanceZero: Scalably Solving Assistance Games</title>
    <summary>  Assistance games are a promising alternative to reinforcement learning from
human feedback (RLHF) for training AI assistants. Assistance games resolve key
drawbacks of RLHF, such as incentives for deceptive behavior, by explicitly
modeling the interaction between assistant and user as a two-player game where
the assistant cannot observe their shared goal. Despite their potential,
assistance games have only been explored in simple settings. Scaling them to
more complex environments is difficult because it requires both solving
intractable decision-making problems under uncertainty and accurately modeling
human users' behavior. We present the first scalable approach to solving
assistance games and apply it to a new, challenging Minecraft-based assistance
game with over $10^{400}$ possible goals. Our approach, AssistanceZero, extends
AlphaZero with a neural network that predicts human actions and rewards,
enabling it to plan under uncertainty. We show that AssistanceZero outperforms
model-free RL algorithms and imitation learning in the Minecraft-based
assistance game. In a human study, our AssistanceZero-trained assistant
significantly reduces the number of actions participants take to complete
building tasks in Minecraft. Our results suggest that assistance games are a
tractable framework for training effective AI assistants in complex
environments. Our code and models are available at
https://github.com/cassidylaidlaw/minecraft-building-assistance-game.
</summary>
    <author>
      <name>Cassidy Laidlaw</name>
    </author>
    <author>
      <name>Eli Bronstein</name>
    </author>
    <author>
      <name>Timothy Guo</name>
    </author>
    <author>
      <name>Dylan Feng</name>
    </author>
    <author>
      <name>Lukas Berglund</name>
    </author>
    <author>
      <name>Justin Svegliato</name>
    </author>
    <author>
      <name>Stuart Russell</name>
    </author>
    <author>
      <name>Anca Dragan</name>
    </author>
    <link href="http://arxiv.org/abs/2504.07091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
