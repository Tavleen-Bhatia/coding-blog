<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-06-17T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">435093</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2506.13763v1</id>
    <updated>2025-06-16T17:59:54Z</updated>
    <published>2025-06-16T17:59:54Z</published>
    <title>Diagnosing and Improving Diffusion Models by Estimating the Optimal Loss
  Value</title>
    <summary>  Diffusion models have achieved remarkable success in generative modeling.
Despite more stable training, the loss of diffusion models is not indicative of
absolute data-fitting quality, since its optimal value is typically not zero
but unknown, leading to confusion between large optimal loss and insufficient
model capacity. In this work, we advocate the need to estimate the optimal loss
value for diagnosing and improving diffusion models. We first derive the
optimal loss in closed form under a unified formulation of diffusion models,
and develop effective estimators for it, including a stochastic variant
scalable to large datasets with proper control of variance and bias. With this
tool, we unlock the inherent metric for diagnosing the training quality of
mainstream diffusion model variants, and develop a more performant training
schedule based on the optimal loss. Moreover, using models with 120M to 1.5B
parameters, we find that the power law is better demonstrated after subtracting
the optimal loss from the actual training loss, suggesting a more principled
setting for investigating the scaling law for diffusion models.
</summary>
    <author>
      <name>Yixian Xu</name>
    </author>
    <author>
      <name>Shengjie Luo</name>
    </author>
    <author>
      <name>Liwei Wang</name>
    </author>
    <author>
      <name>Di He</name>
    </author>
    <author>
      <name>Chang Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 8 figures, 3 tables. Preprint. Work in Progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.13763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.13763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.13762v1</id>
    <updated>2025-06-16T17:59:48Z</updated>
    <published>2025-06-16T17:59:48Z</published>
    <title>Touch begins where vision ends: Generalizable policies for contact-rich
  manipulation</title>
    <summary>  Data-driven approaches struggle with precise manipulation; imitation learning
requires many hard-to-obtain demonstrations, while reinforcement learning
yields brittle, non-generalizable policies. We introduce VisuoTactile Local
(ViTaL) policy learning, a framework that solves fine-grained manipulation
tasks by decomposing them into two phases: a reaching phase, where a
vision-language model (VLM) enables scene-level reasoning to localize the
object of interest, and a local interaction phase, where a reusable,
scene-agnostic ViTaL policy performs contact-rich manipulation using egocentric
vision and tactile sensing. This approach is motivated by the observation that
while scene context varies, the low-level interaction remains consistent across
task instances. By training local policies once in a canonical setting, they
can generalize via a localize-then-execute strategy. ViTaL achieves around 90%
success on contact-rich tasks in unseen environments and is robust to
distractors. ViTaL's effectiveness stems from three key insights: (1)
foundation models for segmentation enable training robust visual encoders via
behavior cloning; (2) these encoders improve the generalizability of policies
learned using residual RL; and (3) tactile sensing significantly boosts
performance in contact-rich tasks. Ablation studies validate each of these
insights, and we demonstrate that ViTaL integrates well with high-level VLMs,
enabling robust, reusable low-level skills. Results and videos are available at
https://vitalprecise.github.io.
</summary>
    <author>
      <name>Zifan Zhao</name>
    </author>
    <author>
      <name>Siddhant Haldar</name>
    </author>
    <author>
      <name>Jinda Cui</name>
    </author>
    <author>
      <name>Lerrel Pinto</name>
    </author>
    <author>
      <name>Raunaq Bhirangi</name>
    </author>
    <link href="http://arxiv.org/abs/2506.13762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.13762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.13759v1</id>
    <updated>2025-06-16T17:59:08Z</updated>
    <published>2025-06-16T17:59:08Z</published>
    <title>Discrete Diffusion in Large Language and Multimodal Models: A Survey</title>
    <summary>  In this work, we provide a systematic survey of Discrete Diffusion Language
Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).
Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,
parallel decoding paradigm using full attention and a denoising-based
generation strategy. This paradigm naturally enables parallel generation,
fine-grained output controllability, and dynamic, response-aware perception.
These capabilities are previously difficult to achieve with AR models.
Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as
a large number of open-source academic d(M)LLMs, have demonstrated performance
comparable to their autoregressive counterparts, while achieving up to 10x
acceleration in inference speed.
  The advancement of discrete diffusion LLMs and MLLMs has been largely driven
by progress in two domains. The first is the development of autoregressive LLMs
and MLLMs, which has accumulated vast amounts of data, benchmarks, and
foundational infrastructure for training and inference. The second contributing
domain is the evolution of the mathematical models underlying discrete
diffusion. Together, these advancements have catalyzed a surge in dLLMs and
dMLLMs research in early 2025.
  In this work, we present a comprehensive overview of the research in the dLLM
and dMLLM domains. We trace the historical development of dLLMs and dMLLMs,
formalize the underlying mathematical frameworks, and categorize representative
models. We further analyze key techniques for training and inference, and
summarize emerging applications across language, vision-language, and
biological domains. We conclude by discussing future directions for research
and deployment.
  Paper collection: https://github.com/LiQiiiii/DLLM-Survey
</summary>
    <author>
      <name>Runpeng Yu</name>
    </author>
    <author>
      <name>Qi Li</name>
    </author>
    <author>
      <name>Xinchao Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2506.13759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.13759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.13758v1</id>
    <updated>2025-06-16T17:59:02Z</updated>
    <published>2025-06-16T17:59:02Z</published>
    <title>AI reconstruction of European weather from the Euro-Atlantic regimes</title>
    <summary>  We present a non-linear AI-model designed to reconstruct monthly mean
anomalies of the European temperature and precipitation based on the
Euro-Atlantic Weather regimes (WR) indices. WR represent recurrent,
quasi-stationary, and persistent states of the atmospheric circulation that
exert considerable influence over the European weather, therefore offering an
opportunity for sub-seasonal to seasonal forecasting. While much research has
focused on studying the correlation and impacts of the WR on European weather,
the estimation of ground-level climate variables, such as temperature and
precipitation, from Euro-Atlantic WR remains largely unexplored and is
currently limited to linear methods. The presented AI model can capture and
introduce complex non-linearities in the relation between the WR indices,
describing the state of the Euro-Atlantic atmospheric circulation and the
corresponding surface temperature and precipitation anomalies in Europe. We
discuss the AI-model performance in reconstructing the monthly mean two-meter
temperature and total precipitation anomalies in the European winter and
summer, also varying the number of WR used to describe the monthly atmospheric
circulation. We assess the impact of errors on the WR indices in the
reconstruction and show that a mean absolute relative error below 80% yields
improved seasonal reconstruction compared to the ECMWF operational seasonal
forecast system, SEAS5. As a demonstration of practical applicability, we
evaluate the model using WR indices predicted by SEAS5, finding slightly better
or comparable skill relative to the SEAS5 forecast itself. Our findings
demonstrate that WR-based anomaly reconstruction, powered by AI tools, offers a
promising pathway for sub-seasonal and seasonal forecasting.
</summary>
    <author>
      <name>A. Camilletti</name>
    </author>
    <author>
      <name>G. Franch</name>
    </author>
    <author>
      <name>E. Tomasi</name>
    </author>
    <author>
      <name>M. Cristoforetti</name>
    </author>
    <link href="http://arxiv.org/abs/2506.13758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.13758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2506.13756v1</id>
    <updated>2025-06-16T17:58:29Z</updated>
    <published>2025-06-16T17:58:29Z</published>
    <title>UltraZoom: Generating Gigapixel Images from Regular Photos</title>
    <summary>  We present UltraZoom, a system for generating gigapixel-resolution images of
objects from casually captured inputs, such as handheld phone photos. Given a
full-shot image (global, low-detail) and one or more close-ups (local,
high-detail), UltraZoom upscales the full image to match the fine detail and
scale of the close-up examples. To achieve this, we construct a per-instance
paired dataset from the close-ups and adapt a pretrained generative model to
learn object-specific low-to-high resolution mappings. At inference, we apply
the model in a sliding window fashion over the full image. Constructing these
pairs is non-trivial: it requires registering the close-ups within the full
image for scale estimation and degradation alignment. We introduce a simple,
robust method for getting registration on arbitrary materials in casual,
in-the-wild captures. Together, these components form a system that enables
seamless pan and zoom across the entire object, producing consistent,
photorealistic gigapixel imagery from minimal input.
</summary>
    <author>
      <name>Jingwei Ma</name>
    </author>
    <author>
      <name>Vivek Jayaram</name>
    </author>
    <author>
      <name>Brian Curless</name>
    </author>
    <author>
      <name>Ira Kemelmacher-Shlizerman</name>
    </author>
    <author>
      <name>Steven M. Seitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://ultra-zoom.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2506.13756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2506.13756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
