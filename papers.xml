<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-04-08T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">417379</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.05304v1</id>
    <updated>2025-04-07T17:59:42Z</updated>
    <published>2025-04-07T17:59:42Z</published>
    <title>Gaussian Mixture Flow Matching Models</title>
    <summary>  Diffusion models approximate the denoising distribution as a Gaussian and
predict its mean, whereas flow matching models reparameterize the Gaussian mean
as flow velocity. However, they underperform in few-step sampling due to
discretization error and tend to produce over-saturated colors under
classifier-free guidance (CFG). To address these limitations, we propose a
novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the
mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a
multi-modal flow velocity distribution, which can be learned with a KL
divergence loss. We demonstrate that GMFlow generalizes previous diffusion and
flow matching models where a single Gaussian is learned with an $L_2$ denoising
loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic
denoising distributions and velocity fields for precise few-step sampling.
Furthermore, we introduce a novel probabilistic guidance scheme that mitigates
the over-saturation issues of CFG and improves image generation quality.
Extensive experiments demonstrate that GMFlow consistently outperforms flow
matching baselines in generation quality, achieving a Precision of 0.942 with
only 6 sampling steps on ImageNet 256$\times$256.
</summary>
    <author>
      <name>Hansheng Chen</name>
    </author>
    <author>
      <name>Kai Zhang</name>
    </author>
    <author>
      <name>Hao Tan</name>
    </author>
    <author>
      <name>Zexiang Xu</name>
    </author>
    <author>
      <name>Fujun Luan</name>
    </author>
    <author>
      <name>Leonidas Guibas</name>
    </author>
    <author>
      <name>Gordon Wetzstein</name>
    </author>
    <author>
      <name>Sai Bi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code: https://github.com/Lakonik/GMFlow</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.05304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.05304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.05300v1</id>
    <updated>2025-04-07T17:59:07Z</updated>
    <published>2025-04-07T17:59:07Z</published>
    <title>Dimension-Free Convergence of Diffusion Models for Approximate Gaussian
  Mixtures</title>
    <summary>  Diffusion models are distinguished by their exceptional generative
performance, particularly in producing high-quality samples through iterative
denoising. While current theory suggests that the number of denoising steps
required for accurate sample generation should scale linearly with data
dimension, this does not reflect the practical efficiency of widely used
algorithms like Denoising Diffusion Probabilistic Models (DDPMs). This paper
investigates the effectiveness of diffusion models in sampling from complex
high-dimensional distributions that can be well-approximated by Gaussian
Mixture Models (GMMs). For these distributions, our main result shows that DDPM
takes at most $\widetilde{O}(1/\varepsilon)$ iterations to attain an
$\varepsilon$-accurate distribution in total variation (TV) distance,
independent of both the ambient dimension $d$ and the number of components $K$,
up to logarithmic factors. Furthermore, this result remains robust to score
estimation errors. These findings highlight the remarkable effectiveness of
diffusion models in high-dimensional settings given the universal approximation
capability of GMMs, and provide theoretical insights into their practical
success.
</summary>
    <author>
      <name>Gen Li</name>
    </author>
    <author>
      <name>Changxiao Cai</name>
    </author>
    <author>
      <name>Yuting Wei</name>
    </author>
    <link href="http://arxiv.org/abs/2504.05300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.05300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.05295v1</id>
    <updated>2025-04-07T17:49:37Z</updated>
    <published>2025-04-07T17:49:37Z</published>
    <title>Dion: A Communication-Efficient Optimizer for Large Models</title>
    <summary>  Training large AI models efficiently requires distributing computation across
multiple accelerators, but this often incurs significant communication overhead
-- especially during gradient synchronization. We introduce Dion, a
communication-efficient optimizer that retains the synchronous semantics of
standard distributed training (e.g., DDP, FSDP) while substantially reducing
I/O costs. Unlike conventional optimizers that synchronize full gradient
matrices, Dion leverages orthonormalized updates with device-local momentum
buffers, eliminating the need for full gradient exchange. It further supports
an efficient sharding strategy that avoids reconstructing large matrices during
training.
</summary>
    <author>
      <name>Kwangjun Ahn</name>
    </author>
    <author>
      <name>Byron Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">technical report; comments welcome!</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.05295v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.05295v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.05291v1</id>
    <updated>2025-04-07T17:45:17Z</updated>
    <published>2025-04-07T17:45:17Z</published>
    <title>Using Physiological Measures, Gaze, and Facial Expressions to Model
  Human Trust in a Robot Partner</title>
    <summary>  With robots becoming increasingly prevalent in various domains, it has become
crucial to equip them with tools to achieve greater fluency in interactions
with humans. One of the promising areas for further exploration lies in human
trust. A real-time, objective model of human trust could be used to maximize
productivity, preserve safety, and mitigate failure. In this work, we attempt
to use physiological measures, gaze, and facial expressions to model human
trust in a robot partner. We are the first to design an in-person, human-robot
supervisory interaction study to create a dedicated trust dataset. Using this
dataset, we train machine learning algorithms to identify the objective
measures that are most indicative of trust in a robot partner, advancing trust
prediction in human-robot interactions. Our findings indicate that a
combination of sensor modalities (blood volume pulse, electrodermal activity,
skin temperature, and gaze) can enhance the accuracy of detecting human trust
in a robot partner. Furthermore, the Extra Trees, Random Forest, and Decision
Trees classifiers exhibit consistently better performance in measuring the
person's trust in the robot partner. These results lay the groundwork for
constructing a real-time trust model for human-robot interaction, which could
foster more efficient interactions between humans and robots.
</summary>
    <author>
      <name>Haley N. Green</name>
    </author>
    <author>
      <name>Tariq Iqbal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the IEEE International Conference on Robotics and
  Automation (ICRA), 2025</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Robotics and Automation (ICRA),
  2025</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2504.05291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.05291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.05287v1</id>
    <updated>2025-04-07T17:38:19Z</updated>
    <published>2025-04-07T17:38:19Z</published>
    <title>RobustDexGrasp: Robust Dexterous Grasping of General Objects from
  Single-view Perception</title>
    <summary>  Robust grasping of various objects from single-view perception is fundamental
for dexterous robots. Previous works often rely on fully observable objects,
expert demonstrations, or static grasping poses, which restrict their
generalization ability and adaptability to external disturbances. In this
paper, we present a reinforcement-learning-based framework that enables
zero-shot dynamic dexterous grasping of a wide range of unseen objects from
single-view perception, while performing adaptive motions to external
disturbances. We utilize a hand-centric object representation for shape feature
extraction that emphasizes interaction-relevant local shapes, enhancing
robustness to shape variance and uncertainty. To enable effective hand
adaptation to disturbances with limited observations, we propose a mixed
curriculum learning strategy, which first utilizes imitation learning to
distill a policy trained with privileged real-time visual-tactile feedback, and
gradually transfers to reinforcement learning to learn adaptive motions under
disturbances caused by observation noises and dynamic randomization. Our
experiments demonstrate strong generalization in grasping unseen objects with
random poses, achieving success rates of 97.0% across 247,786 simulated objects
and 94.6% across 512 real objects. We also demonstrate the robustness of our
method to various disturbances, including unobserved object movement and
external forces, through both quantitative and qualitative evaluations. Project
Page: https://zdchan.github.io/Robust_DexGrasp/
</summary>
    <author>
      <name>Hui Zhang</name>
    </author>
    <author>
      <name>Zijian Wu</name>
    </author>
    <author>
      <name>Linyi Huang</name>
    </author>
    <author>
      <name>Sammy Christen</name>
    </author>
    <author>
      <name>Jie Song</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://zdchan.github.io/Robust_DexGrasp/</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.05287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.05287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
