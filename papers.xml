<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-02-27T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">407970</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2502.19417v1</id>
    <updated>2025-02-26T18:58:41Z</updated>
    <published>2025-02-26T18:58:41Z</published>
    <title>Hi Robot: Open-Ended Instruction Following with Hierarchical
  Vision-Language-Action Models</title>
    <summary>  Generalist robots that can perform a range of different tasks in open-world
settings must be able to not only reason about the steps needed to accomplish
their goals, but also process complex instructions, prompts, and even feedback
during task execution. Intricate instructions (e.g., "Could you make me a
vegetarian sandwich?" or "I don't like that one") require not just the ability
to physically perform the individual steps, but the ability to situate complex
commands and feedback in the physical world. In this work, we describe a system
that uses vision-language models in a hierarchical structure, first reasoning
over complex prompts and user feedback to deduce the most appropriate next step
to fulfill the task, and then performing that step with low-level actions. In
contrast to direct instruction following methods that can fulfill simple
commands ("pick up the cup"), our system can reason through complex prompts and
incorporate situated feedback during task execution ("that's not trash"). We
evaluate our system across three robotic platforms, including single-arm,
dual-arm, and dual-arm mobile robots, demonstrating its ability to handle tasks
such as cleaning messy tables, making sandwiches, and grocery shopping.
</summary>
    <author>
      <name>Lucy Xiaoyang Shi</name>
    </author>
    <author>
      <name>Brian Ichter</name>
    </author>
    <author>
      <name>Michael Equi</name>
    </author>
    <author>
      <name>Liyiming Ke</name>
    </author>
    <author>
      <name>Karl Pertsch</name>
    </author>
    <author>
      <name>Quan Vuong</name>
    </author>
    <author>
      <name>James Tanner</name>
    </author>
    <author>
      <name>Anna Walling</name>
    </author>
    <author>
      <name>Haohuan Wang</name>
    </author>
    <author>
      <name>Niccolo Fusai</name>
    </author>
    <author>
      <name>Adrian Li-Bell</name>
    </author>
    <author>
      <name>Danny Driess</name>
    </author>
    <author>
      <name>Lachy Groom</name>
    </author>
    <author>
      <name>Sergey Levine</name>
    </author>
    <author>
      <name>Chelsea Finn</name>
    </author>
    <link href="http://arxiv.org/abs/2502.19417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.19417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.19414v1</id>
    <updated>2025-02-26T18:58:13Z</updated>
    <published>2025-02-26T18:58:13Z</published>
    <title>Can Language Models Falsify? Evaluating Algorithmic Reasoning with
  Counterexample Creation</title>
    <summary>  There is growing excitement about the potential of Language Models (LMs) to
accelerate scientific discovery. Falsifying hypotheses is key to scientific
progress, as it allows claims to be iteratively refined over time. This process
requires significant researcher effort, reasoning, and ingenuity. Yet current
benchmarks for LMs predominantly assess their ability to generate solutions
rather than challenge them. We advocate for developing benchmarks that evaluate
this inverse capability - creating counterexamples for subtly incorrect
solutions. To demonstrate this approach, we start with the domain of
algorithmic problem solving, where counterexamples can be evaluated
automatically using code execution. Specifically, we introduce REFUTE, a
dynamically updating benchmark that includes recent problems and incorrect
submissions from programming competitions, where human experts successfully
identified counterexamples. Our analysis finds that the best reasoning agents,
even OpenAI o3-mini (high) with code execution feedback, can create
counterexamples for only &lt;9% of incorrect solutions in REFUTE, even though
ratings indicate its ability to solve up to 48% of these problems from scratch.
We hope our work spurs progress in evaluating and enhancing LMs' ability to
falsify incorrect solutions - a capability that is crucial for both
accelerating research and making models self-improve through reliable
reflective reasoning.
</summary>
    <author>
      <name>Shiven Sinha</name>
    </author>
    <author>
      <name>Shashwat Goel</name>
    </author>
    <author>
      <name>Ponnurangam Kumaraguru</name>
    </author>
    <author>
      <name>Jonas Geiping</name>
    </author>
    <author>
      <name>Matthias Bethge</name>
    </author>
    <author>
      <name>Ameya Prabhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.19414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.19414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.19413v1</id>
    <updated>2025-02-26T18:56:52Z</updated>
    <published>2025-02-26T18:56:52Z</published>
    <title>Project Alexandria: Towards Freeing Scientific Knowledge from Copyright
  Burdens via LLMs</title>
    <summary>  Paywalls, licenses and copyright rules often restrict the broad dissemination
and reuse of scientific knowledge. We take the position that it is both legally
and technically feasible to extract the scientific knowledge in scholarly
texts. Current methods, like text embeddings, fail to reliably preserve factual
content, and simple paraphrasing may not be legally sound. We urge the
community to adopt a new idea: convert scholarly documents into Knowledge Units
using LLMs. These units use structured data capturing entities, attributes and
relationships without stylistic content. We provide evidence that Knowledge
Units: (1) form a legally defensible framework for sharing knowledge from
copyrighted research texts, based on legal analyses of German copyright law and
U.S. Fair Use doctrine, and (2) preserve most (~95%) factual knowledge from
original text, measured by MCQ performance on facts from the original
copyrighted text across four research domains. Freeing scientific knowledge
from copyright promises transformative benefits for scientific research and
education by allowing language models to reuse important facts from copyrighted
text. To support this, we share open-source tools for converting research
documents into Knowledge Units. Overall, our work posits the feasibility of
democratizing access to scientific knowledge while respecting copyright.
</summary>
    <author>
      <name>Christoph Schuhmann</name>
    </author>
    <author>
      <name>Gollam Rabby</name>
    </author>
    <author>
      <name>Ameya Prabhu</name>
    </author>
    <author>
      <name>Tawsif Ahmed</name>
    </author>
    <author>
      <name>Andreas Hochlehnert</name>
    </author>
    <author>
      <name>Huu Nguyen</name>
    </author>
    <author>
      <name>Nick Akinci Heidrich</name>
    </author>
    <author>
      <name>Ludwig Schmidt</name>
    </author>
    <author>
      <name>Robert Kaczmarczyk</name>
    </author>
    <author>
      <name>SÃ¶ren Auer</name>
    </author>
    <author>
      <name>Jenia Jitsev</name>
    </author>
    <author>
      <name>Matthias Bethge</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.19413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.19413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.19411v1</id>
    <updated>2025-02-26T18:55:42Z</updated>
    <published>2025-02-26T18:55:42Z</published>
    <title>Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and
  Reasoning-Driven Code Intelligence in LLMs</title>
    <summary>  In large language models (LLMs), code and reasoning reinforce each other:
code offers an abstract, modular, and logic-driven structure that supports
reasoning, while reasoning translates high-level goals into smaller, executable
steps that drive more advanced code intelligence. In this study, we examine how
code serves as a structured medium for enhancing reasoning: it provides
verifiable execution paths, enforces logical decomposition, and enables runtime
validation. We also explore how improvements in reasoning have transformed code
intelligence from basic completion to advanced capabilities, enabling models to
address complex software engineering tasks through planning and debugging.
Finally, we identify key challenges and propose future research directions to
strengthen this synergy, ultimately improving LLM's performance in both areas.
</summary>
    <author>
      <name>Dayu Yang</name>
    </author>
    <author>
      <name>Tianyang Liu</name>
    </author>
    <author>
      <name>Daoan Zhang</name>
    </author>
    <author>
      <name>Antoine Simoulin</name>
    </author>
    <author>
      <name>Xiaoyi Liu</name>
    </author>
    <author>
      <name>Yuwei Cao</name>
    </author>
    <author>
      <name>Zhaopu Teng</name>
    </author>
    <author>
      <name>Xin Qian</name>
    </author>
    <author>
      <name>Grey Yang</name>
    </author>
    <author>
      <name>Jiebo Luo</name>
    </author>
    <author>
      <name>Julian McAuley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Repo: https://github.com/dayuyang1999/Awesome-Code-Reasoning</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.19411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.19411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.19409v1</id>
    <updated>2025-02-26T18:55:06Z</updated>
    <published>2025-02-26T18:55:06Z</published>
    <title>ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal
  Large Language Models</title>
    <summary>  Reasoning over sequences of images remains a challenge for multimodal large
language models (MLLMs). While recent models incorporate multi-image data
during pre-training, they still struggle to recognize sequential structures,
often treating images independently. This work introduces ImageChain, a
framework that enhances MLLMs with sequential reasoning capabilities over image
data by modeling visual sequences as a multi-turn conversation. In ImageChain,
images are interleaved with corresponding textual descriptions to form a
controlled dialogue that explicitly captures temporal dependencies and
narrative progression. Our method optimizes for the task of next-scene
description, where the model generates a context-aware description of an
upcoming scene based on preceding visual and textual cues. We demonstrate that
our approach improves performance on the next-scene description task --
achieving an average improvement from 3.7% to 19% in SimRate, a metric that
quantifies semantic similarity to human-annotated ground truths. Moreover,
ImageChain achieves robust zero-shot out-of-domain performance in applications
ranging from comics to robotics. Extensive experiments validate that
instruction-tuning in a multimodal, multi-turn conversation design is key to
bridging the gap between static image understanding and temporally-aware
reasoning.
</summary>
    <author>
      <name>Danae SÃ¡nchez Villegas</name>
    </author>
    <author>
      <name>Ingo Ziegler</name>
    </author>
    <author>
      <name>Desmond Elliott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code, dataset, and checkpoints are publicly available at
  https://github.com/danaesavi/ImageChain</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.19409v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.19409v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
