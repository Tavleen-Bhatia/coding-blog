<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-05-20T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">426237</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.13448v1</id>
    <updated>2025-05-19T17:59:58Z</updated>
    <published>2025-05-19T17:59:58Z</published>
    <title>CIE: Controlling Language Model Text Generations Using Continuous
  Signals</title>
    <summary>  Aligning language models with user intent is becoming increasingly relevant
to enhance user experience. This calls for designing methods that can allow
users to control the properties of the language that LMs generate. For example,
controlling the length of the generation, the complexity of the language that
gets chosen, the sentiment, tone, etc. Most existing work attempts to integrate
users' control by conditioning LM generations on natural language prompts or
discrete control signals, which are often brittle and hard to scale. In this
work, we are interested in \textit{continuous} control signals, ones that exist
along a spectrum that can't easily be captured in a natural language prompt or
via existing techniques in conditional generation. Through a case study in
controlling the precise response-length of generations produced by LMs, we
demonstrate how after fine-tuning, behaviors of language models can be
controlled via continuous signals -- as vectors that are interpolated between a
"low" and a "high" token embedding. Our method more reliably exerts
response-length control than in-context learning methods or fine-tuning methods
that represent the control signal as a discrete signal. Our full open-sourced
code and datasets are available at https://github.com/vsamuel2003/CIE.
</summary>
    <author>
      <name>Vinay Samuel</name>
    </author>
    <author>
      <name>Harshita Diddee</name>
    </author>
    <author>
      <name>Yiming Zhang</name>
    </author>
    <author>
      <name>Daphne Ippolito</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.13448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.13448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.13447v1</id>
    <updated>2025-05-19T17:59:42Z</updated>
    <published>2025-05-19T17:59:42Z</published>
    <title>Mean Flows for One-step Generative Modeling</title>
    <summary>  We propose a principled and effective framework for one-step generative
modeling. We introduce the notion of average velocity to characterize flow
fields, in contrast to instantaneous velocity modeled by Flow Matching methods.
A well-defined identity between average and instantaneous velocities is derived
and used to guide neural network training. Our method, termed the MeanFlow
model, is self-contained and requires no pre-training, distillation, or
curriculum learning. MeanFlow demonstrates strong empirical performance: it
achieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet
256x256 trained from scratch, significantly outperforming previous
state-of-the-art one-step diffusion/flow models. Our study substantially
narrows the gap between one-step diffusion/flow models and their multi-step
predecessors, and we hope it will motivate future research to revisit the
foundations of these powerful models.
</summary>
    <author>
      <name>Zhengyang Geng</name>
    </author>
    <author>
      <name>Mingyang Deng</name>
    </author>
    <author>
      <name>Xingjian Bai</name>
    </author>
    <author>
      <name>J. Zico Kolter</name>
    </author>
    <author>
      <name>Kaiming He</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Tech report</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.13447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.13447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.13446v1</id>
    <updated>2025-05-19T17:59:35Z</updated>
    <published>2025-05-19T17:59:35Z</published>
    <title>Unlocking Non-Invasive Brain-to-Text</title>
    <summary>  Despite major advances in surgical brain-to-text (B2T), i.e. transcribing
speech from invasive brain recordings, non-invasive alternatives have yet to
surpass even chance on standard metrics. This remains a barrier to building a
non-invasive brain-computer interface (BCI) capable of restoring communication
in paralysed individuals without surgery. Here, we present the first
non-invasive B2T result that significantly exceeds these critical baselines,
raising BLEU by $1.4\mathrm{-}2.6\times$ over prior work. This result is driven
by three contributions: (1) we extend recent word-classification models with
LLM-based rescoring, transforming single-word predictors into closed-vocabulary
B2T systems; (2) we introduce a predictive in-filling approach to handle
out-of-vocabulary (OOV) words, substantially expanding the effective
vocabulary; and (3) we demonstrate, for the first time, how to scale
non-invasive B2T models across datasets, unlocking deep learning at scale and
improving accuracy by $2.1\mathrm{-}2.3\times$. Through these contributions, we
offer new insights into the roles of data quality and vocabulary size.
Together, our results remove a major obstacle to realising practical
non-invasive B2T systems.
</summary>
    <author>
      <name>Dulhan Jayalath</name>
    </author>
    <author>
      <name>Gilad Landau</name>
    </author>
    <author>
      <name>Oiwi Parker Jones</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 10 figures, 10 tables. Under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.13446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.13446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.13445v1</id>
    <updated>2025-05-19T17:59:31Z</updated>
    <published>2025-05-19T17:59:31Z</published>
    <title>Trust, But Verify: A Self-Verification Approach to Reinforcement
  Learning with Verifiable Rewards</title>
    <summary>  Large Language Models (LLMs) show great promise in complex reasoning, with
Reinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement
strategy. However, a prevalent issue is ``superficial self-reflection'', where
models fail to robustly verify their own outputs. We introduce RISE
(Reinforcing Reasoning with Self-Verification), a novel online RL framework
designed to tackle this. RISE explicitly and simultaneously trains an LLM to
improve both its problem-solving and self-verification abilities within a
single, integrated RL process. The core mechanism involves leveraging
verifiable rewards from an outcome verifier to provide on-the-fly feedback for
both solution generation and self-verification tasks. In each iteration, the
model generates solutions, then critiques its own on-policy generated
solutions, with both trajectories contributing to the policy update. Extensive
experiments on diverse mathematical reasoning benchmarks show that RISE
consistently improves model's problem-solving accuracy while concurrently
fostering strong self-verification skills. Our analyses highlight the
advantages of online verification and the benefits of increased verification
compute. Additionally, RISE models exhibit more frequent and accurate
self-verification behaviors during reasoning. These advantages reinforce RISE
as a flexible and effective path towards developing more robust and self-aware
reasoners.
</summary>
    <author>
      <name>Xiaoyuan Liu</name>
    </author>
    <author>
      <name>Tian Liang</name>
    </author>
    <author>
      <name>Zhiwei He</name>
    </author>
    <author>
      <name>Jiahao Xu</name>
    </author>
    <author>
      <name>Wenxuan Wang</name>
    </author>
    <author>
      <name>Pinjia He</name>
    </author>
    <author>
      <name>Zhaopeng Tu</name>
    </author>
    <author>
      <name>Haitao Mi</name>
    </author>
    <author>
      <name>Dong Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">code available at https://github.com/xyliu-cs/RISE</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.13445v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.13445v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.13443v1</id>
    <updated>2025-05-19T17:59:23Z</updated>
    <published>2025-05-19T17:59:23Z</published>
    <title>Wonderings on Wiggly Bispectra: Non-linear Evolution and Reconstruction
  of Oscillations in the Squeezed Bispectrum</title>
    <summary>  Oscillations in the primordial bispectrum are sourced by a range of
inflationary phenomena, including features in the inflaton potential and
interactions with massive fields through the Cosmological Collider scenario.
These signatures offer a powerful window into early-universe physics. In this
work, we study how oscillations of the form $\lim_{q\ll k}B(q,k)\propto
\cos(\mu \ln(q/k))$ impact the non-linear squeezed matter bispectrum. Using a
suite of $N$-body simulations with non-Gaussian initial conditions, we show
that non-linear evolution significantly damps these oscillations, effectively
erasing the signal on scales $k \gtrsim 0.3~h/{\rm Mpc}$ at redshift $z=0$.
This damping is well-described by the Zel'dovich approximation and can be
modeled deep into the non-linear regime using non-perturbative separate
universe simulations. Promisingly, we show that reconstruction techniques
developed for baryon acoustic oscillation (BAO) analyses can largely undo this
damping, improving constraints on the amplitude (phase) of oscillations in the
primordial squeezed bispectrum by up to a factor of five (four) at $z=0$. We
also discuss several challenges with modeling the non-linear evolution of the
squeezed bispectrum in the Cosmological Collider scenario, where the bispectrum
is suppressed by a factor of $(q/k)^{3/2}$ relative to the template studied
here. Our findings pave the way for future searches for oscillatory bispectra
using large-scale structure data.
</summary>
    <author>
      <name>Samuel Goldstein</name>
    </author>
    <author>
      <name>Oliver H. E. Philcox</name>
    </author>
    <author>
      <name>Emanuele Fondi</name>
    </author>
    <author>
      <name>William R. Coulton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6+3 figures, 12+4 pages, lots of wiggles! Prepared for submission to
  Phys. Rev. D</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.13443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.13443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
