<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-03-30T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">415102</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.21781v1</id>
    <updated>2025-03-27T17:59:58Z</updated>
    <published>2025-03-27T17:59:58Z</published>
    <title>VideoMage: Multi-Subject and Motion Customization of Text-to-Video
  Diffusion Models</title>
    <summary>  Customized text-to-video generation aims to produce high-quality videos that
incorporate user-specified subject identities or motion patterns. However,
existing methods mainly focus on personalizing a single concept, either subject
identity or motion pattern, limiting their effectiveness for multiple subjects
with the desired motion patterns. To tackle this challenge, we propose a
unified framework VideoMage for video customization over both multiple subjects
and their interactive motions. VideoMage employs subject and motion LoRAs to
capture personalized content from user-provided images and videos, along with
an appearance-agnostic motion learning approach to disentangle motion patterns
from visual appearance. Furthermore, we develop a spatial-temporal composition
scheme to guide interactions among subjects within the desired motion patterns.
Extensive experiments demonstrate that VideoMage outperforms existing methods,
generating coherent, user-controlled videos with consistent subject identities
and interactions.
</summary>
    <author>
      <name>Chi-Pin Huang</name>
    </author>
    <author>
      <name>Yen-Siang Wu</name>
    </author>
    <author>
      <name>Hung-Kai Chung</name>
    </author>
    <author>
      <name>Kai-Po Chang</name>
    </author>
    <author>
      <name>Fu-En Yang</name>
    </author>
    <author>
      <name>Yu-Chiang Frank Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2025. Project Page:
  https://jasper0314-huang.github.io/videomage-customization</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.21781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.21781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.21779v1</id>
    <updated>2025-03-27T17:59:57Z</updated>
    <published>2025-03-27T17:59:57Z</published>
    <title>X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time
  Tomographic Reconstruction</title>
    <summary>  Four-dimensional computed tomography (4D CT) reconstruction is crucial for
capturing dynamic anatomical changes but faces inherent limitations from
conventional phase-binning workflows. Current methods discretize temporal
resolution into fixed phases with respiratory gating devices, introducing
motion misalignment and restricting clinical practicality. In this paper, We
propose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CT
reconstruction by integrating dynamic radiative Gaussian splatting with
self-supervised respiratory motion learning. Our approach models anatomical
dynamics through a spatiotemporal encoder-decoder architecture that predicts
time-varying Gaussian deformations, eliminating phase discretization. To remove
dependency on external gating devices, we introduce a physiology-driven
periodic consistency loss that learns patient-specific breathing cycles
directly from projections via differentiable optimization. Extensive
experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR
gain over traditional methods and 2.25 dB improvement against prior Gaussian
splatting techniques. By unifying continuous motion modeling with hardware-free
period learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction for
dynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.
</summary>
    <author>
      <name>Weihao Yu</name>
    </author>
    <author>
      <name>Yuanhao Cai</name>
    </author>
    <author>
      <name>Ruyi Zha</name>
    </author>
    <author>
      <name>Zhiwen Fan</name>
    </author>
    <author>
      <name>Chenxin Li</name>
    </author>
    <author>
      <name>Yixuan Yuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://x2-gaussian.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.21779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.21779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.21777v1</id>
    <updated>2025-03-27T17:59:52Z</updated>
    <published>2025-03-27T17:59:52Z</published>
    <title>Test-Time Visual In-Context Tuning</title>
    <summary>  Visual in-context learning (VICL), as a new paradigm in computer vision,
allows the model to rapidly adapt to various tasks with only a handful of
prompts and examples. While effective, the existing VICL paradigm exhibits poor
generalizability under distribution shifts. In this work, we propose test-time
Visual In-Context Tuning (VICT), a method that can adapt VICL models on the fly
with a single test sample. Specifically, we flip the role between the task
prompts and the test sample and use a cycle consistency loss to reconstruct the
original task prompt output. Our key insight is that a model should be aware of
a new test distribution if it can successfully recover the original task
prompts. Extensive experiments on six representative vision tasks ranging from
high-level visual understanding to low-level image processing, with 15 common
corruptions, demonstrate that our VICT can improve the generalizability of VICL
to unseen new domains. In addition, we show the potential of applying VICT for
unseen tasks at test time. Code: https://github.com/Jiahao000/VICT.
</summary>
    <author>
      <name>Jiahao Xie</name>
    </author>
    <author>
      <name>Alessio Tonioni</name>
    </author>
    <author>
      <name>Nathalie Rauschmayr</name>
    </author>
    <author>
      <name>Federico Tombari</name>
    </author>
    <author>
      <name>Bernt Schiele</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2025. Code: https://github.com/Jiahao000/VICT</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.21777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.21777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.21776v1</id>
    <updated>2025-03-27T17:59:51Z</updated>
    <published>2025-03-27T17:59:51Z</published>
    <title>Video-R1: Reinforcing Video Reasoning in MLLMs</title>
    <summary>  Inspired by DeepSeek-R1's success in eliciting reasoning abilities through
rule-based reinforcement learning (RL), we introduce Video-R1 as the first
attempt to systematically explore the R1 paradigm for eliciting video reasoning
within multimodal large language models (MLLMs). However, directly applying RL
training with the GRPO algorithm to video reasoning presents two primary
challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the
scarcity of high-quality video-reasoning data. To address these issues, we
first propose the T-GRPO algorithm, which encourages models to utilize temporal
information in videos for reasoning. Additionally, instead of relying solely on
video data, we incorporate high-quality image-reasoning data into the training
process. We have constructed two datasets: Video-R1-COT-165k for SFT cold start
and Video-R1-260k for RL training, both comprising image and video data.
Experimental results demonstrate that Video-R1 achieves significant
improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as
well as on general video benchmarks including MVBench and TempCompass, etc.
Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning
benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All
codes, models, data are released.
</summary>
    <author>
      <name>Kaituo Feng</name>
    </author>
    <author>
      <name>Kaixiong Gong</name>
    </author>
    <author>
      <name>Bohao Li</name>
    </author>
    <author>
      <name>Zonghao Guo</name>
    </author>
    <author>
      <name>Yibing Wang</name>
    </author>
    <author>
      <name>Tianshuo Peng</name>
    </author>
    <author>
      <name>Benyou Wang</name>
    </author>
    <author>
      <name>Xiangyu Yue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://github.com/tulerfeng/Video-R1</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.21776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.21776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.21775v1</id>
    <updated>2025-03-27T17:59:46Z</updated>
    <published>2025-03-27T17:59:46Z</published>
    <title>StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross
  Fusion</title>
    <summary>  We present StyleMotif, a novel Stylized Motion Latent Diffusion model,
generating motion conditioned on both content and style from multiple
modalities. Unlike existing approaches that either focus on generating diverse
motion content or transferring style from sequences, StyleMotif seamlessly
synthesizes motion across a wide range of content while incorporating stylistic
cues from multi-modal inputs, including motion, text, image, video, and audio.
To achieve this, we introduce a style-content cross fusion mechanism and align
a style encoder with a pre-trained multi-modal model, ensuring that the
generated motion accurately captures the reference style while preserving
realism. Extensive experiments demonstrate that our framework surpasses
existing methods in stylized motion generation and exhibits emergent
capabilities for multi-modal motion stylization, enabling more nuanced motion
synthesis. Source code and pre-trained models will be released upon acceptance.
Project Page: https://stylemotif.github.io
</summary>
    <author>
      <name>Ziyu Guo</name>
    </author>
    <author>
      <name>Young Yoon Lee</name>
    </author>
    <author>
      <name>Joseph Liu</name>
    </author>
    <author>
      <name>Yizhak Ben-Shabat</name>
    </author>
    <author>
      <name>Victor Zordan</name>
    </author>
    <author>
      <name>Mubbasir Kapadia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://stylemotif.github.io</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.21775v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.21775v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
