<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-04-22T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">420184</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.15279v1</id>
    <updated>2025-04-21T17:59:53Z</updated>
    <published>2025-04-21T17:59:53Z</published>
    <title>VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal
  Large Language Models</title>
    <summary>  Visual reasoning is a core component of human intelligence and a critical
capability for advanced multimodal models. Yet current reasoning evaluations of
multimodal large language models (MLLMs) often rely on text descriptions and
allow language-based reasoning shortcuts, failing to measure genuine
vision-centric reasoning. To address this, we introduce VisuLogic: a benchmark
of 1,000 human-verified problems across six categories (e.g., quantitative
shifts, spatial relations, attribute comparisons). These various types of
questions can be evaluated to assess the visual reasoning capabilities of MLLMs
from multiple perspectives. We evaluate leading MLLMs on this benchmark and
analyze their results to identify common failure modes. Most models score below
30% accuracy-only slightly above the 25% random baseline and far below the
51.4% achieved by humans-revealing significant gaps in visual reasoning.
Furthermore, we provide a supplementary training dataset and a
reinforcement-learning baseline to support further progress.
</summary>
    <author>
      <name>Weiye Xu</name>
    </author>
    <author>
      <name>Jiahao Wang</name>
    </author>
    <author>
      <name>Weiyun Wang</name>
    </author>
    <author>
      <name>Zhe Chen</name>
    </author>
    <author>
      <name>Wengang Zhou</name>
    </author>
    <author>
      <name>Aijun Yang</name>
    </author>
    <author>
      <name>Lewei Lu</name>
    </author>
    <author>
      <name>Houqiang Li</name>
    </author>
    <author>
      <name>Xiaohua Wang</name>
    </author>
    <author>
      <name>Xizhou Zhu</name>
    </author>
    <author>
      <name>Wenhai Wang</name>
    </author>
    <author>
      <name>Jifeng Dai</name>
    </author>
    <author>
      <name>Jinguo Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code, data, and baselines are available at
  https://visulogic-benchmark.github.io/VisuLogic</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.15279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15275v1</id>
    <updated>2025-04-21T17:59:02Z</updated>
    <published>2025-04-21T17:59:02Z</published>
    <title>Stop Summation: Min-Form Credit Assignment Is All Process Reward Model
  Needs for Reasoning</title>
    <summary>  Process reward models (PRMs) have proven effective for test-time scaling of
Large Language Models (LLMs) on challenging reasoning tasks. However, reward
hacking issues with PRMs limit their successful application in reinforcement
fine-tuning. In this paper, we identify the main cause of PRM-induced reward
hacking: the canonical summation-form credit assignment in reinforcement
learning (RL), which defines the value as cumulative gamma-decayed future
rewards, easily induces LLMs to hack steps with high rewards. To address this,
we propose PURE: Process sUpervised Reinforcement lEarning. The key innovation
of PURE is a min-form credit assignment that formulates the value function as
the minimum of future rewards. This method significantly alleviates reward
hacking by limiting the value function range and distributing advantages more
reasonably. Through extensive experiments on 3 base models, we show that
PRM-based approaches enabling min-form credit assignment achieve comparable
reasoning performance to verifiable reward-based methods within only 30% steps.
In contrast, the canonical sum-form credit assignment collapses training even
at the beginning! Additionally, when we supplement PRM-based fine-tuning with
just 10% verifiable rewards, we further alleviate reward hacking and produce
the best fine-tuned model based on Qwen2.5-Math-7B in our experiments,
achieving 82.5% accuracy on AMC23 and 53.3% average accuracy across 5
benchmarks. Moreover, we summarize the observed reward hacking cases and
analyze the causes of training collapse. Code and models are available at
https://github.com/CJReinforce/PURE.
</summary>
    <author>
      <name>Jie Cheng</name>
    </author>
    <author>
      <name>Ruixi Qiao</name>
    </author>
    <author>
      <name>Lijun Li</name>
    </author>
    <author>
      <name>Chao Guo</name>
    </author>
    <author>
      <name>Junle Wang</name>
    </author>
    <author>
      <name>Gang Xiong</name>
    </author>
    <author>
      <name>Yisheng Lv</name>
    </author>
    <author>
      <name>Fei-Yue Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2504.15275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15271v1</id>
    <updated>2025-04-21T17:57:28Z</updated>
    <published>2025-04-21T17:57:28Z</published>
    <title>Eagle 2.5: Boosting Long-Context Post-Training for Frontier
  Vision-Language Models</title>
    <summary>  We introduce Eagle 2.5, a family of frontier vision-language models (VLMs)
for long-context multimodal learning. Our work addresses the challenges in long
video comprehension and high-resolution image understanding, introducing a
generalist framework for both tasks. The proposed training framework
incorporates Automatic Degrade Sampling and Image Area Preservation, two
techniques that preserve contextual integrity and visual details. The framework
also includes numerous efficiency optimizations in the pipeline for
long-context data training. Finally, we propose Eagle-Video-110K, a novel
dataset that integrates both story-level and clip-level annotations,
facilitating long-video understanding. Eagle 2.5 demonstrates substantial
improvements on long-context multimodal benchmarks, providing a robust solution
to the limitations of existing VLMs. Notably, our best model Eagle 2.5-8B
achieves 72.4% on Video-MME with 512 input frames, matching the results of
top-tier commercial model such as GPT-4o and large-scale open-source models
like Qwen2.5-VL-72B and InternVL2.5-78B.
</summary>
    <author>
      <name>Guo Chen</name>
    </author>
    <author>
      <name>Zhiqi Li</name>
    </author>
    <author>
      <name>Shihao Wang</name>
    </author>
    <author>
      <name>Jindong Jiang</name>
    </author>
    <author>
      <name>Yicheng Liu</name>
    </author>
    <author>
      <name>Lidong Lu</name>
    </author>
    <author>
      <name>De-An Huang</name>
    </author>
    <author>
      <name>Wonmin Byeon</name>
    </author>
    <author>
      <name>Matthieu Le</name>
    </author>
    <author>
      <name>Tuomas Rintamaki</name>
    </author>
    <author>
      <name>Tyler Poon</name>
    </author>
    <author>
      <name>Max Ehrlich</name>
    </author>
    <author>
      <name>Tuomas Rintamaki</name>
    </author>
    <author>
      <name>Tyler Poon</name>
    </author>
    <author>
      <name>Tong Lu</name>
    </author>
    <author>
      <name>Limin Wang</name>
    </author>
    <author>
      <name>Bryan Catanzaro</name>
    </author>
    <author>
      <name>Jan Kautz</name>
    </author>
    <author>
      <name>Andrew Tao</name>
    </author>
    <author>
      <name>Zhiding Yu</name>
    </author>
    <author>
      <name>Guilin Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2504.15271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15267v1</id>
    <updated>2025-04-21T17:49:06Z</updated>
    <published>2025-04-21T17:49:06Z</published>
    <title>Diffusion Bridge Models for 3D Medical Image Translation</title>
    <summary>  Diffusion tensor imaging (DTI) provides crucial insights into the
microstructure of the human brain, but it can be time-consuming to acquire
compared to more readily available T1-weighted (T1w) magnetic resonance imaging
(MRI). To address this challenge, we propose a diffusion bridge model for 3D
brain image translation between T1w MRI and DTI modalities. Our model learns to
generate high-quality DTI fractional anisotropy (FA) images from T1w images and
vice versa, enabling cross-modality data augmentation and reducing the need for
extensive DTI acquisition. We evaluate our approach using perceptual
similarity, pixel-level agreement, and distributional consistency metrics,
demonstrating strong performance in capturing anatomical structures and
preserving information on white matter integrity. The practical utility of the
synthetic data is validated through sex classification and Alzheimer's disease
classification tasks, where the generated images achieve comparable performance
to real data. Our diffusion bridge model offers a promising solution for
improving neuroimaging datasets and supporting clinical decision-making, with
the potential to significantly impact neuroimaging research and clinical
practice.
</summary>
    <author>
      <name>Shaorong Zhang</name>
    </author>
    <author>
      <name>Tamoghna Chattopadhyay</name>
    </author>
    <author>
      <name>Sophia I. Thomopoulos</name>
    </author>
    <author>
      <name>Jose-Luis Ambite</name>
    </author>
    <author>
      <name>Paul M. Thompson</name>
    </author>
    <author>
      <name>Greg Ver Steeg</name>
    </author>
    <link href="http://arxiv.org/abs/2504.15267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15266v1</id>
    <updated>2025-04-21T17:47:46Z</updated>
    <published>2025-04-21T17:47:46Z</published>
    <title>Roll the dice &amp; look before you leap: Going beyond the creative limits
  of next-token prediction</title>
    <summary>  We design a suite of minimal algorithmic tasks that are a loose abstraction
of open-ended real-world tasks. This allows us to cleanly and controllably
quantify the creative limits of the present-day language model. Much like
real-world tasks that require a creative, far-sighted leap of thought, our
tasks require an implicit, open-ended stochastic planning step that either (a)
discovers new connections in an abstract knowledge graph (like in wordplay,
drawing analogies, or research) or (b) constructs new patterns (like in
designing math problems or new proteins). In these tasks, we empirically and
conceptually argue how next-token learning is myopic and memorizes excessively;
comparatively, multi-token approaches, namely teacherless training and
diffusion models, excel in producing diverse and original output. Secondly, in
our tasks, we find that to elicit randomness from the Transformer without
hurting coherence, it is better to inject noise right at the input layer (via a
method we dub hash-conditioning) rather than defer to temperature sampling from
the output layer. Thus, our work offers a principled, minimal test-bed for
analyzing open-ended creative skills, and offers new arguments for going beyond
next-token learning and softmax-based sampling. We make part of the code
available under https://github.com/chenwu98/algorithmic-creativity
</summary>
    <author>
      <name>Vaishnavh Nagarajan</name>
    </author>
    <author>
      <name>Chen Henry Wu</name>
    </author>
    <author>
      <name>Charles Ding</name>
    </author>
    <author>
      <name>Aditi Raghunathan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.15266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
