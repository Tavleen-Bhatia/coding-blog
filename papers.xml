<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-04-20T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">419547</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.13181v1</id>
    <updated>2025-04-17T17:59:57Z</updated>
    <published>2025-04-17T17:59:57Z</published>
    <title>Perception Encoder: The best visual embeddings are not at the output of
  the network</title>
    <summary>  We introduce Perception Encoder (PE), a state-of-the-art encoder for image
and video understanding trained via simple vision-language learning.
Traditionally, vision encoders have relied on a variety of pretraining
objectives, each tailored to specific downstream tasks such as classification,
captioning, or localization. Surprisingly, after scaling our carefully tuned
image pretraining recipe and refining with our robust video data engine, we
find that contrastive vision-language training alone can produce strong,
general embeddings for all of these downstream tasks. There is only one caveat:
these embeddings are hidden within the intermediate layers of the network. To
draw them out, we introduce two alignment methods, language alignment for
multimodal language modeling, and spatial alignment for dense prediction.
Together with the core contrastive checkpoint, our PE family of models achieves
state-of-the-art performance on a wide variety of tasks, including zero-shot
image and video classification and retrieval; document, image, and video Q&amp;A;
and spatial tasks such as detection, depth estimation, and tracking. To foster
further research, we are releasing our models, code, and a novel dataset of
synthetically and human-annotated videos.
</summary>
    <author>
      <name>Daniel Bolya</name>
    </author>
    <author>
      <name>Po-Yao Huang</name>
    </author>
    <author>
      <name>Peize Sun</name>
    </author>
    <author>
      <name>Jang Hyun Cho</name>
    </author>
    <author>
      <name>Andrea Madotto</name>
    </author>
    <author>
      <name>Chen Wei</name>
    </author>
    <author>
      <name>Tengyu Ma</name>
    </author>
    <author>
      <name>Jiale Zhi</name>
    </author>
    <author>
      <name>Jathushan Rajasegaran</name>
    </author>
    <author>
      <name>Hanoona Rasheed</name>
    </author>
    <author>
      <name>Junke Wang</name>
    </author>
    <author>
      <name>Marco Monteiro</name>
    </author>
    <author>
      <name>Hu Xu</name>
    </author>
    <author>
      <name>Shiyu Dong</name>
    </author>
    <author>
      <name>Nikhila Ravi</name>
    </author>
    <author>
      <name>Daniel Li</name>
    </author>
    <author>
      <name>Piotr Dollár</name>
    </author>
    <author>
      <name>Christoph Feichtenhofer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Initial Submission</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.13181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.13181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.13180v1</id>
    <updated>2025-04-17T17:59:56Z</updated>
    <published>2025-04-17T17:59:56Z</published>
    <title>PerceptionLM: Open-Access Data and Models for Detailed Visual
  Understanding</title>
    <summary>  Vision-language models are integral to computer vision research, yet many
high-performing models remain closed-source, obscuring their data, design and
training recipe. The research community has responded by using distillation
from black-box models to label training data, achieving strong benchmark
results, at the cost of measurable scientific progress. However, without
knowing the details of the teacher model and its data sources, scientific
progress remains difficult to measure. In this paper, we study building a
Perception Language Model (PLM) in a fully open and reproducible framework for
transparent research in image and video understanding. We analyze standard
training pipelines without distillation from proprietary models and explore
large-scale synthetic data to identify critical data gaps, particularly in
detailed video understanding. To bridge these gaps, we release 2.8M
human-labeled instances of fine-grained video question-answer pairs and
spatio-temporally grounded video captions. Additionally, we introduce
PLM-VideoBench, a suite for evaluating challenging video understanding tasks
focusing on the ability to reason about "what", "where", "when", and "how" of a
video. We make our work fully reproducible by providing data, training recipes,
code &amp; models.
</summary>
    <author>
      <name>Jang Hyun Cho</name>
    </author>
    <author>
      <name>Andrea Madotto</name>
    </author>
    <author>
      <name>Effrosyni Mavroudi</name>
    </author>
    <author>
      <name>Triantafyllos Afouras</name>
    </author>
    <author>
      <name>Tushar Nagarajan</name>
    </author>
    <author>
      <name>Muhammad Maaz</name>
    </author>
    <author>
      <name>Yale Song</name>
    </author>
    <author>
      <name>Tengyu Ma</name>
    </author>
    <author>
      <name>Shuming Hu</name>
    </author>
    <author>
      <name>Suyog Jain</name>
    </author>
    <author>
      <name>Miguel Martin</name>
    </author>
    <author>
      <name>Huiyu Wang</name>
    </author>
    <author>
      <name>Hanoona Rasheed</name>
    </author>
    <author>
      <name>Peize Sun</name>
    </author>
    <author>
      <name>Po-Yao Huang</name>
    </author>
    <author>
      <name>Daniel Bolya</name>
    </author>
    <author>
      <name>Nikhila Ravi</name>
    </author>
    <author>
      <name>Shashank Jain</name>
    </author>
    <author>
      <name>Tammy Stark</name>
    </author>
    <author>
      <name>Shane Moon</name>
    </author>
    <author>
      <name>Babak Damavandi</name>
    </author>
    <author>
      <name>Vivian Lee</name>
    </author>
    <author>
      <name>Andrew Westbury</name>
    </author>
    <author>
      <name>Salman Khan</name>
    </author>
    <author>
      <name>Philipp Krähenbühl</name>
    </author>
    <author>
      <name>Piotr Dollár</name>
    </author>
    <author>
      <name>Lorenzo Torresani</name>
    </author>
    <author>
      <name>Kristen Grauman</name>
    </author>
    <author>
      <name>Christoph Feichtenhofer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.13180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.13180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.13178v1</id>
    <updated>2025-04-17T17:59:54Z</updated>
    <published>2025-04-17T17:59:54Z</published>
    <title>Aligning Constraint Generation with Design Intent in Parametric CAD</title>
    <summary>  We adapt alignment techniques from reasoning LLMs to the task of generating
engineering sketch constraints found in computer-aided design (CAD) models.
Engineering sketches consist of geometric primitives (e.g. points, lines)
connected by constraints (e.g. perpendicular, tangent) that define the
relationships between them. For a design to be easily editable, the constraints
must effectively capture design intent, ensuring the geometry updates
predictably when parameters change. Although current approaches can generate
CAD designs, an open challenge remains to align model outputs with design
intent, we label this problem `design alignment'. A critical first step towards
aligning generative CAD models is to generate constraints which fully-constrain
all geometric primitives, without over-constraining or distorting sketch
geometry. Using alignment techniques to train an existing constraint generation
model with feedback from a constraint solver, we are able to fully-constrain
93% of sketches compared to 34% when using a na\"ive supervised fine-tuning
(SFT) baseline and only 8.9% without alignment. Our approach can be applied to
any existing constraint generation model and sets the stage for further
research bridging alignment strategies between the language and design domains.
</summary>
    <author>
      <name>Evan Casey</name>
    </author>
    <author>
      <name>Tianyu Zhang</name>
    </author>
    <author>
      <name>Shu Ishida</name>
    </author>
    <author>
      <name>John Roger Thompson</name>
    </author>
    <author>
      <name>Amir Khasahmadi</name>
    </author>
    <author>
      <name>Joseph George Lambourne</name>
    </author>
    <author>
      <name>Pradeep Kumar Jayaraman</name>
    </author>
    <author>
      <name>Karl D. D. Willis</name>
    </author>
    <link href="http://arxiv.org/abs/2504.13178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.13178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.13175v1</id>
    <updated>2025-04-17T17:59:43Z</updated>
    <published>2025-04-17T17:59:43Z</published>
    <title>Novel Demonstration Generation with Gaussian Splatting Enables Robust
  One-Shot Manipulation</title>
    <summary>  Visuomotor policies learned from teleoperated demonstrations face challenges
such as lengthy data collection, high costs, and limited data diversity.
Existing approaches address these issues by augmenting image observations in
RGB space or employing Real-to-Sim-to-Real pipelines based on physical
simulators. However, the former is constrained to 2D data augmentation, while
the latter suffers from imprecise physical simulation caused by inaccurate
geometric reconstruction. This paper introduces RoboSplat, a novel method that
generates diverse, visually realistic demonstrations by directly manipulating
3D Gaussians. Specifically, we reconstruct the scene through 3D Gaussian
Splatting (3DGS), directly edit the reconstructed scene, and augment data
across six types of generalization with five techniques: 3D Gaussian
replacement for varying object types, scene appearance, and robot embodiments;
equivariant transformations for different object poses; visual attribute
editing for various lighting conditions; novel view synthesis for new camera
perspectives; and 3D content generation for diverse object types. Comprehensive
real-world experiments demonstrate that RoboSplat significantly enhances the
generalization of visuomotor policies under diverse disturbances. Notably,
while policies trained on hundreds of real-world demonstrations with additional
2D data augmentation achieve an average success rate of 57.2%, RoboSplat
attains 87.8% in one-shot settings across six types of generalization in the
real world.
</summary>
    <author>
      <name>Sizhe Yang</name>
    </author>
    <author>
      <name>Wenye Yu</name>
    </author>
    <author>
      <name>Jia Zeng</name>
    </author>
    <author>
      <name>Jun Lv</name>
    </author>
    <author>
      <name>Kerui Ren</name>
    </author>
    <author>
      <name>Cewu Lu</name>
    </author>
    <author>
      <name>Dahua Lin</name>
    </author>
    <author>
      <name>Jiangmiao Pang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at Robotics: Science and Systems (RSS) 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.13175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.13175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.13173v1</id>
    <updated>2025-04-17T17:59:33Z</updated>
    <published>2025-04-17T17:59:33Z</published>
    <title>It's All Connected: A Journey Through Test-Time Memorization,
  Attentional Bias, Retention, and Online Optimization</title>
    <summary>  Designing efficient and effective architectural backbones has been in the
core of research efforts to enhance the capability of foundation models.
Inspired by the human cognitive phenomenon of attentional bias-the natural
tendency to prioritize certain events or stimuli-we reconceptualize neural
architectures, including Transformers, Titans, and modern linear recurrent
neural networks as associative memory modules that learn a mapping of keys and
values using an internal objective, referred to as attentional bias.
Surprisingly, we observed that most existing sequence models leverage either
(1) dot-product similarity, or (2) L2 regression objectives as their
attentional bias. Going beyond these objectives, we present a set of
alternative attentional bias configurations along with their effective
approximations to stabilize their training procedure. We then reinterpret
forgetting mechanisms in modern deep learning architectures as a form of
retention regularization, providing a novel set of forget gates for sequence
models. Building upon these insights, we present Miras, a general framework to
design deep learning architectures based on four choices of: (i) associative
memory architecture, (ii) attentional bias objective, (iii) retention gate, and
(iv) memory learning algorithm. We present three novel sequence models-Moneta,
Yaad, and Memora-that go beyond the power of existing linear RNNs while
maintaining a fast parallelizable training process. Our experiments show
different design choices in Miras yield models with varying strengths. For
example, certain instances of Miras achieve exceptional performance in special
tasks such as language modeling, commonsense reasoning, and recall intensive
tasks, even outperforming Transformers and other modern linear recurrent
models.
</summary>
    <author>
      <name>Ali Behrouz</name>
    </author>
    <author>
      <name>Meisam Razaviyayn</name>
    </author>
    <author>
      <name>Peilin Zhong</name>
    </author>
    <author>
      <name>Vahab Mirrokni</name>
    </author>
    <link href="http://arxiv.org/abs/2504.13173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.13173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
