<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-03-12T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">411338</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.08684v1</id>
    <updated>2025-03-11T17:59:00Z</updated>
    <published>2025-03-11T17:59:00Z</published>
    <title>Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents</title>
    <summary>  Previous studies have found that PLM-based retrieval models exhibit a
preference for LLM-generated content, assigning higher relevance scores to
these documents even when their semantic quality is comparable to human-written
ones. This phenomenon, known as source bias, threatens the sustainable
development of the information access ecosystem. However, the underlying causes
of source bias remain unexplored. In this paper, we explain the process of
information retrieval with a causal graph and discover that PLM-based
retrievers learn perplexity features for relevance estimation, causing source
bias by ranking the documents with low perplexity higher. Theoretical analysis
further reveals that the phenomenon stems from the positive correlation between
the gradients of the loss functions in language modeling task and retrieval
task. Based on the analysis, a causal-inspired inference-time debiasing method
is proposed, called Causal Diagnosis and Correction (CDC). CDC first diagnoses
the bias effect of the perplexity and then separates the bias effect from the
overall estimated relevance score. Experimental results across three domains
demonstrate the superior debiasing effectiveness of CDC, emphasizing the
validity of our proposed explanatory framework. Source codes are available at
https://github.com/WhyDwelledOnAi/Perplexity-Trap.
</summary>
    <author>
      <name>Haoyu Wang</name>
    </author>
    <author>
      <name>Sunhao Dai</name>
    </author>
    <author>
      <name>Haiyuan Zhao</name>
    </author>
    <author>
      <name>Liang Pang</name>
    </author>
    <author>
      <name>Xiao Zhang</name>
    </author>
    <author>
      <name>Gang Wang</name>
    </author>
    <author>
      <name>Zhenhua Dong</name>
    </author>
    <author>
      <name>Jun Xu</name>
    </author>
    <author>
      <name>Ji-Rong Wen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.08684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.08681v1</id>
    <updated>2025-03-11T17:57:44Z</updated>
    <published>2025-03-11T17:57:44Z</published>
    <title>Self-Taught Self-Correction for Small Language Models</title>
    <summary>  Although large language models (LLMs) have achieved remarkable performance
across various tasks, they remain prone to errors. A key challenge is enabling
them to self-correct. While prior research has relied on external tools or
large proprietary models, this work explores self-correction in small language
models (SLMs) through iterative fine-tuning using solely self-generated data.
We introduce the Self-Taught Self-Correction (STaSC) algorithm, which
incorporates multiple algorithmic design choices. Experimental results on a
question-answering task demonstrate that STaSC effectively learns
self-correction, leading to significant performance improvements. Our analysis
further provides insights into the mechanisms of self-correction and the impact
of different design choices on learning dynamics and overall performance. To
support future research, we release our user-friendly codebase and lightweight
models.
</summary>
    <author>
      <name>Viktor Moskvoretskii</name>
    </author>
    <author>
      <name>Chris Biemann</name>
    </author>
    <author>
      <name>Irina Nikishina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code is available at https://github.com/VityaVitalich/STASC</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.08681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.08679v1</id>
    <updated>2025-03-11T17:56:30Z</updated>
    <published>2025-03-11T17:56:30Z</published>
    <title>Chain-of-Thought Reasoning In The Wild Is Not Always Faithful</title>
    <summary>  Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art
AI capabilities. However, recent studies have shown that CoT reasoning is not
always faithful, i.e. CoT reasoning does not always reflect how models arrive
at conclusions. So far, most of these studies have focused on unfaithfulness in
unnatural contexts where an explicit bias has been introduced. In contrast, we
show that unfaithful CoT can occur on realistic prompts with no artificial
bias. Our results reveal concerning rates of several forms of unfaithful
reasoning in frontier models: Sonnet 3.7 (30.6%), DeepSeek R1 (15.8%) and
ChatGPT-4o (12.6%) all answer a high proportion of question pairs unfaithfully.
Specifically, we find that models rationalize their implicit biases in answers
to binary questions ("implicit post-hoc rationalization"). For example, when
separately presented with the questions "Is X bigger than Y?" and "Is Y bigger
than X?", models sometimes produce superficially coherent arguments to justify
answering Yes to both questions or No to both questions, despite such responses
being logically contradictory. We also investigate restoration errors (Dziri et
al., 2023), where models make and then silently correct errors in their
reasoning, and unfaithful shortcuts, where models use clearly illogical
reasoning to simplify solving problems in Putnam questions (a hard benchmark).
Our findings raise challenges for AI safety work that relies on monitoring CoT
to detect undesired behavior.
</summary>
    <author>
      <name>Iv√°n Arcuschin</name>
    </author>
    <author>
      <name>Jett Janiak</name>
    </author>
    <author>
      <name>Robert Krzyzanowski</name>
    </author>
    <author>
      <name>Senthooran Rajamanoharan</name>
    </author>
    <author>
      <name>Neel Nanda</name>
    </author>
    <author>
      <name>Arthur Conmy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the ICLR 2025 Workshop, 10 main paper pages, 38 appendix
  pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.08679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.08674v1</id>
    <updated>2025-03-11T17:54:29Z</updated>
    <published>2025-03-11T17:54:29Z</published>
    <title>Understanding and Mitigating Distribution Shifts For Machine Learning
  Force Fields</title>
    <summary>  Machine Learning Force Fields (MLFFs) are a promising alternative to
expensive ab initio quantum mechanical molecular simulations. Given the
diversity of chemical spaces that are of interest and the cost of generating
new data, it is important to understand how MLFFs generalize beyond their
training distributions. In order to characterize and better understand
distribution shifts in MLFFs, we conduct diagnostic experiments on chemical
datasets, revealing common shifts that pose significant challenges, even for
large foundation models trained on extensive data. Based on these observations,
we hypothesize that current supervised training methods inadequately regularize
MLFFs, resulting in overfitting and learning poor representations of
out-of-distribution systems. We then propose two new methods as initial steps
for mitigating distribution shifts for MLFFs. Our methods focus on test-time
refinement strategies that incur minimal computational cost and do not use
expensive ab initio reference labels. The first strategy, based on spectral
graph theory, modifies the edges of test graphs to align with graph structures
seen during training. Our second strategy improves representations for
out-of-distribution systems at test-time by taking gradient steps using an
auxiliary objective, such as a cheap physical prior. Our test-time refinement
strategies significantly reduce errors on out-of-distribution systems,
suggesting that MLFFs are capable of and can move towards modeling diverse
chemical spaces, but are not being effectively trained to do so. Our
experiments establish clear benchmarks for evaluating the generalization
capabilities of the next generation of MLFFs. Our code is available at
https://tkreiman.github.io/projects/mlff_distribution_shifts/.
</summary>
    <author>
      <name>Tobias Kreiman</name>
    </author>
    <author>
      <name>Aditi S. Krishnapriyan</name>
    </author>
    <link href="http://arxiv.org/abs/2503.08674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mtrl-sci" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.BM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.08665v1</id>
    <updated>2025-03-11T17:51:07Z</updated>
    <published>2025-03-11T17:51:07Z</published>
    <title>REGEN: Learning Compact Video Embedding with (Re-)Generative Decoder</title>
    <summary>  We present a novel perspective on learning video embedders for generative
modeling: rather than requiring an exact reproduction of an input video, an
effective embedder should focus on synthesizing visually plausible
reconstructions. This relaxed criterion enables substantial improvements in
compression ratios without compromising the quality of downstream generative
models. Specifically, we propose replacing the conventional encoder-decoder
video embedder with an encoder-generator framework that employs a diffusion
transformer (DiT) to synthesize missing details from a compact latent space.
Therein, we develop a dedicated latent conditioning module to condition the DiT
decoder on the encoded video latent embedding. Our experiments demonstrate that
our approach enables superior encoding-decoding performance compared to
state-of-the-art methods, particularly as the compression ratio increases. To
demonstrate the efficacy of our approach, we report results from our video
embedders achieving a temporal compression ratio of up to 32x (8x higher than
leading video embedders) and validate the robustness of this ultra-compact
latent space for text-to-video generation, providing a significant efficiency
boost in latent diffusion model training and inference.
</summary>
    <author>
      <name>Yitian Zhang</name>
    </author>
    <author>
      <name>Long Mai</name>
    </author>
    <author>
      <name>Aniruddha Mahapatra</name>
    </author>
    <author>
      <name>David Bourgin</name>
    </author>
    <author>
      <name>Yicong Hong</name>
    </author>
    <author>
      <name>Jonah Casebeer</name>
    </author>
    <author>
      <name>Feng Liu</name>
    </author>
    <author>
      <name>Yun Fu</name>
    </author>
    <link href="http://arxiv.org/abs/2503.08665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
