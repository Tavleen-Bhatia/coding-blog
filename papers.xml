<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-04-09T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">417633</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.06266v1</id>
    <updated>2025-04-08T17:59:59Z</updated>
    <published>2025-04-08T17:59:59Z</published>
    <title>Constraining the [CII] luminosity function from the power spectrum of
  line intensity maps at redshift 3.6</title>
    <summary>  Forthcoming measurements of the line-intensity-mapping power spectrum (PS)
are expected to set precious constraints on several quantities of astrophysical
and cosmological interest. Our study targets the [CII] luminosity function (LF)
at high redshift, which is still highly uncertain, in particular at the faint
end. As an example of future opportunities, we present forecasts for the Deep
Spectroscopic Survey (DSS) that will be conducted with the Fred Young
Submillimeter Telescope at $z \simeq 3.6$ and also make predictions for
eventual $10\times$ wider and/or $\sqrt{10}\times$ more sensitive surveys. The
halo-occupation properties of [CII] emitters in the MARIGOLD simulations
provide us with the motivation to abundance match two versions of the ALPINE LF
against the halo mass function. We employ the resulting luminosity-mass
relation within the halo model to predict the expected PS signal and its
uncertainty. Finally, we use Bayesian inference to analyse mock PS data and
forecast what constraints could be achieved on the first two moments of the LF
and on Schechter fits. Depending on the actual LF, the DSS will measure the
clustering and shot-noise amplitudes of the PS with a signal-to-noise ratio of
$\sim 3$ or higher. However, degeneracies with the bias parameter and
redshift-space distortions make it unfeasible to extract the first moment of
the LF. Even the widest and most sensitive survey we consider can only
constrain it with a $50\%$ uncertainty. By jointly fitting the PS and the LF,
we directly constrain Schechter-function parameters. We find that the
normalisation and the cutoff luminosity are precisely and accurately measured
while the faint-end slope remains highly uncertain (unless the true value
approaches $-2$). Overall, increasing the survey sensitivity at fixed sky
coverage yields greater improvements than covering a larger area at fixed
sensitivity.
</summary>
    <author>
      <name>Elena Marcuzzo</name>
    </author>
    <author>
      <name>Cristiano Porciani</name>
    </author>
    <author>
      <name>Emilio Romano-Díaz</name>
    </author>
    <author>
      <name>Prachi Khatri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 18 figures, 5 tables. Submitted to A&amp;A</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.06266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06265v1</id>
    <updated>2025-04-08T17:59:57Z</updated>
    <published>2025-04-08T17:59:57Z</published>
    <title>GOLLuM: Gaussian Process Optimized LLMs -- Reframing LLM Finetuning
  through Bayesian Optimization</title>
    <summary>  Large Language Models (LLMs) can encode complex relationships in their latent
spaces, yet harnessing them for optimization under uncertainty remains
challenging. We address this gap with a novel architecture that reframes LLM
finetuning as Gaussian process (GP) marginal likelihood optimization via deep
kernel methods. We introduce LLM-based deep kernels, jointly optimized with GPs
to preserve the benefits of both - LLMs to provide a rich and flexible input
space for Bayesian optimization and - GPs to model this space with predictive
uncertainty for more efficient sampling. Applied to Buchwald-Hartwig reaction
optimization, our method nearly doubles the discovery rate of high-performing
reactions compared to static LLM embeddings (from 24% to 43% coverage of the
top 5% reactions in just 50 optimization iterations). We also observe a 14%
improvement over domain-specific representations without requiring specialized
features. Extensive empirical evaluation across 19 benchmarks - ranging from
general chemistry to reaction and molecular property optimization -
demonstrates our method's robustness, generality, and consistent improvements
across: (1) tasks, (2) LLM architectures (encoder, decoder, encoder-decoder),
(3) pretraining domains (chemistry-related or general-purpose) and (4)
hyperparameter settings (tuned once on a single dataset). Finally, we explain
these improvements: joint LLM-GP optimization through marginal likelihood
implicitly performs contrastive learning, aligning representations to produce
(1) better-structured embedding spaces, (2) improved uncertainty calibration,
and (3) more efficient sampling - without requiring any external loss. This
work provides both practical advances in sample-efficient optimization and
insights into what makes effective Bayesian optimization.
</summary>
    <author>
      <name>Bojana Ranković</name>
    </author>
    <author>
      <name>Philippe Schwaller</name>
    </author>
    <link href="http://arxiv.org/abs/2504.06265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06261v2</id>
    <updated>2025-04-09T17:56:08Z</updated>
    <published>2025-04-08T17:59:41Z</published>
    <title>Hogwild! Inference: Parallel LLM Generation via Concurrent Attention</title>
    <summary>  Large Language Models (LLMs) have demonstrated the ability to tackle
increasingly complex tasks through advanced reasoning, long-form content
generation, and tool use. Solving these tasks often involves long
inference-time computations. In human problem solving, a common strategy to
expedite work is collaboration: by dividing the problem into sub-tasks,
exploring different strategies concurrently, etc. Recent research has shown
that LLMs can also operate in parallel by implementing explicit cooperation
frameworks, such as voting mechanisms or the explicit creation of independent
sub-tasks that can be executed in parallel. However, each of these frameworks
may not be suitable for all types of tasks, which can hinder their
applicability. In this work, we propose a different design approach: we run LLM
"workers" in parallel , allowing them to synchronize via a concurrently-updated
attention cache and prompt these workers to decide how best to collaborate. Our
approach allows the instances to come up with their own collaboration strategy
for the problem at hand, all the while "seeing" each other's partial progress
in the concurrent cache. We implement this approach via Hogwild! Inference: a
parallel LLM inference engine where multiple instances of the same LLM run in
parallel with the same attention cache, with "instant" access to each other's
generated tokens. Hogwild! inference takes advantage of Rotary Position
Embeddings (RoPE) to avoid recomputation while improving parallel hardware
utilization. We find that modern reasoning-capable LLMs can perform inference
with shared Key-Value cache out of the box, without additional fine-tuning.
</summary>
    <author>
      <name>Gleb Rodionov</name>
    </author>
    <author>
      <name>Roman Garipov</name>
    </author>
    <author>
      <name>Alina Shutova</name>
    </author>
    <author>
      <name>George Yakushev</name>
    </author>
    <author>
      <name>Vage Egiazarian</name>
    </author>
    <author>
      <name>Anton Sinitsin</name>
    </author>
    <author>
      <name>Denis Kuznedelev</name>
    </author>
    <author>
      <name>Dan Alistarh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint, work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.06261v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06261v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06257v1</id>
    <updated>2025-04-08T17:58:52Z</updated>
    <published>2025-04-08T17:58:52Z</published>
    <title>PainNet: Statistical Relation Network with Episode-Based Training for
  Pain Estimation</title>
    <summary>  Despite the span in estimating pain from facial expressions, limited works
have focused on estimating the sequence-level pain, which is reported by
patients and used commonly in clinics. In this paper, we introduce a novel
Statistical Relation Network, referred to as PainNet, designed for the
estimation of the sequence-level pain. PainNet employs two key modules, the
embedding and the relation modules, for comparing pairs of pain videos, and
producing relation scores indicating if each pair belongs to the same pain
category or not. At the core of the embedding module is a statistical layer
mounted on the top of a RNN for extracting compact video-level features. The
statistical layer is implemented as part of the deep architecture. Doing so,
allows combining multiple training stages used in previous research, into a
single end-to-end training stage. PainNet is trained using the episode-based
training scheme, which involves comparing a query video with a set of videos
representing the different pain categories. Experimental results show the
benefit of using the statistical layer and the episode-based training in the
proposed model. Furthermore, PainNet outperforms the state-of-the-art results
on self-reported pain estimation.
</summary>
    <author>
      <name>Mina Bishay</name>
    </author>
    <author>
      <name>Graham Page</name>
    </author>
    <author>
      <name>Mohammad Mavadati</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the ACII 2024 Workshops</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.06257v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06257v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06256v1</id>
    <updated>2025-04-08T17:58:47Z</updated>
    <published>2025-04-08T17:58:47Z</published>
    <title>Transfer between Modalities with MetaQueries</title>
    <summary>  Unified multimodal models aim to integrate understanding (text output) and
generation (pixel output), but aligning these different modalities within a
single architecture often demands complex training recipes and careful data
balancing. We introduce MetaQueries, a set of learnable queries that act as an
efficient interface between autoregressive multimodal LLMs (MLLMs) and
diffusion models. MetaQueries connects the MLLM's latents to the diffusion
decoder, enabling knowledge-augmented image generation by leveraging the MLLM's
deep understanding and reasoning capabilities. Our method simplifies training,
requiring only paired image-caption data and standard diffusion objectives.
Notably, this transfer is effective even when the MLLM backbone remains frozen,
thereby preserving its state-of-the-art multimodal understanding capabilities
while achieving strong generative performance. Additionally, our method is
flexible and can be easily instruction-tuned for advanced applications such as
image editing and subject-driven generation.
</summary>
    <author>
      <name>Xichen Pan</name>
    </author>
    <author>
      <name>Satya Narayan Shukla</name>
    </author>
    <author>
      <name>Aashu Singh</name>
    </author>
    <author>
      <name>Zhuokai Zhao</name>
    </author>
    <author>
      <name>Shlok Kumar Mishra</name>
    </author>
    <author>
      <name>Jialiang Wang</name>
    </author>
    <author>
      <name>Zhiyang Xu</name>
    </author>
    <author>
      <name>Jiuhai Chen</name>
    </author>
    <author>
      <name>Kunpeng Li</name>
    </author>
    <author>
      <name>Felix Juefei-Xu</name>
    </author>
    <author>
      <name>Ji Hou</name>
    </author>
    <author>
      <name>Saining Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://xichenpan.com/metaquery</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.06256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
