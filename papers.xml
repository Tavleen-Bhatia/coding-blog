<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-03-18T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">412748</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.13447v1</id>
    <updated>2025-03-17T17:59:54Z</updated>
    <published>2025-03-17T17:59:54Z</published>
    <title>MetaScale: Test-Time Scaling with Evolving Meta-Thoughts</title>
    <summary>  One critical challenge for large language models (LLMs) for making complex
reasoning is their reliance on matching reasoning patterns from training data,
instead of proactively selecting the most appropriate cognitive strategy to
solve a given task. Existing approaches impose fixed cognitive structures that
enhance performance in specific tasks but lack adaptability across diverse
scenarios. To address this limitation, we introduce METASCALE, a test-time
scaling framework based on meta-thoughts -- adaptive thinking strategies
tailored to each task. METASCALE initializes a pool of candidate meta-thoughts,
then iteratively selects and evaluates them using a multi-armed bandit
algorithm with upper confidence bound selection, guided by a reward model. To
further enhance adaptability, a genetic algorithm evolves high-reward
meta-thoughts, refining and extending the strategy pool over time. By
dynamically proposing and optimizing meta-thoughts at inference time, METASCALE
improves both accuracy and generalization across a wide range of tasks.
Experimental results demonstrate that MetaScale consistently outperforms
standard inference approaches, achieving an 11% performance gain in win rate on
Arena-Hard for GPT-4o, surpassing o1-mini by 0.9% under style control. Notably,
METASCALE scales more effectively with increasing sampling budgets and produces
more structured, expert-level responses.
</summary>
    <author>
      <name>Qin Liu</name>
    </author>
    <author>
      <name>Wenxuan Zhou</name>
    </author>
    <author>
      <name>Nan Xu</name>
    </author>
    <author>
      <name>James Y. Huang</name>
    </author>
    <author>
      <name>Fei Wang</name>
    </author>
    <author>
      <name>Sheng Zhang</name>
    </author>
    <author>
      <name>Hoifung Poon</name>
    </author>
    <author>
      <name>Muhao Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Work in progress</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.13447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.13447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.13441v1</id>
    <updated>2025-03-17T17:59:09Z</updated>
    <published>2025-03-17T17:59:09Z</published>
    <title>Humanoid Policy ~ Human Policy</title>
    <summary>  Training manipulation policies for humanoid robots with diverse data enhances
their robustness and generalization across tasks and platforms. However,
learning solely from robot demonstrations is labor-intensive, requiring
expensive tele-operated data collection which is difficult to scale. This paper
investigates a more scalable data source, egocentric human demonstrations, to
serve as cross-embodiment training data for robot learning. We mitigate the
embodiment gap between humanoids and humans from both the data and modeling
perspectives. We collect an egocentric task-oriented dataset (PH2D) that is
directly aligned with humanoid manipulation demonstrations. We then train a
human-humanoid behavior policy, which we term Human Action Transformer (HAT).
The state-action space of HAT is unified for both humans and humanoid robots
and can be differentiably retargeted to robot actions. Co-trained with
smaller-scale robot data, HAT directly models humanoid robots and humans as
different embodiments without additional supervision. We show that human data
improves both generalization and robustness of HAT with significantly better
data collection efficiency. Code and data: https://human-as-robot.github.io/
</summary>
    <author>
      <name>Ri-Zhao Qiu</name>
    </author>
    <author>
      <name>Shiqi Yang</name>
    </author>
    <author>
      <name>Xuxin Cheng</name>
    </author>
    <author>
      <name>Chaitanya Chawla</name>
    </author>
    <author>
      <name>Jialong Li</name>
    </author>
    <author>
      <name>Tairan He</name>
    </author>
    <author>
      <name>Ge Yan</name>
    </author>
    <author>
      <name>Lars Paulsen</name>
    </author>
    <author>
      <name>Ge Yang</name>
    </author>
    <author>
      <name>Sha Yi</name>
    </author>
    <author>
      <name>Guanya Shi</name>
    </author>
    <author>
      <name>Xiaolong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code and data: https://human-as-robot.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.13441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.13441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.13439v1</id>
    <updated>2025-03-17T17:59:01Z</updated>
    <published>2025-03-17T17:59:01Z</published>
    <title>Amodal3R: Amodal 3D Reconstruction from Occluded 2D Images</title>
    <summary>  Most image-based 3D object reconstructors assume that objects are fully
visible, ignoring occlusions that commonly occur in real-world scenarios. In
this paper, we introduce Amodal3R, a conditional 3D generative model designed
to reconstruct 3D objects from partial observations. We start from a
"foundation" 3D generative model and extend it to recover plausible 3D geometry
and appearance from occluded objects. We introduce a mask-weighted multi-head
cross-attention mechanism followed by an occlusion-aware attention layer that
explicitly leverages occlusion priors to guide the reconstruction process. We
demonstrate that, by training solely on synthetic data, Amodal3R learns to
recover full 3D objects even in the presence of occlusions in real scenes. It
substantially outperforms existing methods that independently perform 2D amodal
completion followed by 3D reconstruction, thereby establishing a new benchmark
for occlusion-aware 3D reconstruction.
</summary>
    <author>
      <name>Tianhao Wu</name>
    </author>
    <author>
      <name>Chuanxia Zheng</name>
    </author>
    <author>
      <name>Frank Guan</name>
    </author>
    <author>
      <name>Andrea Vedaldi</name>
    </author>
    <author>
      <name>Tat-Jen Cham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://sm0kywu.github.io/Amodal3R/</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.13439v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.13439v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.13438v1</id>
    <updated>2025-03-17T17:58:45Z</updated>
    <published>2025-03-17T17:58:45Z</published>
    <title>Deep Belief Markov Models for POMDP Inference</title>
    <summary>  This work introduces a novel deep learning-based architecture, termed the
Deep Belief Markov Model (DBMM), which provides efficient, model-formulation
agnostic inference in Partially Observable Markov Decision Process (POMDP)
problems. The POMDP framework allows for modeling and solving sequential
decision-making problems under observation uncertainty. In complex,
high-dimensional, partially observable environments, existing methods for
inference based on exact computations (e.g., via Bayes' theorem) or sampling
algorithms do not scale well. Furthermore, ground truth states may not be
available for learning the exact transition dynamics. DBMMs extend deep Markov
models into the partially observable decision-making framework and allow
efficient belief inference entirely based on available observation data via
variational inference methods. By leveraging the potency of neural networks,
DBMMs can infer and simulate non-linear relationships in the system dynamics
and naturally scale to problems with high dimensionality and discrete or
continuous variables. In addition, neural network parameters can be dynamically
updated efficiently based on data availability. DBMMs can thus be used to infer
a belief variable, thus enabling the derivation of POMDP solutions over the
belief space. We evaluate the efficacy of the proposed methodology by
evaluating the capability of model-formulation agnostic inference of DBMMs in
benchmark problems that include discrete and continuous variables.
</summary>
    <author>
      <name>Giacomo Arcieri</name>
    </author>
    <author>
      <name>Konstantinos G. Papakonstantinou</name>
    </author>
    <author>
      <name>Daniel Straub</name>
    </author>
    <author>
      <name>Eleni Chatzi</name>
    </author>
    <link href="http://arxiv.org/abs/2503.13438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.13438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.13436v1</id>
    <updated>2025-03-17T17:58:30Z</updated>
    <published>2025-03-17T17:58:30Z</published>
    <title>Unified Autoregressive Visual Generation and Understanding with
  Continuous Tokens</title>
    <summary>  We present UniFluid, a unified autoregressive framework for joint visual
generation and understanding leveraging continuous visual tokens. Our unified
autoregressive architecture processes multimodal image and text inputs,
generating discrete tokens for text and continuous tokens for image. We find
though there is an inherent trade-off between the image generation and
understanding task, a carefully tuned training recipe enables them to improve
each other. By selecting an appropriate loss balance weight, the unified model
achieves results comparable to or exceeding those of single-task baselines on
both tasks. Furthermore, we demonstrate that employing stronger pre-trained
LLMs and random-order generation during training is important to achieve
high-fidelity image generation within this unified framework. Built upon the
Gemma model series, UniFluid exhibits competitive performance across both image
generation and understanding, demonstrating strong transferability to various
downstream tasks, including image editing for generation, as well as visual
captioning and question answering for understanding.
</summary>
    <author>
      <name>Lijie Fan</name>
    </author>
    <author>
      <name>Luming Tang</name>
    </author>
    <author>
      <name>Siyang Qin</name>
    </author>
    <author>
      <name>Tianhong Li</name>
    </author>
    <author>
      <name>Xuan Yang</name>
    </author>
    <author>
      <name>Siyuan Qiao</name>
    </author>
    <author>
      <name>Andreas Steiner</name>
    </author>
    <author>
      <name>Chen Sun</name>
    </author>
    <author>
      <name>Yuanzhen Li</name>
    </author>
    <author>
      <name>Tao Zhu</name>
    </author>
    <author>
      <name>Michael Rubinstein</name>
    </author>
    <author>
      <name>Michalis Raptis</name>
    </author>
    <author>
      <name>Deqing Sun</name>
    </author>
    <author>
      <name>Radu Soricut</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Tech report</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.13436v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.13436v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
