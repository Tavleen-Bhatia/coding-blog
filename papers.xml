<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-03-19T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">413068</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2503.14505v1</id>
    <updated>2025-03-18T17:59:58Z</updated>
    <published>2025-03-18T17:59:58Z</published>
    <title>MusicInfuser: Making Video Diffusion Listen and Dance</title>
    <summary>  We introduce MusicInfuser, an approach for generating high-quality dance
videos that are synchronized to a specified music track. Rather than attempting
to design and train a new multimodal audio-video model, we show how existing
video diffusion models can be adapted to align with musical inputs by
introducing lightweight music-video cross-attention and a low-rank adapter.
Unlike prior work requiring motion capture data, our approach fine-tunes only
on dance videos. MusicInfuser achieves high-quality music-driven video
generation while preserving the flexibility and generative capabilities of the
underlying models. We introduce an evaluation framework using Video-LLMs to
assess multiple dimensions of dance generation quality. The project page and
code are available at https://susunghong.github.io/MusicInfuser.
</summary>
    <author>
      <name>Susung Hong</name>
    </author>
    <author>
      <name>Ira Kemelmacher-Shlizerman</name>
    </author>
    <author>
      <name>Brian Curless</name>
    </author>
    <author>
      <name>Steven M. Seitz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project page: https://susunghong.github.io/MusicInfuser</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.14505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.14505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.14503v1</id>
    <updated>2025-03-18T17:59:54Z</updated>
    <published>2025-03-18T17:59:54Z</published>
    <title>The Power of Context: How Multimodality Improves Image Super-Resolution</title>
    <summary>  Single-image super-resolution (SISR) remains challenging due to the inherent
difficulty of recovering fine-grained details and preserving perceptual quality
from low-resolution inputs. Existing methods often rely on limited image
priors, leading to suboptimal results. We propose a novel approach that
leverages the rich contextual information available in multiple modalities --
including depth, segmentation, edges, and text prompts -- to learn a powerful
generative prior for SISR within a diffusion model framework. We introduce a
flexible network architecture that effectively fuses multimodal information,
accommodating an arbitrary number of input modalities without requiring
significant modifications to the diffusion process. Crucially, we mitigate
hallucinations, often introduced by text prompts, by using spatial information
from other modalities to guide regional text-based conditioning. Each
modality's guidance strength can also be controlled independently, allowing
steering outputs toward different directions, such as increasing bokeh through
depth or adjusting object prominence via segmentation. Extensive experiments
demonstrate that our model surpasses state-of-the-art generative SISR methods,
achieving superior visual quality and fidelity. See project page at
https://mmsr.kfmei.com/.
</summary>
    <author>
      <name>Kangfu Mei</name>
    </author>
    <author>
      <name>Hossein Talebi</name>
    </author>
    <author>
      <name>Mojtaba Ardakani</name>
    </author>
    <author>
      <name>Vishal M. Patel</name>
    </author>
    <author>
      <name>Peyman Milanfar</name>
    </author>
    <author>
      <name>Mauricio Delbracio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by CVPR2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.14503v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.14503v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.14500v1</id>
    <updated>2025-03-18T17:59:41Z</updated>
    <published>2025-03-18T17:59:41Z</published>
    <title>Utilization of Neighbor Information for Image Classification with
  Different Levels of Supervision</title>
    <summary>  We propose to bridge the gap between semi-supervised and unsupervised image
recognition with a flexible method that performs well for both generalized
category discovery (GCD) and image clustering. Despite the overlap in
motivation between these tasks, the methods themselves are restricted to a
single task -- GCD methods are reliant on the labeled portion of the data, and
deep image clustering methods have no built-in way to leverage the labels
efficiently. We connect the two regimes with an innovative approach that
Utilizes Neighbor Information for Classification (UNIC) both in the
unsupervised (clustering) and semisupervised (GCD) setting. State-of-the-art
clustering methods already rely heavily on nearest neighbors. We improve on
their results substantially in two parts, first with a sampling and cleaning
strategy where we identify accurate positive and negative neighbors, and
secondly by finetuning the backbone with clustering losses computed by sampling
both types of neighbors. We then adapt this pipeline to GCD by utilizing the
labelled images as ground truth neighbors. Our method yields state-of-the-art
results for both clustering (+3% ImageNet-100, Imagenet200) and GCD (+0.8%
ImageNet-100, +5% CUB, +2% SCars, +4% Aircraft).
</summary>
    <author>
      <name>Gihan Jayatilaka</name>
    </author>
    <author>
      <name>Abhinav Shrivastava</name>
    </author>
    <author>
      <name>Matthew Gwilliam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 16 figures, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.14500v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.14500v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.14499v1</id>
    <updated>2025-03-18T17:59:31Z</updated>
    <published>2025-03-18T17:59:31Z</published>
    <title>Measuring AI Ability to Complete Long Tasks</title>
    <summary>  Despite rapid progress on AI benchmarks, the real-world meaning of benchmark
performance remains unclear. To quantify the capabilities of AI systems in
terms of human capabilities, we propose a new metric: 50%-task-completion time
horizon. This is the time humans typically take to complete tasks that AI
models can complete with 50% success rate. We first timed humans with relevant
domain expertise on a combination of RE-Bench, HCAST, and 66 novel shorter
tasks. On these tasks, current frontier AI models such as Claude 3.7 Sonnet
have a 50% time horizon of around 50 minutes. Furthermore, frontier AI time
horizon has been doubling approximately every seven months since 2019, though
the trend may have accelerated in 2024. The increase in AI models' time
horizons seems to be primarily driven by greater reliability and ability to
adapt to mistakes, combined with better logical reasoning and tool use
capabilities. We discuss the limitations of our results -- including their
degree of external validity -- and the implications of increased autonomy for
dangerous capabilities. If these results generalize to real-world software
tasks, extrapolation of this trend predicts that within 5 years, AI systems
will be capable of automating many software tasks that currently take humans a
month.
</summary>
    <author>
      <name>Thomas Kwa</name>
    </author>
    <author>
      <name>Ben West</name>
    </author>
    <author>
      <name>Joel Becker</name>
    </author>
    <author>
      <name>Amy Deng</name>
    </author>
    <author>
      <name>Katharyn Garcia</name>
    </author>
    <author>
      <name>Max Hasin</name>
    </author>
    <author>
      <name>Sami Jawhar</name>
    </author>
    <author>
      <name>Megan Kinniment</name>
    </author>
    <author>
      <name>Nate Rush</name>
    </author>
    <author>
      <name>Sydney Von Arx</name>
    </author>
    <author>
      <name>Ryan Bloom</name>
    </author>
    <author>
      <name>Thomas Broadley</name>
    </author>
    <author>
      <name>Haoxing Du</name>
    </author>
    <author>
      <name>Brian Goodrich</name>
    </author>
    <author>
      <name>Nikola Jurkovic</name>
    </author>
    <author>
      <name>Luke Harold Miles</name>
    </author>
    <author>
      <name>Seraphina Nix</name>
    </author>
    <author>
      <name>Tao Lin</name>
    </author>
    <author>
      <name>Neev Parikh</name>
    </author>
    <author>
      <name>David Rein</name>
    </author>
    <author>
      <name>Lucas Jun Koba Sato</name>
    </author>
    <author>
      <name>Hjalmar Wijk</name>
    </author>
    <author>
      <name>Daniel M. Ziegler</name>
    </author>
    <author>
      <name>Elizabeth Barnes</name>
    </author>
    <author>
      <name>Lawrence Chan</name>
    </author>
    <link href="http://arxiv.org/abs/2503.14499v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.14499v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.14495v1</id>
    <updated>2025-03-18T17:58:28Z</updated>
    <published>2025-03-18T17:58:28Z</published>
    <title>Temporal Consistency for LLM Reasoning Process Error Identification</title>
    <summary>  Verification is crucial for effective mathematical reasoning. We present a
new temporal consistency method where verifiers iteratively refine their
judgments based on the previous assessment. Unlike one-round verification or
multi-model debate approaches, our method leverages consistency in a sequence
of self-reflection actions to improve verification accuracy. Empirical
evaluations across diverse mathematical process error identification benchmarks
(Mathcheck, ProcessBench, and PRM800K) show consistent performance improvements
over baseline methods. When applied to the recent DeepSeek R1 distilled models,
our method demonstrates strong performance, enabling 7B/8B distilled models to
outperform all 70B/72B models and GPT-4o on ProcessBench. Notably, the
distilled 14B model with our method achieves performance comparable to
Deepseek-R1. Our codes are available at
https://github.com/jcguo123/Temporal-Consistency
</summary>
    <author>
      <name>Jiacheng Guo</name>
    </author>
    <author>
      <name>Yue Wu</name>
    </author>
    <author>
      <name>Jiahao Qiu</name>
    </author>
    <author>
      <name>Kaixuan Huang</name>
    </author>
    <author>
      <name>Xinzhe Juan</name>
    </author>
    <author>
      <name>Ling Yang</name>
    </author>
    <author>
      <name>Mengdi Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2503.14495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.14495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
