<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-04-14T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">418296</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.08736v1</id>
    <updated>2025-04-11T17:59:58Z</updated>
    <published>2025-04-11T17:59:58Z</published>
    <title>GigaTok: Scaling Visual Tokenizers to 3 Billion Parameters for
  Autoregressive Image Generation</title>
    <summary>  In autoregressive (AR) image generation, visual tokenizers compress images
into compact discrete latent tokens, enabling efficient training of downstream
autoregressive models for visual generation via next-token prediction. While
scaling visual tokenizers improves image reconstruction quality, it often
degrades downstream generation quality -- a challenge not adequately addressed
in existing literature. To address this, we introduce GigaTok, the first
approach to simultaneously improve image reconstruction, generation, and
representation learning when scaling visual tokenizers. We identify the growing
complexity of latent space as the key factor behind the reconstruction vs.
generation dilemma. To mitigate this, we propose semantic regularization, which
aligns tokenizer features with semantically consistent features from a
pre-trained visual encoder. This constraint prevents excessive latent space
complexity during scaling, yielding consistent improvements in both
reconstruction and downstream autoregressive generation. Building on semantic
regularization, we explore three key practices for scaling tokenizers:(1) using
1D tokenizers for better scalability, (2) prioritizing decoder scaling when
expanding both encoder and decoder, and (3) employing entropy loss to stabilize
training for billion-scale tokenizers. By scaling to $\bf{3 \space billion}$
parameters, GigaTok achieves state-of-the-art performance in reconstruction,
downstream AR generation, and downstream AR representation quality.
</summary>
    <author>
      <name>Tianwei Xiong</name>
    </author>
    <author>
      <name>Jun Hao Liew</name>
    </author>
    <author>
      <name>Zilong Huang</name>
    </author>
    <author>
      <name>Jiashi Feng</name>
    </author>
    <author>
      <name>Xihui Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">project page: https://silentview.github.io/GigaTok</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.08736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.08736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.08732v1</id>
    <updated>2025-04-11T17:57:35Z</updated>
    <published>2025-04-11T17:57:35Z</published>
    <title>Quantum Large Language Model Fine-Tuning</title>
    <summary>  We introduce a hybrid quantum-classical deep learning architecture for large
language model fine-tuning. The classical portion of the architecture is a
sentence transformer that is powerful enough to display significant accuracy
for complex tasks such as sentiment prediction. The quantum portion of the
architecture consists of parameterized quantum circuits that utilize long-range
connections between qubits.
  We analyze the performance of the hybrid models for various settings of
hyperparameters, including the number of qubits, the depth of the quantum
circuits, learning rate, number of re-uploading steps, etc. Based on a
screening study of main effects, we show an overall improvement in prediction
accuracy over a comparable classical baseline, with a trend of increasing
accuracy with number of qubits. We observe up to $3.14\%$ improvements in
accuracy over classical architectures of comparable model size, within the set
of hyperparameters probed in this study.
  We demonstrate the contribution of each module in our architecture through
ablation studies. Our studies are based on finite shot-counts and include
simulations based on noisy quantum gates.
</summary>
    <author>
      <name>Sang Hyub Kim</name>
    </author>
    <author>
      <name>Jonathan Mei</name>
    </author>
    <author>
      <name>Claudio Girotto</name>
    </author>
    <author>
      <name>Masako Yamada</name>
    </author>
    <author>
      <name>Martin Roetteler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 11 figures, 15 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.08732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.08732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.08730v1</id>
    <updated>2025-04-11T17:56:52Z</updated>
    <published>2025-04-11T17:56:52Z</published>
    <title>Dimension reduction for derivative-informed operator learning: An
  analysis of approximation errors</title>
    <summary>  We study the derivative-informed learning of nonlinear operators between
infinite-dimensional separable Hilbert spaces by neural networks. Such
operators can arise from the solution of partial differential equations (PDEs),
and are used in many simulation-based outer-loop tasks in science and
engineering, such as PDE-constrained optimization, Bayesian inverse problems,
and optimal experimental design. In these settings, the neural network
approximations can be used as surrogate models to accelerate the solution of
the outer-loop tasks. However, since outer-loop tasks in infinite dimensions
often require knowledge of the underlying geometry, the approximation accuracy
of the operator's derivatives can also significantly impact the performance of
the surrogate model. Motivated by this, we analyze the approximation errors of
neural operators in Sobolev norms over infinite-dimensional Gaussian input
measures. We focus on the reduced basis neural operator (RBNO), which uses
linear encoders and decoders defined on dominant input/output subspaces spanned
by reduced sets of orthonormal bases. To this end, we study two methods for
generating the bases; principal component analysis (PCA) and
derivative-informed subspaces (DIS), which use the dominant eigenvectors of the
covariance of the data or the derivatives as the reduced bases, respectively.
We then derive bounds for errors arising from both the dimension reduction and
the latent neural network approximation, including the sampling errors
associated with the empirical estimation of the PCA/DIS. Our analysis is
validated on numerical experiments with elliptic PDEs, where our results show
that bases informed by the map (i.e., DIS or output PCA) yield accurate
reconstructions and generalization errors for both the operator and its
derivatives, while input PCA may underperform unless ranks and training sample
sizes are sufficiently large.
</summary>
    <author>
      <name>Dingcheng Luo</name>
    </author>
    <author>
      <name>Thomas O'Leary-Roseberry</name>
    </author>
    <author>
      <name>Peng Chen</name>
    </author>
    <author>
      <name>Omar Ghattas</name>
    </author>
    <link href="http://arxiv.org/abs/2504.08730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.08730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.08729v1</id>
    <updated>2025-04-11T17:56:09Z</updated>
    <published>2025-04-11T17:56:09Z</published>
    <title>Steering CLIP's vision transformer with sparse autoencoders</title>
    <summary>  While vision models are highly capable, their internal mechanisms remain
poorly understood -- a challenge which sparse autoencoders (SAEs) have helped
address in language, but which remains underexplored in vision. We address this
gap by training SAEs on CLIP's vision transformer and uncover key differences
between vision and language processing, including distinct sparsity patterns
for SAEs trained across layers and token types. We then provide the first
systematic analysis on the steerability of CLIP's vision transformer by
introducing metrics to quantify how precisely SAE features can be steered to
affect the model's output. We find that 10-15\% of neurons and features are
steerable, with SAEs providing thousands more steerable features than the base
model. Through targeted suppression of SAE features, we then demonstrate
improved performance on three vision disentanglement tasks (CelebA, Waterbirds,
and typographic attacks), finding optimal disentanglement in middle model
layers, and achieving state-of-the-art performance on defense against
typographic attacks.
</summary>
    <author>
      <name>Sonia Joseph</name>
    </author>
    <author>
      <name>Praneet Suresh</name>
    </author>
    <author>
      <name>Ethan Goldfarb</name>
    </author>
    <author>
      <name>Lorenz Hufe</name>
    </author>
    <author>
      <name>Yossi Gandelsman</name>
    </author>
    <author>
      <name>Robert Graham</name>
    </author>
    <author>
      <name>Danilo Bzdok</name>
    </author>
    <author>
      <name>Wojciech Samek</name>
    </author>
    <author>
      <name>Blake Aaron Richards</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures. Accepted to the CVPR 2025 Workshop on Mechanistic
  Interpretability for Vision (MIV)</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.08729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.08729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.08728v1</id>
    <updated>2025-04-11T17:55:58Z</updated>
    <published>2025-04-11T17:55:58Z</published>
    <title>End-to-End Demonstration of Quantum Generative Adversarial Networks for
  Steel Microstructure Image Augmentation on a Trapped-Ion Quantum Computer</title>
    <summary>  Generative adversarial networks (GANs) are a machine learning technique
capable of producing high-quality synthetic images. In the field of materials
science, when a crystallographic dataset includes inadequate or
difficult-to-obtain images, synthetic images can be used for image augmentation
to mitigate data scarcity and streamline the preparation of datasets for
high-throughput analysis. We integrate quantum computing with GANs into a
hybrid quantum-classical GAN to generate complex 5-channel electron backscatter
diffraction (EBSD) images of two distinct microstructure phases of steel. By
training a quantum circuit at the input layer of a large classical Wasserstein
GAN (WGAN) model, we mitigate mode collapse and achieve higher image quality
compared to a baseline classical GAN. We generate images from both ferrite and
bainite microstructure phases in an end-to-end workflow. With respect to
maximum mean discrepancy score, we find that the hybrid quantum-classical WGAN
improves over classical Bernoulli GANs in 70% of samples. As the quantum
computer is part of the training procedure, our method has potential to scale
to larger number of qubits. Our results indicate that the WGAN model based on
the quantum circuit ansatz may be effectively leveraged to enhance the quality
of synthetic EBSD images on both quantum simulators and actual quantum
hardware.
</summary>
    <author>
      <name>Samwel Sekwao</name>
    </author>
    <author>
      <name>Jason Iaconis</name>
    </author>
    <author>
      <name>Claudio Girotto</name>
    </author>
    <author>
      <name>Martin Roetteler</name>
    </author>
    <author>
      <name>Minwoo Kang</name>
    </author>
    <author>
      <name>Donghwi Kim</name>
    </author>
    <author>
      <name>Seunghyo Noh</name>
    </author>
    <author>
      <name>Woomin Kyoung</name>
    </author>
    <author>
      <name>Kyujin Shin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.08728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.08728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
