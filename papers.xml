<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-05-03T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">422206</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2505.00703v1</id>
    <updated>2025-05-01T17:59:46Z</updated>
    <published>2025-05-01T17:59:46Z</published>
    <title>T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level
  and Token-level CoT</title>
    <summary>  Recent advancements in large language models have demonstrated how
chain-of-thought (CoT) and reinforcement learning (RL) can improve performance.
However, applying such reasoning strategies to the visual generation domain
remains largely unexplored. In this paper, we present T2I-R1, a novel
reasoning-enhanced text-to-image generation model, powered by RL with a
bi-level CoT reasoning process. Specifically, we identify two levels of CoT
that can be utilized to enhance different stages of generation: (1) the
semantic-level CoT for high-level planning of the prompt and (2) the
token-level CoT for low-level pixel processing during patch-by-patch
generation. To better coordinate these two levels of CoT, we introduce
BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes
both generation CoTs within the same training step. By applying our reasoning
strategies to the baseline model, Janus-Pro, we achieve superior performance
with 13% improvement on T2I-CompBench and 19% improvement on the WISE
benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available
at: https://github.com/CaraJ7/T2I-R1
</summary>
    <author>
      <name>Dongzhi Jiang</name>
    </author>
    <author>
      <name>Ziyu Guo</name>
    </author>
    <author>
      <name>Renrui Zhang</name>
    </author>
    <author>
      <name>Zhuofan Zong</name>
    </author>
    <author>
      <name>Hao Li</name>
    </author>
    <author>
      <name>Le Zhuo</name>
    </author>
    <author>
      <name>Shilin Yan</name>
    </author>
    <author>
      <name>Pheng-Ann Heng</name>
    </author>
    <author>
      <name>Hongsheng Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://github.com/CaraJ7/T2I-R1</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.00703v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00703v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.00693v1</id>
    <updated>2025-05-01T17:55:05Z</updated>
    <published>2025-05-01T17:55:05Z</published>
    <title>Robotic Visual Instruction</title>
    <summary>  Recently, natural language has been the primary medium for human-robot
interaction. However, its inherent lack of spatial precision for robotic
control introduces challenges such as ambiguity and verbosity. To address these
limitations, we introduce the Robotic Visual Instruction (RoVI), a novel
paradigm to guide robotic tasks through an object-centric, hand-drawn symbolic
representation. RoVI effectively encodes spatial-temporal information into
human-interpretable visual instructions through 2D sketches, utilizing arrows,
circles, colors, and numbers to direct 3D robotic manipulation. To enable
robots to understand RoVI better and generate precise actions based on RoVI, we
present Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for
RoVI-conditioned policies. This approach leverages Vision-Language Models
(VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from
2D pixel space via keypoint extraction, and then transform them into executable
3D action sequences. We additionally curate a specialized dataset of 15K
instances to fine-tune small VLMs for edge deployment, enabling them to
effectively learn RoVI capabilities. Our approach is rigorously validated
across 11 novel tasks in both real and simulated environments, demonstrating
significant generalization capability. Notably, VIEW achieves an 87.5% success
rate in real-world scenarios involving unseen tasks that feature multi-step
actions, with disturbances, and trajectory-following requirements. Code and
Datasets in this paper will be released soon.
</summary>
    <author>
      <name>Yanbang Li</name>
    </author>
    <author>
      <name>Ziyang Gong</name>
    </author>
    <author>
      <name>Haoyang Li</name>
    </author>
    <author>
      <name>Haoyang Li</name>
    </author>
    <author>
      <name>Xiaoqi Huang</name>
    </author>
    <author>
      <name>Haolan Kang</name>
    </author>
    <author>
      <name>Guangping Bai</name>
    </author>
    <author>
      <name>Xianzheng Ma</name>
    </author>
    <link href="http://arxiv.org/abs/2505.00693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.00690v1</id>
    <updated>2025-05-01T17:52:29Z</updated>
    <published>2025-05-01T17:52:29Z</published>
    <title>Towards Autonomous Micromobility through Scalable Urban Simulation</title>
    <summary>  Micromobility, which utilizes lightweight mobile machines moving in urban
public spaces, such as delivery robots and mobility scooters, emerges as a
promising alternative to vehicular mobility. Current micromobility depends
mostly on human manual operation (in-person or remote control), which raises
safety and efficiency concerns when navigating busy urban environments full of
unpredictable obstacles and pedestrians. Assisting humans with AI agents in
maneuvering micromobility devices presents a viable solution for enhancing
safety and efficiency. In this work, we present a scalable urban simulation
solution to advance autonomous micromobility. First, we build URBAN-SIM - a
high-performance robot learning platform for large-scale training of embodied
agents in interactive urban scenes. URBAN-SIM contains three critical modules:
Hierarchical Urban Generation pipeline, Interactive Dynamics Generation
strategy, and Asynchronous Scene Sampling scheme, to improve the diversity,
realism, and efficiency of robot learning in simulation. Then, we propose
URBAN-BENCH - a suite of essential tasks and benchmarks to gauge various
capabilities of the AI agents in achieving autonomous micromobility.
URBAN-BENCH includes eight tasks based on three core skills of the agents:
Urban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robots
with heterogeneous embodiments, such as the wheeled and legged robots, across
these tasks. Experiments on diverse terrains and urban structures reveal each
robot's strengths and limitations.
</summary>
    <author>
      <name>Wayne Wu</name>
    </author>
    <author>
      <name>Honglin He</name>
    </author>
    <author>
      <name>Chaoyuan Zhang</name>
    </author>
    <author>
      <name>Jack He</name>
    </author>
    <author>
      <name>Seth Z. Zhao</name>
    </author>
    <author>
      <name>Ran Gong</name>
    </author>
    <author>
      <name>Quanyi Li</name>
    </author>
    <author>
      <name>Bolei Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CVPR 2025 Highlight. Project page:
  https://metadriverse.github.io/urban-sim/</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.00690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.00685v1</id>
    <updated>2025-05-01T17:47:44Z</updated>
    <published>2025-05-01T17:47:44Z</published>
    <title>On the Importance of Gaussianizing Representations</title>
    <summary>  The normal distribution plays a central role in information theory - it is at
the same time the best-case signal and worst-case noise distribution, has the
greatest representational capacity of any distribution, and offers an
equivalence between uncorrelatedness and independence for joint distributions.
Accounting for the mean and variance of activations throughout the layers of
deep neural networks has had a significant effect on facilitating their
effective training, but seldom has a prescription for precisely what
distribution these activations should take, and how this might be achieved,
been offered. Motivated by the information-theoretic properties of the normal
distribution, we address this question and concurrently present normality
normalization: a novel normalization layer which encourages normality in the
feature representations of neural networks using the power transform and
employs additive Gaussian noise during training. Our experiments
comprehensively demonstrate the effectiveness of normality normalization, in
regards to its generalization performance on an array of widely used model and
dataset combinations, its strong performance across various common factors of
variation such as model width, depth, and training minibatch size, its
suitability for usage wherever existing normalization layers are conventionally
used, and as a means to improving model robustness to random perturbations.
</summary>
    <author>
      <name>Daniel Eftekhari</name>
    </author>
    <author>
      <name>Vardan Papyan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2025 Proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.00685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.00684v1</id>
    <updated>2025-05-01T17:45:59Z</updated>
    <published>2025-05-01T17:45:59Z</published>
    <title>Visual Test-time Scaling for GUI Agent Grounding</title>
    <summary>  We introduce RegionFocus, a visual test-time scaling approach for Vision
Language Model Agents. Understanding webpages is challenging due to the visual
complexity of GUI images and the large number of interface elements, making
accurate action selection difficult. Our approach dynamically zooms in on
relevant regions, reducing background clutter and improving grounding accuracy.
To support this process, we propose an image-as-map mechanism that visualizes
key landmarks at each step, providing a transparent action record and enables
the agent to effectively choose among action candidates. Even with a simple
region selection strategy, we observe significant performance gains of 28+\% on
Screenspot-pro and 24+\% on WebVoyager benchmarks on top of two
state-of-the-art open vision language model agents, UI-TARS and Qwen2.5-VL,
highlighting the effectiveness of visual test-time scaling in interactive
settings. We achieve a new state-of-the-art grounding performance of 61.6\% on
the ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model.
Our code will be released publicly at https://github.com/tiangeluo/RegionFocus.
</summary>
    <author>
      <name>Tiange Luo</name>
    </author>
    <author>
      <name>Lajanugen Logeswaran</name>
    </author>
    <author>
      <name>Justin Johnson</name>
    </author>
    <author>
      <name>Honglak Lee</name>
    </author>
    <link href="http://arxiv.org/abs/2505.00684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
