<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-02-12T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">404174</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2502.07784v1</id>
    <updated>2025-02-11T18:59:59Z</updated>
    <published>2025-02-11T18:59:59Z</published>
    <title>MatSwap: Light-aware material transfers in images</title>
    <summary>  We present MatSwap, a method to transfer materials to designated surfaces in
an image photorealistically. Such a task is non-trivial due to the large
entanglement of material appearance, geometry, and lighting in a photograph. In
the literature, material editing methods typically rely on either cumbersome
text engineering or extensive manual annotations requiring artist knowledge and
3D scene properties that are impractical to obtain. In contrast, we propose to
directly learn the relationship between the input material -- as observed on a
flat surface -- and its appearance within the scene, without the need for
explicit UV mapping. To achieve this, we rely on a custom light- and
geometry-aware diffusion model. We fine-tune a large-scale pre-trained
text-to-image model for material transfer using our synthetic dataset,
preserving its strong priors to ensure effective generalization to real images.
As a result, our method seamlessly integrates a desired material into the
target location in the photograph while retaining the identity of the scene. We
evaluate our method on synthetic and real images and show that it compares
favorably to recent work both qualitatively and quantitatively. We will release
our code and data upon publication.
</summary>
    <author>
      <name>Ivan Lopes</name>
    </author>
    <author>
      <name>Valentin Deschaintre</name>
    </author>
    <author>
      <name>Yannick Hold-Geoffroy</name>
    </author>
    <author>
      <name>Raoul de Charette</name>
    </author>
    <link href="http://arxiv.org/abs/2502.07784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.07784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.07783v1</id>
    <updated>2025-02-11T18:59:57Z</updated>
    <published>2025-02-11T18:59:57Z</published>
    <title>Curvature Tuning: Provable Training-free Model Steering From a Single
  Parameter</title>
    <summary>  The scaling of model size and data size has reshaped the paradigm of AI. As a
result, the common protocol to leverage the latest models is to steer them
towards a specific downstream task of interest through {\em fine-tuning}.
Despite its importance, the main methods for fine-tuning remain limited to full
or low-rank adapters--containing countless hyper-parameters and lacking
interpretability. In this paper, we take a step back and demonstrate how novel
and explainable post-training steering solutions can be derived theoretically
from {\em spline operators}, a rich mathematical framing of Deep Networks that
was recently developed. Our method--coined \textbf{Curvature Tuning (CT)}--has
a single parameter that provably modulates the curvature of the model's
decision boundary henceforth allowing training-free steering. This makes CT
both more efficient and interpretable than conventional fine-tuning methods. We
empirically validate its effectiveness in improving generalization and
robustness of pretrained models. For example, CT improves out-of-distribution
transfer performances of ResNet-18/50 by 2.57\%/1.74\% across seventeen
downstream datasets, and improves RobustBench robust accuracy by
11.76\%/348.44\%. Additionally, we apply CT to ReLU-based Swin-T/S, improving
their generalization on nine downstream datasets by 2.43\%/3.33\%. Our code is
available at
\href{https://github.com/Leon-Leyang/curvature-tuning}{https://github.com/Leon-Leyang/curvature-tuning}.
</summary>
    <author>
      <name>Leyang Hu</name>
    </author>
    <author>
      <name>Randall Balestriero</name>
    </author>
    <link href="http://arxiv.org/abs/2502.07783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.07783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.07782v1</id>
    <updated>2025-02-11T18:59:52Z</updated>
    <published>2025-02-11T18:59:52Z</published>
    <title>A Flag Decomposition for Hierarchical Datasets</title>
    <summary>  Flag manifolds encode hierarchical nested sequences of subspaces and serve as
powerful structures for various computer vision and machine learning
applications. Despite their utility in tasks such as dimensionality reduction,
motion averaging, and subspace clustering, current applications are often
restricted to extracting flags using common matrix decomposition methods like
the singular value decomposition. Here, we address the need for a general
algorithm to factorize and work with hierarchical datasets. In particular, we
propose a novel, flag-based method that decomposes arbitrary hierarchical
real-valued data into a hierarchy-preserving flag representation in Stiefel
coordinates. Our work harnesses the potential of flag manifolds in applications
including denoising, clustering, and few-shot learning.
</summary>
    <author>
      <name>Nathan Mankovich</name>
    </author>
    <author>
      <name>Ignacio Santamaria</name>
    </author>
    <author>
      <name>Gustau Camps-Valls</name>
    </author>
    <author>
      <name>Tolga Birdal</name>
    </author>
    <link href="http://arxiv.org/abs/2502.07782v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.07782v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.07780v1</id>
    <updated>2025-02-11T18:59:35Z</updated>
    <published>2025-02-11T18:59:35Z</published>
    <title>DarwinLM: Evolutionary Structured Pruning of Large Language Models</title>
    <summary>  Large Language Models (LLMs) have achieved significant success across various
NLP tasks. However, their massive computational costs limit their widespread
use, particularly in real-time applications. Structured pruning offers an
effective solution by compressing models and directly providing end-to-end
speed improvements, regardless of the hardware environment. Meanwhile,
different components of the model exhibit varying sensitivities towards
pruning, calling for \emph{non-uniform} model compression. However, a pruning
method should not only identify a capable substructure, but also account for
post-compression training. To this end, we propose \sysname, a method for
\emph{training-aware} structured pruning. \sysname builds upon an evolutionary
search process, generating multiple offspring models in each generation through
mutation, and selecting the fittest for survival. To assess the effect of
post-training, we incorporate a lightweight, multistep training process within
the offspring population, progressively increasing the number of tokens and
eliminating poorly performing models in each selection stage. We validate our
method through extensive experiments on Llama-2-7B, Llama-3.1-8B and
Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured
pruning. For instance, \sysname surpasses ShearedLlama while requiring
$5\times$ less training data during post-compression training.
</summary>
    <author>
      <name>Shengkun Tang</name>
    </author>
    <author>
      <name>Oliver Sieberling</name>
    </author>
    <author>
      <name>Eldar Kurtic</name>
    </author>
    <author>
      <name>Zhiqiang Shen</name>
    </author>
    <author>
      <name>Dan Alistarh</name>
    </author>
    <link href="http://arxiv.org/abs/2502.07780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.07780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.07777v1</id>
    <updated>2025-02-11T18:58:16Z</updated>
    <published>2025-02-11T18:58:16Z</published>
    <title>Feasibility study of multiplexing analog signals from SiPMs for a single
  layer monolithic PET detector design</title>
    <summary>  Semi monolithic detector designs with a series of stacked thin monolithic
scintillator plates and side readout are an attractive approach for potentially
achieving very high performance in a positron emission tomography (PET)
scanner. In this work, a simulation study of a single layer monolithic detector
module was performed with side read out of scintillation light using GATEv8.2.
In this design, a single layer LSO crystal was used with dimensions 40 mm*40
mm*40 mm, with 0.60 mm thickness of the ESR (enhanced specular reflector) films
covering the crystal's top and bottom surfaces. The photons generated in the
scintillation process induced by the gamma ray hitting the crystal were
detected by four 1*8 SiPM (silicon photomultiplier) arrays placed along the
four sides of the crystal. The scintillation light distribution detected by all
of the 32 SiPMs surrounding the crystal layer was then used to extract the
gamma-crystal interaction location based on machine learning analysis. In this
work, the spatial resolution of the detector module was explored when analog
signals from each of the 32 SiPMs were summed to 28, 24, 20, 16, 12, 8, and 4
total outputs. This study showed that good spatial resolution can be achieved
even when the number of read out channels is decreased by multiplexing, which
can reduce the overall detector manufacturing cost.
</summary>
    <author>
      <name>Shiv K. Subedi</name>
    </author>
    <author>
      <name>Simon R. Cherry</name>
    </author>
    <author>
      <name>Yi Qiang</name>
    </author>
    <author>
      <name>Peng Peng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 22 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.07777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.07777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
