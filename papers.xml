<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-05-01T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">421992</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.21850v1</id>
    <updated>2025-04-30T17:57:22Z</updated>
    <published>2025-04-30T17:57:22Z</published>
    <title>COMPACT: COMPositional Atomic-to-Complex Visual Capability Tuning</title>
    <summary>  Multimodal Large Language Models (MLLMs) excel at simple vision-language
tasks but struggle when faced with complex tasks that require multiple
capabilities, such as simultaneously recognizing objects, counting them, and
understanding their spatial relationships. This might be partially the result
of the fact that Visual Instruction Tuning (VIT), a critical training step for
MLLMs, has traditionally focused on scaling data volume, but not the
compositional complexity of training examples. We propose COMPACT
(COMPositional Atomic-to-complex visual Capability Tuning), which generates a
training dataset explicitly controlling for the compositional complexity of the
training examples. The data from COMPACT allows MLLMs to train on combinations
of atomic capabilities to learn complex capabilities more efficiently. Across
all benchmarks, COMPACT achieves comparable performance to the LLaVA-665k VIT
while using less than 10% of its data budget, and even outperforms it on
several, especially those involving complex multi-capability tasks. For
example, COMPACT achieves substantial 83.3% improvement on MMStar and 94.0%
improvement on MM-Vet compared to the full-scale VIT on particularly complex
questions that require four or more atomic capabilities. COMPACT offers a
scalable, data-efficient, visual compositional tuning recipe to improve on
complex visual-language tasks.
</summary>
    <author>
      <name>Xindi Wu</name>
    </author>
    <author>
      <name>Hee Seung Hwang</name>
    </author>
    <author>
      <name>Polina Kirichenko</name>
    </author>
    <author>
      <name>Olga Russakovsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.21850v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.21850v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.21847v1</id>
    <updated>2025-04-30T17:55:29Z</updated>
    <published>2025-04-30T17:55:29Z</published>
    <title>Differentiable Room Acoustic Rendering with Multi-View Vision Priors</title>
    <summary>  An immersive acoustic experience enabled by spatial audio is just as crucial
as the visual aspect in creating realistic virtual environments. However,
existing methods for room impulse response estimation rely either on
data-demanding learning-based models or computationally expensive physics-based
modeling. In this work, we introduce Audio-Visual Differentiable Room Acoustic
Rendering (AV-DAR), a framework that leverages visual cues extracted from
multi-view images and acoustic beam tracing for physics-based room acoustic
rendering. Experiments across six real-world environments from two datasets
demonstrate that our multimodal, physics-based approach is efficient,
interpretable, and accurate, significantly outperforming a series of prior
methods. Notably, on the Real Acoustic Field dataset, AV-DAR achieves
comparable performance to models trained on 10 times more data while delivering
relative gains ranging from 16.6% to 50.9% when trained at the same scale.
</summary>
    <author>
      <name>Derong Jin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Maryland, College Park</arxiv:affiliation>
    </author>
    <author>
      <name>Ruohan Gao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Maryland, College Park</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://humathe.github.io/avdar/</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.21847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.21847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.21844v1</id>
    <updated>2025-04-30T17:53:08Z</updated>
    <published>2025-04-30T17:53:08Z</published>
    <title>Scalable Multi-Task Learning for Particle Collision Event Reconstruction
  with Heterogeneous Graph Neural Networks</title>
    <summary>  The growing luminosity frontier at the Large Hadron Collider is challenging
the reconstruction and analysis of particle collision events. Increased
particle multiplicities are straining latency and storage requirements at the
data acquisition stage, while new complications are emerging, including higher
background levels and more frequent particle vertex misassociations. This in
turn necessitates the development of more holistic and scalable reconstruction
methods that take advantage of recent advances in machine learning. We propose
a novel Heterogeneous Graph Neural Network (HGNN) architecture featuring unique
representations for diverse particle collision relationships and integrated
graph pruning layers for scalability. Trained with a multi-task paradigm in an
environment mimicking the LHCb experiment, this HGNN significantly improves
beauty hadron reconstruction performance. Notably, it concurrently performs
particle vertex association and graph pruning within a single framework. We
quantify reconstruction and pruning performance, demonstrate enhanced inference
time scaling with event complexity, and mitigate potential performance loss
using a weighted message passing scheme.
</summary>
    <author>
      <name>William Sutcliffe</name>
    </author>
    <author>
      <name>Marta Calvi</name>
    </author>
    <author>
      <name>Simone Capelli</name>
    </author>
    <author>
      <name>Jonas Eschle</name>
    </author>
    <author>
      <name>Julián García Pardiñas</name>
    </author>
    <author>
      <name>Abhijit Mathad</name>
    </author>
    <author>
      <name>Azusa Uzuki</name>
    </author>
    <author>
      <name>Nicola Serra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 10 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.21844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.21844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="hep-ex" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.21841v1</id>
    <updated>2025-04-30T17:51:20Z</updated>
    <published>2025-04-30T17:51:20Z</published>
    <title>Neuro-Symbolic Generation of Explanations for Robot Policies with
  Weighted Signal Temporal Logic</title>
    <summary>  Neural network-based policies have demonstrated success in many robotic
applications, but often lack human-explanability, which poses challenges in
safety-critical deployments. To address this, we propose a neuro-symbolic
explanation framework that generates a weighted signal temporal logic (wSTL)
specification to describe a robot policy in a interpretable form. Existing
methods typically produce explanations that are verbose and inconsistent, which
hinders explainability, and loose, which do not give meaningful insights into
the underlying policy. We address these issues by introducing a simplification
process consisting of predicate filtering, regularization, and iterative
pruning. We also introduce three novel explainability evaluation metrics --
conciseness, consistency, and strictness -- to assess explanation quality
beyond conventional classification metrics. Our method is validated in three
simulated robotic environments, where it outperforms baselines in generating
concise, consistent, and strict wSTL explanations without sacrificing
classification accuracy. This work bridges policy learning with formal methods,
contributing to safer and more transparent decision-making in robotics.
</summary>
    <author>
      <name>Mikihisa Yuasa</name>
    </author>
    <author>
      <name>Ramavarapu S. Sreenivas</name>
    </author>
    <author>
      <name>Huy T. Tran</name>
    </author>
    <link href="http://arxiv.org/abs/2504.21841v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.21841v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.21840v1</id>
    <updated>2025-04-30T17:50:47Z</updated>
    <published>2025-04-30T17:50:47Z</published>
    <title>Parameter Inference of Black Hole Images using Deep Learning in
  Visibility Space</title>
    <summary>  Using very long baseline interferometry, the Event Horizon Telescope (EHT)
collaboration has resolved the shadows of two supermassive black holes. Model
comparison is traditionally performed in image space, where imaging algorithms
introduce uncertainties in the recovered structure. Here, we develop a deep
learning framework to perform parameter inference in visibility space, directly
using the data measured by the interferometer without introducing potential
errors and biases from image reconstruction. First, we train and validate our
framework on synthetic data derived from general relativistic
magnetohydrodynamics (GRMHD) simulations that vary in magnetic field state,
spin, and $R_\mathrm{high}$. Applying these models to the real data obtained
during the 2017 EHT campaign, and only considering total intensity, we do not
derive meaningful constraints on either of these parameters. At present, our
method is limited both by theoretical uncertainties in the GRMHD simulations
and variation between snapshots of the same underlying physical model. However,
we demonstrate that spin and $R_\mathrm{high}$ could be recovered using this
framework through continuous monitoring of our sources, which mitigates
variations due to turbulence. In future work, we anticipate that including
spectral or polarimetric information will greatly improve the performance of
this framework.
</summary>
    <author>
      <name>Franc O</name>
    </author>
    <author>
      <name>Pavlos Protopapas</name>
    </author>
    <author>
      <name>Dominic W. Pesce</name>
    </author>
    <author>
      <name>Angelo Ricarte</name>
    </author>
    <author>
      <name>Sheperd S. Doeleman</name>
    </author>
    <author>
      <name>Cecilia Garraffo</name>
    </author>
    <author>
      <name>Lindy Blackburn</name>
    </author>
    <author>
      <name>Mauricio Santillana</name>
    </author>
    <link href="http://arxiv.org/abs/2504.21840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.21840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.GA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
