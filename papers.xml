<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-04-30T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">421755</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2504.20997v1</id>
    <updated>2025-04-29T17:59:48Z</updated>
    <published>2025-04-29T17:59:48Z</published>
    <title>Toward Efficient Exploration by Large Language Model Agents</title>
    <summary>  A burgeoning area within reinforcement learning (RL) is the design of
sequential decision-making agents centered around large language models (LLMs).
While autonomous decision-making agents powered by modern LLMs could facilitate
numerous real-world applications, such successes demand agents that are capable
of data-efficient RL. One key obstacle to achieving data efficiency in RL is
exploration, a challenge that we demonstrate many recent proposals for LLM
agent designs struggle to contend with. Meanwhile, classic algorithms from the
RL literature known to gracefully address exploration require technical
machinery that can be challenging to operationalize in purely natural language
settings. In this work, rather than relying on finetuning or in-context
learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate
how LLMs can be used to explicitly implement an existing RL algorithm
(Posterior Sampling for Reinforcement Learning) whose capacity for
statistically-efficient exploration is already well-studied. We offer empirical
results demonstrating how our LLM-based implementation of a known,
data-efficient RL algorithm can be considerably more effective in natural
language tasks that demand prudent exploration.
</summary>
    <author>
      <name>Dilip Arumugam</name>
    </author>
    <author>
      <name>Thomas L. Griffiths</name>
    </author>
    <link href="http://arxiv.org/abs/2504.20997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.20997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.20995v1</id>
    <updated>2025-04-29T17:59:30Z</updated>
    <published>2025-04-29T17:59:30Z</published>
    <title>TesserAct: Learning 4D Embodied World Models</title>
    <summary>  This paper presents an effective approach for learning novel 4D embodied
world models, which predict the dynamic evolution of 3D scenes over time in
response to an embodied agent's actions, providing both spatial and temporal
consistency. We propose to learn a 4D world model by training on RGB-DN (RGB,
Depth, and Normal) videos. This not only surpasses traditional 2D models by
incorporating detailed shape, configuration, and temporal changes into their
predictions, but also allows us to effectively learn accurate inverse dynamic
models for an embodied agent. Specifically, we first extend existing robotic
manipulation video datasets with depth and normal information leveraging
off-the-shelf models. Next, we fine-tune a video generation model on this
annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for
each frame. We then present an algorithm to directly convert generated RGB,
Depth, and Normal videos into a high-quality 4D scene of the world. Our method
ensures temporal and spatial coherence in 4D scene predictions from embodied
scenarios, enables novel view synthesis for embodied environments, and
facilitates policy learning that significantly outperforms those derived from
prior video-based world models.
</summary>
    <author>
      <name>Haoyu Zhen</name>
    </author>
    <author>
      <name>Qiao Sun</name>
    </author>
    <author>
      <name>Hongxin Zhang</name>
    </author>
    <author>
      <name>Junyan Li</name>
    </author>
    <author>
      <name>Siyuan Zhou</name>
    </author>
    <author>
      <name>Yilun Du</name>
    </author>
    <author>
      <name>Chuang Gan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Project Page: https://tesseractworld.github.io/</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.20995v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.20995v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.20993v1</id>
    <updated>2025-04-29T17:58:47Z</updated>
    <published>2025-04-29T17:58:47Z</published>
    <title>GDP-GFCF Dynamics Across Global Economies: A Comparative Study of Panel
  Regressions and Random Forest</title>
    <summary>  This study examines the relationship between GDP growth and Gross Fixed
Capital Formation (GFCF) across developed economies (G7, EU-15, OECD) and
emerging markets (BRICS). We integrate Random Forest machine learning
(non-linear regression) with traditional econometric models (linear regression)
to better capture non-linear interactions in investment analysis. Our findings
reveal that while GDP growth positively influences corporate investment, its
impact varies significantly by region. Developed economies show stronger
GDP-GFCF linkages due to stable financial systems, while emerging markets
demonstrate weaker connections due to economic heterogeneity and structural
constraints. Random Forest models indicate that GDP growth's importance is
lower than suggested by traditional econometrics, with lagged GFCF emerging as
the dominant predictor-confirming investment follows path-dependent patterns
rather than short-term GDP fluctuations. Regional variations in investment
drivers are substantial: taxation significantly influences developed economies
but minimally affects BRICS, while unemployment strongly drives investment in
BRICS but less so elsewhere. We introduce a parallelized p-value importance
algorithm for Random Forest that enhances computational efficiency while
maintaining statistical rigor through sequential testing methods (SPRT and
SAPT). The research demonstrates that hybrid methodologies combining machine
learning with econometric techniques provide more nuanced understanding of
investment dynamics, supporting region-specific policy design and improving
forecasting accuracy.
</summary>
    <author>
      <name>Alina Landowska</name>
    </author>
    <author>
      <name>Robert A. KÅ‚opotek</name>
    </author>
    <author>
      <name>Dariusz Filip</name>
    </author>
    <author>
      <name>Konrad Raczkowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 5 figures, 6 tables, data and source code can be access via
  OFS
  https://osf.io/xpyhw/files/osfstorage?view_only=aa41c1d52ea54058b5f7d89aa3ea166c</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.20993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.20993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.20989v1</id>
    <updated>2025-04-29T17:57:01Z</updated>
    <published>2025-04-29T17:57:01Z</published>
    <title>Photonic Quantum Convolutional Neural Networks with Adaptive State
  Injection</title>
    <summary>  Linear optical architectures have been extensively investigated for quantum
computing and quantum machine learning applications. Recently, proposals for
photonic quantum machine learning have combined linear optics with resource
adaptivity, such as adaptive circuit reconfiguration, which promises to enhance
expressivity and improve algorithm performances and scalability. Moreover,
linear optical platforms preserve some subspaces due to the fixed number of
particles during the computation, a property recently exploited to design a
novel quantum convolutional neural networks. This last architecture has shown
an advantage in terms of running time complexity and of the number of
parameters needed with respect to other quantum neural network proposals. In
this work, we design and experimentally implement the first photonic quantum
convolutional neural network (PQCNN) architecture based on particle-number
preserving circuits equipped with state injection, an approach recently
proposed to increase the controllability of linear optical circuits.
Subsequently, we experimentally validate the PQCNN for a binary image
classification on a photonic platform utilizing a semiconductor quantum
dot-based single-photon source and programmable integrated photonic
interferometers comprising 8 and 12 modes. In order to investigate the
scalability of the PQCNN design, we have performed numerical simulations on
datasets of different sizes. We highlight the potential utility of a simple
adaptive technique for a nonlinear Boson Sampling task, compatible with
near-term quantum devices.
</summary>
    <author>
      <name>LÃ©o Monbroussou</name>
    </author>
    <author>
      <name>Beatrice Polacchi</name>
    </author>
    <author>
      <name>Verena Yacoub</name>
    </author>
    <author>
      <name>Eugenio Caruccio</name>
    </author>
    <author>
      <name>Giovanni Rodari</name>
    </author>
    <author>
      <name>Francesco Hoch</name>
    </author>
    <author>
      <name>Gonzalo Carvacho</name>
    </author>
    <author>
      <name>NicolÃ² Spagnolo</name>
    </author>
    <author>
      <name>Taira Giordani</name>
    </author>
    <author>
      <name>Mattia Bossi</name>
    </author>
    <author>
      <name>Abhiram Rajan</name>
    </author>
    <author>
      <name>Niki Di Giano</name>
    </author>
    <author>
      <name>Riccardo Albiero</name>
    </author>
    <author>
      <name>Francesco Ceccarelli</name>
    </author>
    <author>
      <name>Roberto Osellame</name>
    </author>
    <author>
      <name>Elham Kashefi</name>
    </author>
    <author>
      <name>Fabio Sciarrino</name>
    </author>
    <link href="http://arxiv.org/abs/2504.20989v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.20989v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.20988v1</id>
    <updated>2025-04-29T17:56:55Z</updated>
    <published>2025-04-29T17:56:55Z</published>
    <title>Hubs and Spokes Learning: Efficient and Scalable Collaborative Machine
  Learning</title>
    <summary>  We introduce the Hubs and Spokes Learning (HSL) framework, a novel paradigm
for collaborative machine learning that combines the strengths of Federated
Learning (FL) and Decentralized Learning (P2PL). HSL employs a two-tier
communication structure that avoids the single point of failure inherent in FL
and outperforms the state-of-the-art P2PL framework, Epidemic Learning Local
(ELL). At equal communication budgets (total edges), HSL achieves higher
performance than ELL, while at significantly lower communication budgets, it
can match ELL's performance. For instance, with only 400 edges, HSL reaches the
same test accuracy that ELL achieves with 1000 edges for 100 peers (spokes) on
CIFAR-10, demonstrating its suitability for resource-constrained systems. HSL
also achieves stronger consensus among nodes after mixing, resulting in
improved performance with fewer training rounds. We substantiate these claims
through rigorous theoretical analyses and extensive experimental results,
showcasing HSL's practicality for large-scale collaborative learning.
</summary>
    <author>
      <name>Atul Sharma</name>
    </author>
    <author>
      <name>Kavindu Herath</name>
    </author>
    <author>
      <name>Saurabh Bagchi</name>
    </author>
    <author>
      <name>Chaoyue Liu</name>
    </author>
    <author>
      <name>Somali Chaterji</name>
    </author>
    <link href="http://arxiv.org/abs/2504.20988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.20988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
