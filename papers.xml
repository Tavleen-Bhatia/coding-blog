<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Adeep%20learning%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:deep learning&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/o6m5I8YDMO+W1ZfiBhSOoh9xyHo</id>
  <updated>2025-02-21T00:00:00-05:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">406463</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2502.14866v1</id>
    <updated>2025-02-20T18:59:52Z</updated>
    <published>2025-02-20T18:59:52Z</published>
    <title>LServe: Efficient Long-sequence LLM Serving with Unified Sparse
  Attention</title>
    <summary>  Large language models (LLMs) have shown remarkable potential in processing
long sequences, yet efficiently serving these long-context models remains
challenging due to the quadratic computational complexity of attention in the
prefilling stage and the large memory footprint of the KV cache in the decoding
stage. To address these issues, we introduce LServe, an efficient system that
accelerates long-sequence LLM serving via hybrid sparse attention. This method
unifies different hardware-friendly, structured sparsity patterns for both
prefilling and decoding attention into a single framework, where computations
on less important tokens are skipped block-wise. LServe demonstrates the
compatibility of static and dynamic sparsity in long-context LLM attention.
This design enables multiplicative speedups by combining these optimizations.
Specifically, we convert half of the attention heads to nearly free streaming
heads in both the prefilling and decoding stages. Additionally, we find that
only a constant number of KV pages is required to preserve long-context
capabilities, irrespective of context length. We then design a hierarchical KV
page selection policy that dynamically prunes KV pages based on query-centric
similarity. On average, LServe accelerates LLM prefilling by up to 2.9x and
decoding by 1.3-2.1x over vLLM, maintaining long-context accuracy. Code is
released at https://github.com/mit-han-lab/omniserve.
</summary>
    <author>
      <name>Shang Yang</name>
    </author>
    <author>
      <name>Junxian Guo</name>
    </author>
    <author>
      <name>Haotian Tang</name>
    </author>
    <author>
      <name>Qinghao Hu</name>
    </author>
    <author>
      <name>Guangxuan Xiao</name>
    </author>
    <author>
      <name>Jiaming Tang</name>
    </author>
    <author>
      <name>Yujun Lin</name>
    </author>
    <author>
      <name>Zhijian Liu</name>
    </author>
    <author>
      <name>Yao Lu</name>
    </author>
    <author>
      <name>Song Han</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by MLSys 2025. Code available at:
  https://github.com/mit-han-lab/omniserve</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.14866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.14866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.14865v1</id>
    <updated>2025-02-20T18:59:51Z</updated>
    <published>2025-02-20T18:59:51Z</published>
    <title>Time Travel: A Comprehensive Benchmark to Evaluate LMMs on Historical
  and Cultural Artifacts</title>
    <summary>  Understanding historical and cultural artifacts demands human expertise and
advanced computational techniques, yet the process remains complex and
time-intensive. While large multimodal models offer promising support, their
evaluation and improvement require a standardized benchmark. To address this,
we introduce TimeTravel, a benchmark of 10,250 expert-verified samples spanning
266 distinct cultures across 10 major historical regions. Designed for
AI-driven analysis of manuscripts, artworks, inscriptions, and archaeological
discoveries, TimeTravel provides a structured dataset and robust evaluation
framework to assess AI models' capabilities in classification, interpretation,
and historical comprehension. By integrating AI with historical research,
TimeTravel fosters AI-powered tools for historians, archaeologists,
researchers, and cultural tourists to extract valuable insights while ensuring
technology contributes meaningfully to historical discovery and cultural
heritage preservation. We evaluate contemporary AI models on TimeTravel,
highlighting their strengths and identifying areas for improvement. Our goal is
to establish AI as a reliable partner in preserving cultural heritage, ensuring
that technological advancements contribute meaningfully to historical
discovery. Our code is available at:
\url{https://github.com/mbzuai-oryx/TimeTravel}.
</summary>
    <author>
      <name>Sara Ghaboura</name>
    </author>
    <author>
      <name>Ketan More</name>
    </author>
    <author>
      <name>Ritesh Thawkar</name>
    </author>
    <author>
      <name>Wafa Alghallabi</name>
    </author>
    <author>
      <name>Omkar Thawakar</name>
    </author>
    <author>
      <name>Fahad Shahbaz Khan</name>
    </author>
    <author>
      <name>Hisham Cholakkal</name>
    </author>
    <author>
      <name>Salman Khan</name>
    </author>
    <author>
      <name>Rao Muhammad Anwer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.14865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.14865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.14860v1</id>
    <updated>2025-02-20T18:59:31Z</updated>
    <published>2025-02-20T18:59:31Z</published>
    <title>Aligning LLMs to Ask Good Questions A Case Study in Clinical Reasoning</title>
    <summary>  Large language models (LLMs) often fail to ask effective questions under
uncertainty, making them unreliable in domains where proactive
information-gathering is essential for decisionmaking. We present ALFA, a
framework that improves LLM question-asking by (i) decomposing the notion of a
"good" question into a set of theory-grounded attributes (e.g., clarity,
relevance), (ii) controllably synthesizing attribute-specific question
variations, and (iii) aligning models via preference-based optimization to
explicitly learn to ask better questions along these fine-grained attributes.
Focusing on clinical reasoning as a case study, we introduce the MediQ-AskDocs
dataset, composed of 17k real-world clinical interactions augmented with 80k
attribute-specific preference pairs of follow-up questions, as well as a novel
expert-annotated interactive healthcare QA task to evaluate question-asking
abilities. Models aligned with ALFA reduce diagnostic errors by 56.6% on
MediQ-AskDocs compared to SOTA instruction-tuned LLMs, with a question-level
win-rate of 64.4% and strong generalizability. Our findings suggest that
explicitly guiding question-asking with structured, fine-grained attributes
offers a scalable path to improve LLMs, especially in expert application
domains.
</summary>
    <author>
      <name>Shuyue Stella Li</name>
    </author>
    <author>
      <name>Jimin Mun</name>
    </author>
    <author>
      <name>Faeze Brahman</name>
    </author>
    <author>
      <name>Jonathan S. Ilgen</name>
    </author>
    <author>
      <name>Yulia Tsvetkov</name>
    </author>
    <author>
      <name>Maarten Sap</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 8 figures, 8 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.14860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.14860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.14856v1</id>
    <updated>2025-02-20T18:58:10Z</updated>
    <published>2025-02-20T18:58:10Z</published>
    <title>FR-Spec: Accelerating Large-Vocabulary Language Models via
  Frequency-Ranked Speculative Sampling</title>
    <summary>  Speculative sampling has emerged as an important technique for accelerating
the auto-regressive generation process of large language models (LLMs) by
utilizing a draft-then-verify mechanism to produce multiple tokens per forward
pass. While state-of-the-art speculative sampling methods use only a single
layer and a language modeling (LM) head as the draft model to achieve
impressive layer compression, their efficiency gains are substantially reduced
for large-vocabulary LLMs, such as Llama-3-8B with a vocabulary of 128k tokens.
To address this, we present FR-Spec, a frequency-ranked speculative sampling
framework that optimizes draft candidate selection through vocabulary space
compression. By constraining the draft search to a frequency-prioritized token
subset, our method reduces LM Head computation overhead by 75% while ensuring
the equivalence of the final output distribution. Experiments across multiple
datasets demonstrate an average of 1.12$\times$ speedup over the
state-of-the-art speculative sampling method EAGLE-2.
</summary>
    <author>
      <name>Weilin Zhao</name>
    </author>
    <author>
      <name>Tengyu Pan</name>
    </author>
    <author>
      <name>Xu Han</name>
    </author>
    <author>
      <name>Yudi Zhang</name>
    </author>
    <author>
      <name>Ao Sun</name>
    </author>
    <author>
      <name>Yuxiang Huang</name>
    </author>
    <author>
      <name>Kaihuo Zhang</name>
    </author>
    <author>
      <name>Weilun Zhao</name>
    </author>
    <author>
      <name>Yuxuan Li</name>
    </author>
    <author>
      <name>Jianyong Wang</name>
    </author>
    <author>
      <name>Zhiyuan Liu</name>
    </author>
    <author>
      <name>Maosong Sun</name>
    </author>
    <link href="http://arxiv.org/abs/2502.14856v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.14856v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.14855v1</id>
    <updated>2025-02-20T18:58:07Z</updated>
    <published>2025-02-20T18:58:07Z</published>
    <title>Prompt-to-Leaderboard</title>
    <summary>  Large language model (LLM) evaluations typically rely on aggregated metrics
like accuracy or human preference, averaging across users and prompts. This
averaging obscures user- and prompt-specific variations in model performance.
To address this, we propose Prompt-to-Leaderboard (P2L), a method that produces
leaderboards specific to a prompt. The core idea is to train an LLM taking
natural language prompts as input to output a vector of Bradley-Terry
coefficients which are then used to predict the human preference vote. The
resulting prompt-dependent leaderboards allow for unsupervised task-specific
evaluation, optimal routing of queries to models, personalization, and
automated evaluation of model strengths and weaknesses. Data from Chatbot Arena
suggest that P2L better captures the nuanced landscape of language model
performance than the averaged leaderboard. Furthermore, our findings suggest
that P2L's ability to produce prompt-specific evaluations follows a power law
scaling similar to that observed in LLMs themselves. In January 2025, the
router we trained based on this methodology achieved the \#1 spot in the
Chatbot Arena leaderboard. Our code is available at this GitHub link:
https://github.com/lmarena/p2l.
</summary>
    <author>
      <name>Evan Frick</name>
    </author>
    <author>
      <name>Connor Chen</name>
    </author>
    <author>
      <name>Joseph Tennyson</name>
    </author>
    <author>
      <name>Tianle Li</name>
    </author>
    <author>
      <name>Wei-Lin Chiang</name>
    </author>
    <author>
      <name>Anastasios N. Angelopoulos</name>
    </author>
    <author>
      <name>Ion Stoica</name>
    </author>
    <link href="http://arxiv.org/abs/2502.14855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.14855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
